{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# READ THE CSV INTO DATAFRAME\n",
    "\n",
    "df = pd.read_csv('Syngenta/Syngenta_2017/Experiment_dataset.csv')\n",
    "# df2 = pd.read_csv('Syngenta/Syngenta_2017/Region_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE SOME OPTIONAL INFORMATION ABOUT THE DATAFRAME\n",
    "\n",
    "# print(df.head())\n",
    "# print(df.describe())\n",
    "print(df)\n",
    "print(df.Variety.unique(), \"\\nthere are \", len(df.Variety.unique()), \" unique varieties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL CORRELATION CHECKER\n",
    "\n",
    "#these were selected mainly due to their dtype\n",
    "potential_feature_columns = ['Location', 'Temperature', 'Precipitation', 'Solar Radiation', 'Soil class', 'CEC', 'pH', 'Clay', 'Silt', 'Sand', 'Area']\n",
    "# print(potential_feature_columns)\n",
    "potential_output_column = df.Yield\n",
    "\n",
    "potential_X = df.loc[:, potential_feature_columns]\n",
    "potential_y = potential_output_column\n",
    "\n",
    "for x in potential_X:\n",
    "    print(\"%s has correlation %s with yield\" % (x, np.correlate(df[x], potential_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY UNUSED OPTIONAL ONEHOT ENCODER\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas import Series\n",
    "le = LabelEncoder()\n",
    "integer_encoded = le.fit_transform(df.Variety)\n",
    "# le.classes_\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "print(onehot_encoded)\n",
    "\n",
    "# df['VarietyEncode'] = onehot_encoded\n",
    "# print(pd.get_dummies(df.Variety))\n",
    "# print(df.Variety)\n",
    "    \n",
    "from numpy import argmax\n",
    "inverted = le.inverse_transform([argmax(onehot_encoded[300, :])])\n",
    "print([argmax(onehot_encoded[300, :])])\n",
    "verted = le.transform(['V150834'])\n",
    "print(inverted)\n",
    "# inverted = le.inverse_transform([df.])\n",
    "# print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USED FROM CONVERTING EACH VARIETY TO A ONE HOT\n",
    "\n",
    "def convert_variety_to_one_hot(variety):\n",
    "    return onehot_encoded[le.transform([variety])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL APPLY ABOVE FUNCTION TO REPLACE EACH ROW IN VARIETY COLUMN WITH ONE HOT REPRESENTATION\n",
    "\n",
    "df.Variety = df.Variety.apply(lambda val: convert_variety_to_one_hot(val))\n",
    "\n",
    "# OPTIONAL RESHAPE\n",
    "df.Variety = df.Variety.apply(lambda val: val.reshape(174))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS A DIFFERENT APPROACH TO THE ABOVE FOUR CELLS, WHERE WE HAVE 174 ADDITIONAL FEATURE COLUMNS\n",
    "# EACH WITH A 0 (IF IT IS NOT OF THAT VARIETY) OR A 1 (IF IT IS OF THAT VARIETY)\n",
    "\n",
    "print(df)\n",
    "dummies = pd.get_dummies(df.Variety)\n",
    "print(dummies)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL COLUMNS! WE WANT TO PICK ONES THAT ARE GOOD FOR OUR LEARNING ALGORITHM AND DROP THE REST IN THE\n",
    "# FOLLOWING CELL\n",
    "\n",
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP ALL THE CELLS THAT ARE NOT USABLE SUCH AS THE ONES THAT ARE STRINGS OR DATES\n",
    "\n",
    "df = df.drop(['Experiment', 'Location', 'Planting date', 'Check Yield', 'Yield difference', 'Latitude', 'Longitude', 'Variety', 'PI'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET US ALSO MAKE SURE THERE ARE NO NAN IN THE DATA\n",
    "print(\"We expect to be %s nan values and there actually are %s nan values\\n\" % (0, np.sum(df.isnull().sum())))\n",
    "# AFTER COLUMNS, MAKE SURE NO SKETCHY ONES\n",
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "\n",
    "# feature_columns = ['Variety', 'Solar Radiation', 'Temperature', 'Precipitation', 'Location', 'Clay', 'Silt', 'Sand', 'pH', 'Precipitation', 'CEC', 'Soil class']\n",
    "\n",
    "X = df.drop(['Yield'], axis=1)\n",
    "# X = df.loc[:, feature_columns]\n",
    "\n",
    "y = df.Yield\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.01, train_size = 0.05, random_state = 42)\n",
    "\n",
    "train_visual, zz, zzz, zzzz = train_test_split(df, y, test_size=0.01, train_size=0.05, random_state = 42)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standard_scalar = StandardScaler()\n",
    "x_std = standard_scalar.fit_transform(X_train)\n",
    "print(x_std.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0, verbose=1)\n",
    "x_2d = tsne.fit_transform(x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_2d.shape)\n",
    "print(x_2d)\n",
    "from matplotlib import pyplot as plt\n",
    "markers=('s', 'd', 'o', '^', 'v')\n",
    "color_map = {0:'red', 1:'blue', 2:'lightgreen', 3:'purple', 4:'cyan'}\n",
    "plt.figure()\n",
    "# for idx, cl in enumerate(np.unique(x_2d)):\n",
    "#     print(cl)\n",
    "for idx, cl in enumerate(np.unique(y_train)):\n",
    "    plt.scatter(x=x_2d[y_train==cl,0], y=x_2d[y_train==cl,1], c=color_map[idx], marker=markers[idx], label=cl)\n",
    "\n",
    "#     plt.scatter(x=x_2d[cl,0], y=x_2d[cl,1], c=color_map[idx], marker=markers[idx], label=cl)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE FEATURE DISTRIBUTIONS OPTIONALLY\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "X_train[X_train.dtypes[(X_train.dtypes==\"float64\")|(X_train.dtypes==\"int64\")]\n",
    "                        .index.values].hist(figsize=[11,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MORE VISUALIZATION OPTIONAL\n",
    "\n",
    "# Import the class\n",
    "import kmapper as km\n",
    "\n",
    "# Some sample data\n",
    "from sklearn import datasets\n",
    "# data, labels = datasets.make_circles(n_samples=5000, noise=0.03, factor=0.3)\n",
    "# print(data)\n",
    "# print(labels)\n",
    "\n",
    "# Initialize\n",
    "mapper = km.KeplerMapper(verbose=1)\n",
    "\n",
    "# Fit to and transform the data\n",
    "projected_data = mapper.fit_transform(X_train, projection=\"knn_distance_5\") # X-Y axis\n",
    "\n",
    "# Create dictionary called 'graph' with nodes, edges and meta-information\n",
    "graph = mapper.map(projected_data, X_train, nr_cubes=10)\n",
    "\n",
    "# Visualize it\n",
    "mapper.visualize(graph, path_html=\"keplermapper.html\",\n",
    "                 inverse_X_names=['Solar Radiation', 'Temperature', 'Location', 'Precipitation'],\n",
    "                 title=\"keplermapper of features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT COULDN'T HURT TO SEE THE FINAL SHAPES\n",
    "\n",
    "# print(X_train.describe())\n",
    "print(X_train.head())\n",
    "# print(y_train.describe())\n",
    "print(y_train.head())\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(n_estimators=10, max_depth=20, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "print(errors)\n",
    "print(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET OUTPUT OF FEATURE IMPORTANCE\n",
    "\n",
    "feature_importances = regr.feature_importances_\n",
    "feature_importances = pd.Series(feature_importances)\n",
    "feature_importance_df = pd.DataFrame({'feature': X_train.columns,'feature_importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by=['feature_importance'])\n",
    "for index, row in feature_importance_df.iterrows():\n",
    "    print(row['feature'], 'has importance: ', row['feature_importance'])\n",
    "# for feature_importance in regr.feature_importances_:\n",
    "    \n",
    "print(errors.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "classifiers = [\n",
    "    svm.SVR(),\n",
    "    MLPRegressor(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(5, 2), random_state=1),\n",
    "#     linear_model.SGDRegressor(),\n",
    "#     linear_model.BayesianRidge(),\n",
    "#     linear_model.LassoLars(),\n",
    "#     linear_model.ARDRegression(),\n",
    "#     linear_model.ARDRegression(),\n",
    "    linear_model.PassiveAggressiveRegressor(),\n",
    "    linear_model.TheilSenRegressor(),\n",
    "    linear_model.LinearRegression()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# estimator = svm.SVR(kernel=\"linear\")\n",
    "\n",
    "# selector = RFECV(estimator, step=1, cv=5, verbose=1)\n",
    "# selector = selector.fit(X_train, y_train)\n",
    "# selector.support_ \n",
    "# # array([ True,  True,  True,  True,  True,\n",
    "# #         False, False, False, False, False], dtype=bool)\n",
    "# selector.ranking_\n",
    "# # array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "\n",
    "\n",
    "#     print(np.sum(preds - y_test))\n",
    "#     print(clf.predict(X_test),'\\n')\n",
    "#     print(y_test)\n",
    "#     print('accuracy score:', accuracy_score(y_test, clf.predict(X_test)), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "    print(errors)\n",
    "    print(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import tree\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
