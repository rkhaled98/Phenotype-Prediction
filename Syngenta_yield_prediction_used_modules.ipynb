{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THERE ARE SEPARATE NOTEBOOKS FOR VISUALIZATIONS, DATASET ANALYSIS, ETC. IN THE REPO.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# READ THE CSV INTO DATAFRAME\n",
    "\n",
    "df = pd.read_csv('Syngenta/Syngenta_2017/Experiment_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS USED WHEN CONNECTING TO COMPUTE NODES TO CHECK IF EVERYTHING IS WORKING\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY NECESSARY IF: USING 174 ADDITIONAL VARIETY COLUMNS METHOD\n",
    "\n",
    "variety_dummies = pd.get_dummies(df.Variety)\n",
    "df = pd.concat([df, variety_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY NECESSARY IF: USING 8 ADDITIONAL VARIETY COLUMNS (BINARY REPRESENTATION)\n",
    "\n",
    "UNIQUE_VARIETIES = np.unique(df.Variety)\n",
    "INDICES = [i for i in range(0, len(UNIQUE_VARIETIES))]\n",
    "print(np.where(UNIQUE_VARIETIES=='V000017'))\n",
    "df.Variety = df.Variety.apply(lambda var: np.where(UNIQUE_VARIETIES==var)[0][0])\n",
    "print(UNIQUE_VARIETIES)\n",
    "print(df.Variety)\n",
    "\n",
    "df.Variety = df.Variety.apply(lambda var: '{0:08b}'.format(var))\n",
    "print(\"done\")\n",
    "df.Variety = df.Variety.apply(lambda var: list(var))\n",
    "print(\"done\")\n",
    "variety_binary_df = pd.DataFrame(pd.Series(df.Variety[i]) for i in range(0, len(df.Variety)))\n",
    "df = pd.concat([df, variety_binary_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GOAL OF THIS MODULE:\n",
    "# Encode the planting date as a season\n",
    "# NEW GOAL:\n",
    "# GET DUMMIES FOR SEASONS\n",
    "\n",
    "# remove the dates that are \".\"\n",
    "df = df[~df['Planting date'].str.match(\"\\.\")]\n",
    "plant_date = df['Planting date'].apply(lambda dt: pd.to_datetime(dt))\n",
    "plant_months = plant_date.apply(lambda dt: dt.month)\n",
    "season = plant_date.rename(\"Season\")\n",
    "season = pd.to_datetime(season)\n",
    "season = season.apply(lambda dt: (dt.month%12 + 3)//3)\n",
    "# df['Plant date'] = pd.to_datetime(df['Plant date'])\n",
    "df = pd.concat([df, season], axis=1)\n",
    "\n",
    "# plant_date = pd.to_datetime(df['Planting date'], infer_datetime_format=True)\n",
    "# df = df['Planting date'].apply(lambda dt: (dt.month%12 + 3)//3)\n",
    "# pd.get_dummies(df['Planting date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MONTH OF MAY AND JUNE ONE HOT ENCODING INTO THE DATAFRAME\n",
    "pd.get_dummies(plant_months).sum()\n",
    "june = pd.get_dummies(plant_months).loc[:,6]\n",
    "june = june.rename(\"June\")\n",
    "may = pd.get_dummies(plant_months).loc[:,5]\n",
    "may = may.rename(\"May\")\n",
    "df = pd.concat([df, may], axis=1)\n",
    "df = pd.concat([df, june], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATITUDE AND LONGITUDE CLUSTERING INTO FEATURES\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "latlong = df.loc[:, ['Latitude', 'Longitude']]\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(latlong)\n",
    "kmeans.labels_.shape\n",
    "lat_long_dummies = pd.get_dummies(kmeans.labels_)\n",
    "lat_long_dummies = lat_long_dummies.rename(index=int, columns={0: \"Loc Clust 0\",\n",
    "                                                               1: \"Loc Clust 1\",\n",
    "                                                               2: \"Loc Clust 2\",\n",
    "                                                               3: \"Loc Clust 3\"})\n",
    "df = pd.concat([df, lat_long_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE ANY NAN VALUES\n",
    "\n",
    "print(df.columns)\n",
    "df = df[~df.Silt.isnull()]\n",
    "df = df[~df['Loc Clust 1'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final dataframe has columns:  Index(['Yield', 'Year', 'Temperature', 'Precipitation', 'Solar Radiation',\n",
      "       'Soil class', 'CEC', 'Organic matter', 'pH', 'Clay', 'Silt', 'Sand',\n",
      "       'Area', 'YieldBucket'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# DROP ALL THE CELLS THAT ARE NOT USABLE SUCH AS THE ONES THAT ARE STRINGS OR DATES\n",
    "\n",
    "# set if want to drop some columns specifically\n",
    "should_drop = 1\n",
    "columns_to_drop = ['Experiment', 'Location',\n",
    "                   'Check Yield', 'Yield difference', 'Latitude',\n",
    "                   'Longitude', 'PI', 'Variety', 'Planting date']\n",
    "\n",
    "# set if want to keep some columns specifically\n",
    "should_keep = 0\n",
    "# columns_to_keep = ['Loc Clust 0', 'Loc Clust 1', 'Loc Clust 2', 'Loc Clust 3']\n",
    "columns_to_keep_top = ['Silt', 'Precipitation', 'Temperature', 'Solar Radiation', 'Organic matter']\n",
    "columns_VARIETIES_ONLY = np.asarray(df.iloc[:, df.columns.str.match('V\\d\\d\\d\\d\\d\\d')].columns)\n",
    "\n",
    "#set the below variable to whatever columns you want to keep\n",
    "columns_to_keep = columns_to_keep_top\n",
    "\n",
    "MUST_HAVE_COLUMNS = ['Yield']\n",
    "# print(columns_to_keep)\n",
    "\n",
    "df = df.drop(columns_to_drop, axis=1) if should_drop else df\n",
    "df = df.loc[:, np.concatenate((columns_to_keep, MUST_HAVE_COLUMNS))] if should_keep else df\n",
    "df['YieldBucket'] = pd.Series(pd.qcut(df.Yield, q=3, labels=[\"high\", \"medium\", \"low\"]))\n",
    "print(\"The final dataframe has columns: \", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET US ALSO MAKE SURE THERE ARE NO NAN IN THE DATA\n",
    "print(\"We expect to be %s nan values and there actually are %s nan values\\n\" % (0, np.sum(df.isnull().sum())))\n",
    "print(df.isnull().sum())\n",
    "# AFTER COLUMNS, MAKE SURE NO SKETCHY ONES\n",
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'Temperature', 'Precipitation', 'Solar Radiation', 'Soil class',\n",
      "       'CEC', 'Organic matter', 'pH', 'Clay', 'Silt', 'Sand', 'Area'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "\n",
    "X = df.drop(['Yield', 'YieldBucket'], axis=1)\n",
    "\n",
    "print(X.columns)\n",
    "\n",
    "y = df.Yield\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, train_size = 0.1, random_state = 42)\n",
    "\n",
    "INPUT_COLS = X_train.columns\n",
    "# TEST_COLS = y_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8203, 12) \n",
      "y_train shape: (8203,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape, \"\\ny_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will evaluate the errors based on RMSE (from the challenge spec)\n",
    "# also will evaluate based on average error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def evaluate_errors(prediction, actual):\n",
    "    print(\"RMSE Error: \", np.sqrt(mean_squared_error(prediction, actual)))\n",
    "    avg_error_vector = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(\"Average Error: \", np.mean(avg_error_vector))\n",
    "    print(\"Average Error details:\\n\", avg_error_vector.describe())\n",
    "    return avg_error_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Error:  7.291467200711183\n",
      "Average Error details:\n",
      " count    4102.000000\n",
      "mean       10.227188\n",
      "std        10.643095\n",
      "min         0.002144\n",
      "25%         3.559194\n",
      "50%         7.510455\n",
      "75%        13.420467\n",
      "max       171.625417\n",
      "Name: Yield, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76141    12.019414\n",
       "11321    39.339842\n",
       "68096     1.333623\n",
       "10813     3.818964\n",
       "30586    14.469933\n",
       "62263     1.644269\n",
       "58403     7.806270\n",
       "45601    23.186419\n",
       "69170    13.819154\n",
       "20865     1.612155\n",
       "14592     7.335614\n",
       "79035     0.273740\n",
       "48667    13.037783\n",
       "31714     7.532355\n",
       "48739     8.071056\n",
       "73832     8.379273\n",
       "38065    10.077663\n",
       "17050    15.403074\n",
       "61447     4.204653\n",
       "9293      2.772576\n",
       "64269     3.415104\n",
       "50211     3.160133\n",
       "15219     8.077335\n",
       "43234    10.691826\n",
       "60842     7.684873\n",
       "33703     0.715237\n",
       "40339     6.234396\n",
       "78791    16.080917\n",
       "54940    15.679956\n",
       "32595     5.937887\n",
       "           ...    \n",
       "47740     0.882921\n",
       "35395     4.693666\n",
       "54690    21.296788\n",
       "58371     1.535508\n",
       "41131     7.195533\n",
       "48939     5.250511\n",
       "71954     0.860517\n",
       "80128     4.969059\n",
       "23210    61.458396\n",
       "59380     8.524590\n",
       "49343     2.462502\n",
       "75084     3.251400\n",
       "3821      6.784155\n",
       "27294     4.427039\n",
       "56009    12.015100\n",
       "8107      1.085901\n",
       "4086      1.015050\n",
       "2522     13.690517\n",
       "81788     6.654065\n",
       "3024      5.971741\n",
       "71351    11.991317\n",
       "16141    29.100622\n",
       "74593     1.558963\n",
       "25983     8.884165\n",
       "12450    17.986186\n",
       "24760     6.324135\n",
       "16370     0.522497\n",
       "52472     0.057717\n",
       "37028    24.400015\n",
       "20546    33.921307\n",
       "Name: Yield, Length: 4102, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(n_estimators=20, max_depth=13, random_state=0, verbose=1, n_jobs=-1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "evaluate_errors(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET OUTPUT OF FEATURE IMPORTANCE\n",
    "def get_feature_importances(regr):\n",
    "    feature_importances = regr.feature_importances_\n",
    "    feature_importances = pd.Series(feature_importances)\n",
    "    feature_importance_df = pd.DataFrame({'feature': X_train.columns,'feature_importance': feature_importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by=['feature_importance'])\n",
    "    for index, row in feature_importance_df.iterrows():\n",
    "        print(row['feature'], 'has importance: ', row['feature_importance'])\n",
    "get_feature_importances(regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "#     MLPRegressor(solver='lbfgs', alpha=1e-5,\n",
    "#                      hidden_layer_sizes=(5, 2), random_state=1),\n",
    "#     RandomForestRegressor(n_estimators=20, max_depth=13, random_state=0, verbose=1, n_jobs=-1),\n",
    "#     linear_model.SGDRegressor(),\n",
    "#     linear_model.BayesianRidge(),\n",
    "#     linear_model.LassoLars(),\n",
    "#     linear_model.ARDRegression(),\n",
    "#     linear_model.PassiveAggressiveRegressor(),\n",
    "#     linear_model.TheilSenRegressor(),\n",
    "#     linear_model.LinearRegression(),\n",
    "    LGBMRegressor(n_estimators=1000, learning_rate=0.01),\n",
    "    svm.SVR(kernel=\"linear\"),\n",
    "    linear_model.ARDRegression()]\n",
    "\n",
    "# estimator = svm.SVR(kernel=\"linear\")\n",
    "# estimator = \n",
    "\n",
    "# selector = RFECV(estimator, step=1, cv=5, verbose=1, n_jobs=-1)\n",
    "# selector = selector.fit(X_train, y_train)\n",
    "# selector.support_ \n",
    "# # array([ True,  True,  True,  True,  True,\n",
    "# #         False, False, False, False, False], dtype=bool)\n",
    "# selector.ranking_\n",
    "# # array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "\n",
    "\n",
    "#     print(np.sum(preds - y_test))\n",
    "#     print(clf.predict(X_test),'\\n')\n",
    "#     print(y_test)\n",
    "#     print('accuracy score:', accuracy_score(y_test, clf.predict(X_test)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    selector = selector = RFECV(clf, step=1, verbose=1, n_jobs=-1)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    print(selector.support_)\n",
    "    print(selector.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS MODULE WILL OUTPUT A DICTIONARY WITH THE FEATURE RANKING OF A CLASSIFIER\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "def RFEVC_(clf, X_train_, y_train_, step_=1, verbose_=0):\n",
    "    selector = RFECV(clf, step=step_, verbose=verbose_, n_jobs=-1)\n",
    "    selector = selector.fit(X_train_, y_train_)\n",
    "    \n",
    "    INPUT_COLS = X_train.columns\n",
    "    RANKS = selector.ranking_\n",
    "\n",
    "    rank_dict = {}\n",
    "    for idx, item in enumerate(RANKS):\n",
    "        rank_dict[INPUT_COLS[idx]] = item\n",
    "    \n",
    "    import pprint\n",
    "    pprint.pprint(rank_dict)\n",
    "    \n",
    "    return rank_dict, selector\n",
    "\n",
    "# USAGE EXAMPLE:\n",
    "# rank_dict, selec = RFEVC_(regr, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Area': 1,\n",
      " 'CEC': 1,\n",
      " 'Clay': 1,\n",
      " 'Organic matter': 1,\n",
      " 'Precipitation': 1,\n",
      " 'Sand': 1,\n",
      " 'Silt': 1,\n",
      " 'Soil class': 1,\n",
      " 'Solar Radiation': 1,\n",
      " 'Temperature': 1,\n",
      " 'Year': 1,\n",
      " 'pH': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Area': 1,\n",
      " 'CEC': 1,\n",
      " 'Clay': 1,\n",
      " 'Organic matter': 1,\n",
      " 'Precipitation': 1,\n",
      " 'Sand': 1,\n",
      " 'Silt': 1,\n",
      " 'Soil class': 1,\n",
      " 'Solar Radiation': 1,\n",
      " 'Temperature': 1,\n",
      " 'Year': 1,\n",
      " 'pH': 1}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INPUT_COLS = X_train.columns\n",
    "\n",
    "outputs = \"1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"\n",
    "[ 4 10  9 11  8  7  3  2  1  1  1  1  1  5  6  1]\n",
    "import re\n",
    "outputs = re.sub(r'\\n', '', outputs)\n",
    "outputs = re.sub(r'  ', ' ', outputs)\n",
    "outputs = re.sub(r' ', ',', outputs)\n",
    "# print(outputs)\n",
    "\n",
    "# outputs.replace('\\n', \"\")\n",
    "# outputs.replace(\"  \", \",\")\n",
    "output = np.asarray(outputs.split(','))\n",
    "print(output)\n",
    "output = output == \"True\"\n",
    "\n",
    "for i in range(0, len(output)):\n",
    "    if output[i] == True:\n",
    "        print(\"CHOSEN: \", INPUT_COLS[i])\n",
    "    else:\n",
    "        print(\"NOT CHOSEN: ,\", INPUT_COLS[i])\n",
    "        \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)\n",
    "errors = evaluate_errors(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    errors = evaluate_errors(preds, y_test)\n",
    "    try:\n",
    "        get_feature_importances(clf)\n",
    "    except:\n",
    "        print(\"NO FEATURE IMPORTANCE METRIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WILL ONLY WORK WITH THE BUCKET METHOD\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regr = RandomForestClassifier(n_estimators=10, max_depth=20, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "#     KNeighborsClassifier(3),\n",
    "#     SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "from sklearn.metrics import accuracy_score\n",
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(scale(X_train), y_train)\n",
    "    preds = clf.predict(scale(X_test))\n",
    "    print(accuracy_score(y_test, preds))\n",
    "#     errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(errors)\n",
    "#     print(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_VARIETIES = 174\n",
    "\n",
    "def best_yield_variety(regr, test_set, random_sel = True, n_samples = 174, print_variety_preds = True):\n",
    "    \n",
    "    #create empty df\n",
    "    dup_df = pd.DataFrame()\n",
    "    \n",
    "    #choose a rand sample of input test_set (for dev purposes, wouldn't be used in app)\n",
    "    test_set_sample = test_set.sample(n= n_samples) if random_sel else test_set\n",
    "    \n",
    "    #progress of intensive, long for loop upcoming\n",
    "    counter = 0\n",
    "    \n",
    "    #for loop will, for each row in test_set_sample, duplicate that row by NUM_VARIETIES\n",
    "    for index, row in test_set_sample.iterrows():\n",
    "        counter+=1\n",
    "        print(counter)\n",
    "        dup_df = dup_df.append([row] * NUM_VARIETIES, ignore_index = True)\n",
    "        \n",
    "    #extract the varieties columns\n",
    "    duplicated_df_varieties = dup_df.loc[:, dup_df.columns.str.match('V\\d\\d\\d\\d\\d\\d')]\n",
    "    #extract the names of the varieties\n",
    "    varieties_array = duplicated_df_varieties.columns\n",
    "    num_expanded_data_pts = duplicated_df_varieties.shape[0]\n",
    "    #we must have a zeroed matrix of the same shape as the duplicated_df_varieties\n",
    "    #so that we can input a 1 just once for each variety per 174 rows\n",
    "    d = np.zeros((duplicated_df_varieties.shape[0], NUM_VARIETIES))\n",
    "    #make d our dataframe, with columns equal to the varieties\n",
    "    duplicated_df_varieties = pd.DataFrame(d, columns=varieties_array)\n",
    "    #for loop will place a 1 just once for each variety per 174 rows (one hot rep)\n",
    "    for i in range(duplicated_df_varieties.shape[0]):\n",
    "        var_index = i % 174\n",
    "        duplicated_df_varieties.loc[i, varieties_array[var_index]] = 1\n",
    "    #remove the varieties (will be added back with the new values)\n",
    "    dup_df = dup_df.drop(varieties_array, axis = 1)\n",
    "    #add the new values (one hot representations) from above for loop\n",
    "    dup_df = pd.concat([dup_df, duplicated_df_varieties], axis=1)\n",
    "    #do prediction on the entire dataframe\n",
    "    preds_per_variety = regr.predict(dup_df)\n",
    "    #*******make it into a dataframe where each row will give the performance of each variety\n",
    "    #with the same environmental conditions*******\n",
    "    preds_df = pd.DataFrame(preds_per_variety.reshape((int(num_expanded_data_pts/NUM_VARIETIES), NUM_VARIETIES)),\n",
    "                            columns=varieties_array)\n",
    "    \n",
    "    #environmental conditions (everything except the variety data)\n",
    "    envcond = test_set_sample.drop(varieties_array, axis=1)\n",
    "    \n",
    "    #a simple print out for best variety given the environmental conditions\n",
    "    hr_preds = []\n",
    "    if print_variety_preds:\n",
    "        envcond_cols = envcond.columns\n",
    "        counter = -1\n",
    "        for idx, row in envcond.iterrows():\n",
    "            counter+=1\n",
    "            out = \"For environmental conditions:\\n%s\\nthe best variety is:%s\" % (row, preds_df.idxmax(axis=1)[counter])\n",
    "            hr_preds.append(out)\n",
    "            print(out)\n",
    "    \n",
    "    return preds_df, envcond, hr_preds\n",
    "    \n",
    "        \n",
    "preds_df, envcond, hr_preds = best_yield_variety(regr, X_test, n_samples = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find number of times a variety is the maximum in the above prediction dataframe\n",
    "maxes = preds_df.idxmax(axis=1) # get first max variety per env\n",
    "# print(pd.get_dummies(maxes).describe())\n",
    "print(pd.get_dummies(maxes).sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find number of times a variety is the maximum in the above prediction dataframe\n",
    "maxes = preds_df.idxmax(axis=1) # get first max variety per env\n",
    "# print(pd.get_dummies(maxes).describe())\n",
    "print(pd.get_dummies(maxes).sum().sort_values(ascending=False))\n",
    "print(len(pd.get_dummies(maxes).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hr_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.describe().sort_values(by=\"mean\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.describe().loc['mean',:].sort_values().describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
