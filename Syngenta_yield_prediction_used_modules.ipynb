{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# READ THE CSV INTO DATAFRAME\n",
    "\n",
    "df = pd.read_csv('Syngenta/Syngenta_2017/Experiment_dataset.csv')\n",
    "# df2 = pd.read_csv('Syngenta/Syngenta_2017/Region_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Experiment  Location  Variety Planting date  Yield  Check Yield  \\\n",
      "0      09SUBWYG201      2291  V000047       5/20/09  55.62        53.61   \n",
      "1      09SUBWYG202      2291  V000047       5/20/09  53.72        53.61   \n",
      "2      09SUBWYG203      2291  V000047       5/20/09  49.09        53.61   \n",
      "3      09SUBWYG204      2291  V000047       5/20/09  53.00        53.61   \n",
      "4      09SUBWYG301      2290  V130307        5/8/09  44.51        57.32   \n",
      "5      09SUBWYG301      2290  V000025        5/8/09  47.41        57.32   \n",
      "6      09SUBWYG302      2290  V000024        5/8/09  49.15        57.32   \n",
      "7      09SUBWYG302      2290  V130307        5/8/09  43.23        57.32   \n",
      "8      09SUBWYG302      2290  V000124        5/8/09  50.99        57.32   \n",
      "9      09SUBWYG302      2290  V000025        5/8/09  51.32        57.32   \n",
      "10     09SUBWYG302      3490  V000024       5/26/09  70.99        61.90   \n",
      "11     09SUBWYG302      3490  V130307       5/26/09  65.61        61.90   \n",
      "12     09SUBWYG302      3490  V000124       5/26/09  68.84        61.90   \n",
      "13     09SUBWYG302      3490  V000025       5/26/09  74.75        61.90   \n",
      "14     09SUBWYG304      2290  V139094        5/8/09  48.81        57.32   \n",
      "15     09SUBWYG304      2290  V000024        5/8/09  45.01        57.32   \n",
      "16     09SUBWYG304      2290  V000023        5/8/09  50.71        57.32   \n",
      "17     09SUBWYG304      2290  V000124        5/8/09  52.16        57.32   \n",
      "18     09SUBWYG304      3490  V139094       5/26/09  70.99        61.90   \n",
      "19     09SUBWYG304      3490  V000024       5/26/09  66.15        61.90   \n",
      "20     09SUBWYG304      3490  V000023       5/26/09  66.69        61.90   \n",
      "21     09SUBWYG304      3490  V000124       5/26/09  72.06        61.90   \n",
      "22     09SUBWYG306      3490  V000125       5/26/09  64.00        61.90   \n",
      "23     09SUBWYG306      3490  V000051       5/26/09  69.91        61.90   \n",
      "24     09SUBWYG306      3490  V000025       5/26/09  70.45        61.90   \n",
      "25     09SUBWYG309      3210  V000051       5/11/09  48.54        64.12   \n",
      "26     09SUBWYG309      3210  V000025       5/11/09  42.89        64.12   \n",
      "27     09SUBWYG309      3490  V000051       5/28/09  65.61        61.90   \n",
      "28     09SUBWYG309      3490  V000025       5/28/09  65.61        61.90   \n",
      "29     09SUBWYG310      3490  V000051       5/28/09  70.45        61.90   \n",
      "...            ...       ...      ...           ...    ...          ...   \n",
      "82006  15SUSGYGY23      3395  V137136        6/6/15  59.12        55.49   \n",
      "82007  15SUSGYGY23      3395  V113396        6/6/15  59.95        55.49   \n",
      "82008  15SUSGYGY23      3395  V152440        6/6/15  53.73        55.49   \n",
      "82009  15SUSGYGY23      3395  V120047        6/6/15  55.49        55.49   \n",
      "82010  15SUSGYGY23      3435  V152779        6/5/15  58.64        53.36   \n",
      "82011  15SUSGYGY23      3435  V121015        6/5/15  54.16        53.36   \n",
      "82012  15SUSGYGY23      3435  V137136        6/5/15  57.39        53.36   \n",
      "82013  15SUSGYGY23      3435  V113396        6/5/15  57.25        53.36   \n",
      "82014  15SUSGYGY23      3435  V152440        6/5/15  59.38        53.36   \n",
      "82015  15SUSGYGY23      3435  V120047        6/5/15  49.29        53.36   \n",
      "82016  15SUSGYGY24      3325  V121015        6/6/15  69.85        58.72   \n",
      "82017  15SUSGYGY24      3325  V137136        6/6/15  65.46        58.72   \n",
      "82018  15SUSGYGY24      3325  V113396        6/6/15  66.17        58.72   \n",
      "82019  15SUSGYGY24      3325  V152440        6/6/15  64.53        58.72   \n",
      "82020  15SUSGYGY24      3325  V136868        6/6/15  65.71        58.72   \n",
      "82021  15SUSGYGY24      3395  V121015        6/6/15  56.32        55.49   \n",
      "82022  15SUSGYGY24      3395  V137136        6/6/15  58.19        55.49   \n",
      "82023  15SUSGYGY24      3395  V113396        6/6/15  61.61        55.49   \n",
      "82024  15SUSGYGY24      3395  V152440        6/6/15  59.84        55.49   \n",
      "82025  15SUSGYGY24      3395  V136868        6/6/15  59.01        55.49   \n",
      "82026  15SUSGYGY25      3325  V121015        6/6/15  59.09        58.72   \n",
      "82027  15SUSGYGY25      3325  V137136        6/6/15  57.98        58.72   \n",
      "82028  15SUSGYGY25      3325  V113396        6/6/15  65.99        58.72   \n",
      "82029  15SUSGYGY25      3325  V152440        6/6/15  62.43        58.72   \n",
      "82030  15SUSGYGY25      3325  V136868        6/6/15  73.75        58.72   \n",
      "82031  15SUSGYGY25      3395  V121015        6/6/15  54.14        55.49   \n",
      "82032  15SUSGYGY25      3395  V137136        6/6/15  51.96        55.49   \n",
      "82033  15SUSGYGY25      3395  V113396        6/6/15  59.64        55.49   \n",
      "82034  15SUSGYGY25      3395  V152440        6/6/15  55.49        55.49   \n",
      "82035  15SUSGYGY25      3395  V136868        6/6/15  55.07        55.49   \n",
      "\n",
      "       Yield difference  Year   Latitude  Longitude   ...     V156565  \\\n",
      "0                  2.01  2009  42.019111 -93.525735   ...           0   \n",
      "1                  0.11  2009  42.019111 -93.525735   ...           0   \n",
      "2                 -4.52  2009  42.019111 -93.525735   ...           0   \n",
      "3                 -0.61  2009  42.019111 -93.525735   ...           0   \n",
      "4                -12.81  2009  42.016877 -93.526748   ...           0   \n",
      "5                 -9.90  2009  42.016877 -93.526748   ...           0   \n",
      "6                 -8.17  2009  42.016877 -93.526748   ...           0   \n",
      "7                -14.09  2009  42.016877 -93.526748   ...           0   \n",
      "8                 -6.33  2009  42.016877 -93.526748   ...           0   \n",
      "9                 -5.99  2009  42.016877 -93.526748   ...           0   \n",
      "10                 9.09  2009  40.699070 -86.900038   ...           0   \n",
      "11                 3.71  2009  40.699070 -86.900038   ...           0   \n",
      "12                 6.94  2009  40.699070 -86.900038   ...           0   \n",
      "13                12.85  2009  40.699070 -86.900038   ...           0   \n",
      "14                -8.51  2009  42.016877 -93.526748   ...           0   \n",
      "15               -12.30  2009  42.016877 -93.526748   ...           0   \n",
      "16                -6.61  2009  42.016877 -93.526748   ...           0   \n",
      "17                -5.16  2009  42.016877 -93.526748   ...           0   \n",
      "18                 9.09  2009  40.699070 -86.900038   ...           0   \n",
      "19                 4.25  2009  40.699070 -86.900038   ...           0   \n",
      "20                 4.79  2009  40.699070 -86.900038   ...           0   \n",
      "21                10.16  2009  40.699070 -86.900038   ...           0   \n",
      "22                 2.10  2009  40.699070 -86.900038   ...           0   \n",
      "23                 8.01  2009  40.699070 -86.900038   ...           0   \n",
      "24                 8.55  2009  40.699070 -86.900038   ...           0   \n",
      "25               -15.59  2009  41.272980 -91.666230   ...           0   \n",
      "26               -21.24  2009  41.272980 -91.666230   ...           0   \n",
      "27                 3.71  2009  40.699070 -86.900038   ...           0   \n",
      "28                 3.71  2009  40.699070 -86.900038   ...           0   \n",
      "29                 8.55  2009  40.699070 -86.900038   ...           0   \n",
      "...                 ...   ...        ...        ...   ...         ...   \n",
      "82006              3.63  2015  39.153770 -87.879040   ...           0   \n",
      "82007              4.46  2015  39.153770 -87.879040   ...           0   \n",
      "82008             -1.77  2015  39.153770 -87.879040   ...           0   \n",
      "82009             -0.01  2015  39.153770 -87.879040   ...           0   \n",
      "82010              5.29  2015  40.225240 -83.844521   ...           0   \n",
      "82011              0.81  2015  40.225240 -83.844521   ...           0   \n",
      "82012              4.03  2015  40.225240 -83.844521   ...           0   \n",
      "82013              3.90  2015  40.225240 -83.844521   ...           0   \n",
      "82014              6.03  2015  40.225240 -83.844521   ...           0   \n",
      "82015             -4.07  2015  40.225240 -83.844521   ...           0   \n",
      "82016             11.12  2015  39.697050 -89.962130   ...           0   \n",
      "82017              6.73  2015  39.697050 -89.962130   ...           0   \n",
      "82018              7.45  2015  39.697050 -89.962130   ...           0   \n",
      "82019              5.81  2015  39.697050 -89.962130   ...           0   \n",
      "82020              6.99  2015  39.697050 -89.962130   ...           0   \n",
      "82021              0.82  2015  39.153770 -87.879040   ...           0   \n",
      "82022              2.69  2015  39.153770 -87.879040   ...           0   \n",
      "82023              6.11  2015  39.153770 -87.879040   ...           0   \n",
      "82024              4.35  2015  39.153770 -87.879040   ...           0   \n",
      "82025              3.52  2015  39.153770 -87.879040   ...           0   \n",
      "82026              0.36  2015  39.697050 -89.962130   ...           0   \n",
      "82027             -0.74  2015  39.697050 -89.962130   ...           0   \n",
      "82028              7.27  2015  39.697050 -89.962130   ...           0   \n",
      "82029              3.71  2015  39.697050 -89.962130   ...           0   \n",
      "82030             15.03  2015  39.697050 -89.962130   ...           0   \n",
      "82031             -1.35  2015  39.153770 -87.879040   ...           0   \n",
      "82032             -3.53  2015  39.153770 -87.879040   ...           0   \n",
      "82033              4.14  2015  39.153770 -87.879040   ...           0   \n",
      "82034             -0.01  2015  39.153770 -87.879040   ...           0   \n",
      "82035             -0.42  2015  39.153770 -87.879040   ...           0   \n",
      "\n",
      "       V156574  V156642  V156763  V156774  V156783  V156786  V156797  V156806  \\\n",
      "0            0        0        0        0        0        0        0        0   \n",
      "1            0        0        0        0        0        0        0        0   \n",
      "2            0        0        0        0        0        0        0        0   \n",
      "3            0        0        0        0        0        0        0        0   \n",
      "4            0        0        0        0        0        0        0        0   \n",
      "5            0        0        0        0        0        0        0        0   \n",
      "6            0        0        0        0        0        0        0        0   \n",
      "7            0        0        0        0        0        0        0        0   \n",
      "8            0        0        0        0        0        0        0        0   \n",
      "9            0        0        0        0        0        0        0        0   \n",
      "10           0        0        0        0        0        0        0        0   \n",
      "11           0        0        0        0        0        0        0        0   \n",
      "12           0        0        0        0        0        0        0        0   \n",
      "13           0        0        0        0        0        0        0        0   \n",
      "14           0        0        0        0        0        0        0        0   \n",
      "15           0        0        0        0        0        0        0        0   \n",
      "16           0        0        0        0        0        0        0        0   \n",
      "17           0        0        0        0        0        0        0        0   \n",
      "18           0        0        0        0        0        0        0        0   \n",
      "19           0        0        0        0        0        0        0        0   \n",
      "20           0        0        0        0        0        0        0        0   \n",
      "21           0        0        0        0        0        0        0        0   \n",
      "22           0        0        0        0        0        0        0        0   \n",
      "23           0        0        0        0        0        0        0        0   \n",
      "24           0        0        0        0        0        0        0        0   \n",
      "25           0        0        0        0        0        0        0        0   \n",
      "26           0        0        0        0        0        0        0        0   \n",
      "27           0        0        0        0        0        0        0        0   \n",
      "28           0        0        0        0        0        0        0        0   \n",
      "29           0        0        0        0        0        0        0        0   \n",
      "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "82006        0        0        0        0        0        0        0        0   \n",
      "82007        0        0        0        0        0        0        0        0   \n",
      "82008        0        0        0        0        0        0        0        0   \n",
      "82009        0        0        0        0        0        0        0        0   \n",
      "82010        0        0        0        0        0        0        0        0   \n",
      "82011        0        0        0        0        0        0        0        0   \n",
      "82012        0        0        0        0        0        0        0        0   \n",
      "82013        0        0        0        0        0        0        0        0   \n",
      "82014        0        0        0        0        0        0        0        0   \n",
      "82015        0        0        0        0        0        0        0        0   \n",
      "82016        0        0        0        0        0        0        0        0   \n",
      "82017        0        0        0        0        0        0        0        0   \n",
      "82018        0        0        0        0        0        0        0        0   \n",
      "82019        0        0        0        0        0        0        0        0   \n",
      "82020        0        0        0        0        0        0        0        0   \n",
      "82021        0        0        0        0        0        0        0        0   \n",
      "82022        0        0        0        0        0        0        0        0   \n",
      "82023        0        0        0        0        0        0        0        0   \n",
      "82024        0        0        0        0        0        0        0        0   \n",
      "82025        0        0        0        0        0        0        0        0   \n",
      "82026        0        0        0        0        0        0        0        0   \n",
      "82027        0        0        0        0        0        0        0        0   \n",
      "82028        0        0        0        0        0        0        0        0   \n",
      "82029        0        0        0        0        0        0        0        0   \n",
      "82030        0        0        0        0        0        0        0        0   \n",
      "82031        0        0        0        0        0        0        0        0   \n",
      "82032        0        0        0        0        0        0        0        0   \n",
      "82033        0        0        0        0        0        0        0        0   \n",
      "82034        0        0        0        0        0        0        0        0   \n",
      "82035        0        0        0        0        0        0        0        0   \n",
      "\n",
      "       V156807  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "5            0  \n",
      "6            0  \n",
      "7            0  \n",
      "8            0  \n",
      "9            0  \n",
      "10           0  \n",
      "11           0  \n",
      "12           0  \n",
      "13           0  \n",
      "14           0  \n",
      "15           0  \n",
      "16           0  \n",
      "17           0  \n",
      "18           0  \n",
      "19           0  \n",
      "20           0  \n",
      "21           0  \n",
      "22           0  \n",
      "23           0  \n",
      "24           0  \n",
      "25           0  \n",
      "26           0  \n",
      "27           0  \n",
      "28           0  \n",
      "29           0  \n",
      "...        ...  \n",
      "82006        0  \n",
      "82007        0  \n",
      "82008        0  \n",
      "82009        0  \n",
      "82010        0  \n",
      "82011        0  \n",
      "82012        0  \n",
      "82013        0  \n",
      "82014        0  \n",
      "82015        0  \n",
      "82016        0  \n",
      "82017        0  \n",
      "82018        0  \n",
      "82019        0  \n",
      "82020        0  \n",
      "82021        0  \n",
      "82022        0  \n",
      "82023        0  \n",
      "82024        0  \n",
      "82025        0  \n",
      "82026        0  \n",
      "82027        0  \n",
      "82028        0  \n",
      "82029        0  \n",
      "82030        0  \n",
      "82031        0  \n",
      "82032        0  \n",
      "82033        0  \n",
      "82034        0  \n",
      "82035        0  \n",
      "\n",
      "[82036 rows x 196 columns]\n"
     ]
    }
   ],
   "source": [
    "# CURRENTLY NECESSARY IF: USING 174 ADDITIONAL VARIETY COLUMNS METHOD\n",
    "\n",
    "# THIS IS A DIFFERENT APPROACH TO THE ABOVE FOUR CELLS, WHERE WE HAVE 174 ADDITIONAL FEATURE COLUMNS\n",
    "# EACH WITH A 0 (IF IT IS NOT OF THAT VARIETY) OR A 1 (IF IT IS OF THAT VARIETY)\n",
    "\n",
    "# print(df)\n",
    "dummies = pd.get_dummies(df.Variety)\n",
    "# print(dummies)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP ALL THE CELLS THAT ARE NOT USABLE SUCH AS THE ONES THAT ARE STRINGS OR DATES\n",
    "\n",
    "df = df.drop(['Experiment', 'Location', 'Planting date', 'Check Yield', 'Yield difference', 'Latitude', 'Longitude', 'Variety', 'PI'], axis=1)\n",
    "df['YieldBucket'] = pd.Series(pd.qcut(df.Yield, q=3, labels=[\"high\", \"medium\", \"low\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We expect to be 0 nan values and there actually are 0 nan values\n",
      "\n",
      "Yield <class 'numpy.float64'>\n",
      "Year <class 'numpy.int64'>\n",
      "Temperature <class 'numpy.float64'>\n",
      "Precipitation <class 'numpy.float64'>\n",
      "Solar Radiation <class 'numpy.int64'>\n",
      "Soil class <class 'numpy.int64'>\n",
      "CEC <class 'numpy.float64'>\n",
      "Organic matter <class 'numpy.float64'>\n",
      "pH <class 'numpy.float64'>\n",
      "Clay <class 'numpy.float64'>\n",
      "Silt <class 'numpy.float64'>\n",
      "Sand <class 'numpy.float64'>\n",
      "Area <class 'numpy.float64'>\n",
      "V000016 <class 'numpy.uint8'>\n",
      "V000017 <class 'numpy.uint8'>\n",
      "V000018 <class 'numpy.uint8'>\n",
      "V000023 <class 'numpy.uint8'>\n",
      "V000024 <class 'numpy.uint8'>\n",
      "V000025 <class 'numpy.uint8'>\n",
      "V000030 <class 'numpy.uint8'>\n",
      "V000032 <class 'numpy.uint8'>\n",
      "V000034 <class 'numpy.uint8'>\n",
      "V000036 <class 'numpy.uint8'>\n",
      "V000039 <class 'numpy.uint8'>\n",
      "V000047 <class 'numpy.uint8'>\n",
      "V000050 <class 'numpy.uint8'>\n",
      "V000051 <class 'numpy.uint8'>\n",
      "V000058 <class 'numpy.uint8'>\n",
      "V000060 <class 'numpy.uint8'>\n",
      "V000062 <class 'numpy.uint8'>\n",
      "V000067 <class 'numpy.uint8'>\n",
      "V000070 <class 'numpy.uint8'>\n",
      "V000071 <class 'numpy.uint8'>\n",
      "V000075 <class 'numpy.uint8'>\n",
      "V000078 <class 'numpy.uint8'>\n",
      "V000079 <class 'numpy.uint8'>\n",
      "V000080 <class 'numpy.uint8'>\n",
      "V000081 <class 'numpy.uint8'>\n",
      "V000082 <class 'numpy.uint8'>\n",
      "V000092 <class 'numpy.uint8'>\n",
      "V000096 <class 'numpy.uint8'>\n",
      "V000098 <class 'numpy.uint8'>\n",
      "V000110 <class 'numpy.uint8'>\n",
      "V000115 <class 'numpy.uint8'>\n",
      "V000122 <class 'numpy.uint8'>\n",
      "V000123 <class 'numpy.uint8'>\n",
      "V000124 <class 'numpy.uint8'>\n",
      "V000125 <class 'numpy.uint8'>\n",
      "V000134 <class 'numpy.uint8'>\n",
      "V000147 <class 'numpy.uint8'>\n",
      "V000157 <class 'numpy.uint8'>\n",
      "V000172 <class 'numpy.uint8'>\n",
      "V000721 <class 'numpy.uint8'>\n",
      "V031243 <class 'numpy.uint8'>\n",
      "V051214 <class 'numpy.uint8'>\n",
      "V103132 <class 'numpy.uint8'>\n",
      "V103136 <class 'numpy.uint8'>\n",
      "V103139 <class 'numpy.uint8'>\n",
      "V103142 <class 'numpy.uint8'>\n",
      "V103150 <class 'numpy.uint8'>\n",
      "V103155 <class 'numpy.uint8'>\n",
      "V103156 <class 'numpy.uint8'>\n",
      "V103159 <class 'numpy.uint8'>\n",
      "V103163 <class 'numpy.uint8'>\n",
      "V103173 <class 'numpy.uint8'>\n",
      "V103193 <class 'numpy.uint8'>\n",
      "V103198 <class 'numpy.uint8'>\n",
      "V103259 <class 'numpy.uint8'>\n",
      "V103266 <class 'numpy.uint8'>\n",
      "V103273 <class 'numpy.uint8'>\n",
      "V103277 <class 'numpy.uint8'>\n",
      "V103281 <class 'numpy.uint8'>\n",
      "V103286 <class 'numpy.uint8'>\n",
      "V103293 <class 'numpy.uint8'>\n",
      "V103302 <class 'numpy.uint8'>\n",
      "V103303 <class 'numpy.uint8'>\n",
      "V103308 <class 'numpy.uint8'>\n",
      "V103332 <class 'numpy.uint8'>\n",
      "V103425 <class 'numpy.uint8'>\n",
      "V103466 <class 'numpy.uint8'>\n",
      "V103620 <class 'numpy.uint8'>\n",
      "V103624 <class 'numpy.uint8'>\n",
      "V103866 <class 'numpy.uint8'>\n",
      "V103970 <class 'numpy.uint8'>\n",
      "V104000 <class 'numpy.uint8'>\n",
      "V110768 <class 'numpy.uint8'>\n",
      "V110890 <class 'numpy.uint8'>\n",
      "V110923 <class 'numpy.uint8'>\n",
      "V111237 <class 'numpy.uint8'>\n",
      "V111336 <class 'numpy.uint8'>\n",
      "V113396 <class 'numpy.uint8'>\n",
      "V113424 <class 'numpy.uint8'>\n",
      "V113476 <class 'numpy.uint8'>\n",
      "V114530 <class 'numpy.uint8'>\n",
      "V114541 <class 'numpy.uint8'>\n",
      "V114545 <class 'numpy.uint8'>\n",
      "V114553 <class 'numpy.uint8'>\n",
      "V114564 <class 'numpy.uint8'>\n",
      "V114565 <class 'numpy.uint8'>\n",
      "V114655 <class 'numpy.uint8'>\n",
      "V114944 <class 'numpy.uint8'>\n",
      "V114951 <class 'numpy.uint8'>\n",
      "V119975 <class 'numpy.uint8'>\n",
      "V120038 <class 'numpy.uint8'>\n",
      "V120047 <class 'numpy.uint8'>\n",
      "V120246 <class 'numpy.uint8'>\n",
      "V120410 <class 'numpy.uint8'>\n",
      "V120585 <class 'numpy.uint8'>\n",
      "V120810 <class 'numpy.uint8'>\n",
      "V120912 <class 'numpy.uint8'>\n",
      "V120999 <class 'numpy.uint8'>\n",
      "V121015 <class 'numpy.uint8'>\n",
      "V121097 <class 'numpy.uint8'>\n",
      "V121140 <class 'numpy.uint8'>\n",
      "V121524 <class 'numpy.uint8'>\n",
      "V124343 <class 'numpy.uint8'>\n",
      "V130305 <class 'numpy.uint8'>\n",
      "V130307 <class 'numpy.uint8'>\n",
      "V130308 <class 'numpy.uint8'>\n",
      "V131675 <class 'numpy.uint8'>\n",
      "V131778 <class 'numpy.uint8'>\n",
      "V136868 <class 'numpy.uint8'>\n",
      "V137136 <class 'numpy.uint8'>\n",
      "V137147 <class 'numpy.uint8'>\n",
      "V137160 <class 'numpy.uint8'>\n",
      "V137237 <class 'numpy.uint8'>\n",
      "V137289 <class 'numpy.uint8'>\n",
      "V139094 <class 'numpy.uint8'>\n",
      "V139107 <class 'numpy.uint8'>\n",
      "V139548 <class 'numpy.uint8'>\n",
      "V140091 <class 'numpy.uint8'>\n",
      "V140364 <class 'numpy.uint8'>\n",
      "V140784 <class 'numpy.uint8'>\n",
      "V150834 <class 'numpy.uint8'>\n",
      "V150844 <class 'numpy.uint8'>\n",
      "V150847 <class 'numpy.uint8'>\n",
      "V150853 <class 'numpy.uint8'>\n",
      "V150974 <class 'numpy.uint8'>\n",
      "V151036 <class 'numpy.uint8'>\n",
      "V151236 <class 'numpy.uint8'>\n",
      "V151273 <class 'numpy.uint8'>\n",
      "V151284 <class 'numpy.uint8'>\n",
      "V151329 <class 'numpy.uint8'>\n",
      "V151331 <class 'numpy.uint8'>\n",
      "V151332 <class 'numpy.uint8'>\n",
      "V151333 <class 'numpy.uint8'>\n",
      "V151334 <class 'numpy.uint8'>\n",
      "V151336 <class 'numpy.uint8'>\n",
      "V151337 <class 'numpy.uint8'>\n",
      "V151340 <class 'numpy.uint8'>\n",
      "V151399 <class 'numpy.uint8'>\n",
      "V151407 <class 'numpy.uint8'>\n",
      "V152053 <class 'numpy.uint8'>\n",
      "V152061 <class 'numpy.uint8'>\n",
      "V152067 <class 'numpy.uint8'>\n",
      "V152079 <class 'numpy.uint8'>\n",
      "V152102 <class 'numpy.uint8'>\n",
      "V152253 <class 'numpy.uint8'>\n",
      "V152300 <class 'numpy.uint8'>\n",
      "V152312 <class 'numpy.uint8'>\n",
      "V152320 <class 'numpy.uint8'>\n",
      "V152322 <class 'numpy.uint8'>\n",
      "V152440 <class 'numpy.uint8'>\n",
      "V152734 <class 'numpy.uint8'>\n",
      "V152779 <class 'numpy.uint8'>\n",
      "V155180 <class 'numpy.uint8'>\n",
      "V155820 <class 'numpy.uint8'>\n",
      "V155842 <class 'numpy.uint8'>\n",
      "V155843 <class 'numpy.uint8'>\n",
      "V155918 <class 'numpy.uint8'>\n",
      "V156247 <class 'numpy.uint8'>\n",
      "V156305 <class 'numpy.uint8'>\n",
      "V156314 <class 'numpy.uint8'>\n",
      "V156367 <class 'numpy.uint8'>\n",
      "V156368 <class 'numpy.uint8'>\n",
      "V156516 <class 'numpy.uint8'>\n",
      "V156553 <class 'numpy.uint8'>\n",
      "V156565 <class 'numpy.uint8'>\n",
      "V156574 <class 'numpy.uint8'>\n",
      "V156642 <class 'numpy.uint8'>\n",
      "V156763 <class 'numpy.uint8'>\n",
      "V156774 <class 'numpy.uint8'>\n",
      "V156783 <class 'numpy.uint8'>\n",
      "V156786 <class 'numpy.uint8'>\n",
      "V156797 <class 'numpy.uint8'>\n",
      "V156806 <class 'numpy.uint8'>\n",
      "V156807 <class 'numpy.uint8'>\n",
      "YieldBucket <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# LET US ALSO MAKE SURE THERE ARE NO NAN IN THE DATA\n",
    "print(\"We expect to be %s nan values and there actually are %s nan values\\n\" % (0, np.sum(df.isnull().sum())))\n",
    "# AFTER COLUMNS, MAKE SURE NO SKETCHY ONES\n",
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'Temperature', 'Precipitation', 'Solar Radiation', 'Soil class',\n",
      "       'CEC', 'Organic matter', 'pH', 'Clay', 'Silt',\n",
      "       ...\n",
      "       'V156565', 'V156574', 'V156642', 'V156763', 'V156774', 'V156783',\n",
      "       'V156786', 'V156797', 'V156806', 'V156807'],\n",
      "      dtype='object', length=186)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "\n",
    "# feature_columns = ['Variety', 'Solar Radiation', 'Temperature', 'Precipitation', 'Location', 'Clay', 'Silt', 'Sand', 'pH', 'Precipitation', 'CEC', 'Soil class']\n",
    "\n",
    "X = df.drop(['Yield', 'YieldBucket'], axis=1)\n",
    "\n",
    "print(X.columns)\n",
    "# X = df.loc[:, feature_columns]\n",
    "\n",
    "y = df.Yield\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, train_size = 0.1, random_state = 42)\n",
    "\n",
    "# train_visual, zz, zzz, zzzz = train_test_split(df, y, test_size=0.2, train_size=0.8, random_state = 42)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-dbf4d5c533b7>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-dbf4d5c533b7>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    avg_error_vector = np.absolute(((preds - y_test) / y_test) * 100))\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def evaluate_errors(prediction, actual):\n",
    "    print(\"RMSE Error: \", np.sqrt(mean_squared_error(prediction, actual)))\n",
    "    avg_error_vector = np.absolute(((preds - y_test) / y_test) * 100))\n",
    "    print(\"Average Error: \", np.mean(avg_error_vector))\n",
    "    print(\"Average Error std: \", avg_error_vector.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 53.13332545359653\n",
      "10.005838207423524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(n_estimators=10, max_depth=20, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(preds, y_test)\n",
    "\n",
    "errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "# erros = \n",
    "# print(errors)\n",
    "print(\"mse:\", mse)\n",
    "print(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V000030 has importance:  0.0\n",
      "V140784 has importance:  1.206935965562934e-05\n",
      "V110923 has importance:  1.5094549292347343e-05\n",
      "V151340 has importance:  2.3597708000294508e-05\n",
      "V151331 has importance:  3.685303763063591e-05\n",
      "V103308 has importance:  3.805664579449813e-05\n",
      "V137289 has importance:  3.8592331805538116e-05\n",
      "V155180 has importance:  5.005169316426522e-05\n",
      "V152779 has importance:  5.354413365757188e-05\n",
      "V139094 has importance:  5.6237778946373004e-05\n",
      "V000017 has importance:  6.27706373232473e-05\n",
      "V121524 has importance:  6.619417827328127e-05\n",
      "V103332 has importance:  6.681548513345065e-05\n",
      "V156783 has importance:  6.906792443807952e-05\n",
      "V114944 has importance:  9.306059238104696e-05\n",
      "V151336 has importance:  9.317815692431577e-05\n",
      "V000036 has importance:  9.320209313144743e-05\n",
      "V113424 has importance:  9.772856666629215e-05\n",
      "V000070 has importance:  0.00010775266059769694\n",
      "V031243 has importance:  0.00011526433297735909\n",
      "V120585 has importance:  0.00011959113313673203\n",
      "V103193 has importance:  0.00012528959239342877\n",
      "V140091 has importance:  0.0001490285199293413\n",
      "V151337 has importance:  0.00015732813743043905\n",
      "V156367 has importance:  0.0001627220933833279\n",
      "V121140 has importance:  0.00016289678925430535\n",
      "V151332 has importance:  0.00017621575061782334\n",
      "V151329 has importance:  0.0001777073560608017\n",
      "V131778 has importance:  0.00017930361584550292\n",
      "V151407 has importance:  0.00019973910203964894\n",
      "V114951 has importance:  0.00020681523506454482\n",
      "V000075 has importance:  0.00020804089874283584\n",
      "V121097 has importance:  0.00021752888005822265\n",
      "V000096 has importance:  0.00021755557880500032\n",
      "V152061 has importance:  0.00022686609408003672\n",
      "V000078 has importance:  0.0002294814490590507\n",
      "V051214 has importance:  0.0002398664557479471\n",
      "V156574 has importance:  0.0002421519064392963\n",
      "V110768 has importance:  0.00025260731349576566\n",
      "V156763 has importance:  0.00026147442001947334\n",
      "V103303 has importance:  0.0002616402161217758\n",
      "V114541 has importance:  0.00026625430583363115\n",
      "V103970 has importance:  0.00027118381074566844\n",
      "V152734 has importance:  0.00029284081259154125\n",
      "V103173 has importance:  0.0002959903700966396\n",
      "V120912 has importance:  0.00030019531293102106\n",
      "V000125 has importance:  0.0003154488554327756\n",
      "V150974 has importance:  0.0003322311359973541\n",
      "V000721 has importance:  0.0003399108432554093\n",
      "V000115 has importance:  0.00034057663230370125\n",
      "V103266 has importance:  0.0003542817022320201\n",
      "V151273 has importance:  0.00035545524291914743\n",
      "V151334 has importance:  0.00035685373377891027\n",
      "V114530 has importance:  0.00035904021308189407\n",
      "V000080 has importance:  0.00036486211614559854\n",
      "V151236 has importance:  0.0003674900547519798\n",
      "V121015 has importance:  0.0003832054082105954\n",
      "V103159 has importance:  0.0003958706896597452\n",
      "V000147 has importance:  0.0004037119583990939\n",
      "V156565 has importance:  0.0004144201546636352\n",
      "V152320 has importance:  0.00042175071671412667\n",
      "V000122 has importance:  0.00043325938175288605\n",
      "V000050 has importance:  0.0004472994741933353\n",
      "V152102 has importance:  0.00044781602832438583\n",
      "V152312 has importance:  0.00045073900456825975\n",
      "V137147 has importance:  0.0004534113818376066\n",
      "V113476 has importance:  0.0004632107873353132\n",
      "V152322 has importance:  0.000477236529815671\n",
      "V000079 has importance:  0.0004813463828894013\n",
      "V103259 has importance:  0.0004916298781526432\n",
      "V114545 has importance:  0.000502330510149516\n",
      "V130305 has importance:  0.0005168445285268422\n",
      "V000032 has importance:  0.0005180594031895933\n",
      "V152300 has importance:  0.0005332084286174873\n",
      "V124343 has importance:  0.0005462164539339138\n",
      "V000082 has importance:  0.000546755298879391\n",
      "V151333 has importance:  0.0005628796293199632\n",
      "V000016 has importance:  0.0005694227163868207\n",
      "V156807 has importance:  0.000615992262117585\n",
      "V103466 has importance:  0.0006226445439829861\n",
      "V000110 has importance:  0.000627063401470588\n",
      "V120999 has importance:  0.0006473896418483099\n",
      "V152067 has importance:  0.0006616932115690273\n",
      "V000157 has importance:  0.00067011655590539\n",
      "V137160 has importance:  0.0007132515624934191\n",
      "V151036 has importance:  0.0007175417011873669\n",
      "V103286 has importance:  0.0007402976899241672\n",
      "V103273 has importance:  0.0007915464600589783\n",
      "V000071 has importance:  0.0008024791865183869\n",
      "V103155 has importance:  0.0008219881182713453\n",
      "V151284 has importance:  0.0008299527270977422\n",
      "V113396 has importance:  0.0008570232704888228\n",
      "V152440 has importance:  0.0008690089321604294\n",
      "V156786 has importance:  0.0008696759048078169\n",
      "V103156 has importance:  0.0008728166155595769\n",
      "V000018 has importance:  0.0008757692492898466\n",
      "V000058 has importance:  0.0008929508314891165\n",
      "V000098 has importance:  0.000923590481920898\n",
      "V110890 has importance:  0.0009251519964150774\n",
      "V114553 has importance:  0.0009564678183133166\n",
      "V000039 has importance:  0.0009809575921737638\n",
      "V140364 has importance:  0.0010371522832589124\n",
      "V000067 has importance:  0.001112570953415866\n",
      "V103425 has importance:  0.0011202827038484498\n",
      "V103277 has importance:  0.001128034815635342\n",
      "V103132 has importance:  0.0011559115896039472\n",
      "V000172 has importance:  0.0011563227505411416\n",
      "V000051 has importance:  0.0011585569610562713\n",
      "V103302 has importance:  0.001173326939198883\n",
      "V000034 has importance:  0.0011806862885611624\n",
      "V137136 has importance:  0.0011807288299828763\n",
      "V103139 has importance:  0.001211534299494892\n",
      "V120038 has importance:  0.0012121971309285746\n",
      "V120410 has importance:  0.001228505445409665\n",
      "V000134 has importance:  0.0012328455172134017\n",
      "V000023 has importance:  0.0012495978970591159\n",
      "V103620 has importance:  0.0012795587016504039\n",
      "V120810 has importance:  0.001338529209566662\n",
      "V103198 has importance:  0.0014259851002397296\n",
      "V103624 has importance:  0.0014571041655158435\n",
      "V131675 has importance:  0.0014822122243527396\n",
      "V000092 has importance:  0.0015173993873711\n",
      "V103281 has importance:  0.001546193136539317\n",
      "V000060 has importance:  0.001556095476113947\n",
      "V156797 has importance:  0.0016191356251341442\n",
      "V119975 has importance:  0.0016449556203571903\n",
      "V156516 has importance:  0.001672917553717086\n",
      "V000081 has importance:  0.0016863958518438158\n",
      "V114565 has importance:  0.001754679106745653\n",
      "V000024 has importance:  0.001784665110980738\n",
      "V152253 has importance:  0.001804585264394817\n",
      "V000124 has importance:  0.001841362499149785\n",
      "V103866 has importance:  0.00188240502440973\n",
      "V150834 has importance:  0.0019208392766471541\n",
      "V151399 has importance:  0.0019377230422169922\n",
      "V114655 has importance:  0.001996291116958456\n",
      "V103293 has importance:  0.002049824562270794\n",
      "V137237 has importance:  0.0020768523970647445\n",
      "V103136 has importance:  0.0021959653493092935\n",
      "V152079 has importance:  0.00229492527499747\n",
      "V136868 has importance:  0.0023027102180872934\n",
      "V000062 has importance:  0.002343213633239897\n",
      "V156774 has importance:  0.002599896789294414\n",
      "V139107 has importance:  0.002618588122233396\n",
      "V150853 has importance:  0.002624114201778223\n",
      "V152053 has importance:  0.002644513836587246\n",
      "V156314 has importance:  0.002744810303112002\n",
      "V104000 has importance:  0.0028127970304659476\n",
      "V155918 has importance:  0.002817136353107183\n",
      "V000047 has importance:  0.0029240437821485645\n",
      "V114564 has importance:  0.0030188756769184456\n",
      "V150844 has importance:  0.0030704124832302863\n",
      "V130307 has importance:  0.003079479172387915\n",
      "V130308 has importance:  0.0031095547537052805\n",
      "V120047 has importance:  0.003193827836279137\n",
      "V156553 has importance:  0.0032421859512765687\n",
      "V150847 has importance:  0.003311057347103054\n",
      "V156305 has importance:  0.0035092857517977473\n",
      "V000025 has importance:  0.003763993601003208\n",
      "V103150 has importance:  0.0039628908824025765\n",
      "V111237 has importance:  0.004036191910799632\n",
      "V139548 has importance:  0.004040925237468502\n",
      "V155820 has importance:  0.004137963492932342\n",
      "V111336 has importance:  0.0045304620068261445\n",
      "V156247 has importance:  0.004566957992135904\n",
      "V000123 has importance:  0.004581515521719574\n",
      "V156642 has importance:  0.0051084125446957266\n",
      "V156368 has importance:  0.005485053917979767\n",
      "V120246 has importance:  0.005687783474081219\n",
      "V155843 has importance:  0.006024273140477688\n",
      "V156806 has importance:  0.0060296678067602175\n",
      "V103142 has importance:  0.00742803416480369\n",
      "V155842 has importance:  0.010263105992457408\n",
      "V103163 has importance:  0.010452424210800273\n",
      "Soil class has importance:  0.018836909601833825\n",
      "pH has importance:  0.03623450560089099\n",
      "Sand has importance:  0.03631295403775019\n",
      "Clay has importance:  0.04524769741969042\n",
      "CEC has importance:  0.052702927128553886\n",
      "Area has importance:  0.05852921203447801\n",
      "Organic matter has importance:  0.06319494235996584\n",
      "Solar Radiation has importance:  0.0734023786070973\n",
      "Temperature has importance:  0.07615095416309085\n",
      "Year has importance:  0.08153415738898684\n",
      "Precipitation has importance:  0.08619818110068501\n",
      "Silt has importance:  0.13126257488792414\n",
      "count    4102.000000\n",
      "mean       10.005838\n",
      "std        10.290940\n",
      "min         0.000096\n",
      "25%         3.492916\n",
      "50%         7.430850\n",
      "75%        13.260937\n",
      "max       138.164462\n",
      "Name: Yield, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# GET OUTPUT OF FEATURE IMPORTANCE\n",
    "\n",
    "feature_importances = regr.feature_importances_\n",
    "feature_importances = pd.Series(feature_importances)\n",
    "feature_importance_df = pd.DataFrame({'feature': X_train.columns,'feature_importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by=['feature_importance'])\n",
    "for index, row in feature_importance_df.iterrows():\n",
    "    print(row['feature'], 'has importance: ', row['feature_importance'])\n",
    "# for feature_importance in regr.feature_importances_:\n",
    "    \n",
    "print(errors.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-981038a4e9e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mregr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_class_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_y_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mDOUBLE\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_validate_y_class_weight\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_y_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "# THIS WILL ONLY WORK WITH THE BUCKET METHOD\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regr = RandomForestClassifier(n_estimators=10, max_depth=20, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "classifiers = [\n",
    "    svm.SVR(),\n",
    "    MLPRegressor(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(5, 2), random_state=1),\n",
    "    linear_model.SGDRegressor(),\n",
    "    linear_model.BayesianRidge(),\n",
    "    linear_model.LassoLars(),\n",
    "#     linear_model.ARDRegression(),\n",
    "#     linear_model.ARDRegression(),\n",
    "    linear_model.PassiveAggressiveRegressor(),\n",
    "    linear_model.TheilSenRegressor(),\n",
    "    linear_model.LinearRegression()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# estimator = svm.SVR(kernel=\"linear\")\n",
    "\n",
    "# selector = RFECV(estimator, step=1, cv=5, verbose=1)\n",
    "# selector = selector.fit(X_train, y_train)\n",
    "# selector.support_ \n",
    "# # array([ True,  True,  True,  True,  True,\n",
    "# #         False, False, False, False, False], dtype=bool)\n",
    "# selector.ranking_\n",
    "# # array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "\n",
    "\n",
    "#     print(np.sum(preds - y_test))\n",
    "#     print(clf.predict(X_test),'\\n')\n",
    "#     print(y_test)\n",
    "#     print('accuracy score:', accuracy_score(y_test, clf.predict(X_test)), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "67.91605839333502\n",
      "MLPRegressor(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "132.20461002435297\n",
      "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
      "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "       loss='squared_loss', max_iter=None, n_iter=None, penalty='l2',\n",
      "       power_t=0.25, random_state=None, shuffle=True, tol=None, verbose=0,\n",
      "       warm_start=False)\n",
      "3.536828786322753e+41\n",
      "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "       normalize=False, tol=0.001, verbose=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.77074625717506\n",
      "LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16,\n",
      "     fit_intercept=True, fit_path=True, max_iter=500, normalize=True,\n",
      "     positive=False, precompute='auto', verbose=False)\n",
      "132.2046100236892\n",
      "ARDRegression(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "       normalize=False, threshold_lambda=10000.0, tol=0.001, verbose=False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cd24bd4a6bb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/linear_model/bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    457\u001b[0m                            np.dot(X[:, keep_lambda] *\n\u001b[1;32m    458\u001b[0m                            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep_lambda\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m                            X[:, keep_lambda].T))\n\u001b[0m\u001b[1;32m    460\u001b[0m             sigma_ = np.dot(sigma_, X[:, keep_lambda] *\n\u001b[1;32m    461\u001b[0m                             np.reshape(1. / lambda_[keep_lambda], [1, -1]))\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/scipy/linalg/basic.py\u001b[0m in \u001b[0;36mpinvh\u001b[0;34m(a, cond, rcond, lower, return_rank, check_finite)\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \"\"\"\n\u001b[1;32m   1451\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1452\u001b[0;31m     \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrcond\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/scipy/linalg/decomp.py\u001b[0m in \u001b[0;36meigh\u001b[0;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meigvals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             w, v, info = evr(a1, uplo=uplo, jobz=_job, range=\"A\", il=1,\n\u001b[0;32m--> 432\u001b[0;31m                              iu=a1.shape[0], overwrite_a=overwrite_a)\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigvals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    errors = mean_squared_error(preds, y_test)\n",
    "#     errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(errors)\n",
    "    print(errors)\n",
    "#     print(errors.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "#     KNeighborsClassifier(3),\n",
    "#     SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "from sklearn.metrics import accuracy_score\n",
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(scale(X_train), y_train)\n",
    "    preds = clf.predict(scale(X_test))\n",
    "    print(accuracy_score(y_test, preds))\n",
    "#     errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(errors)\n",
    "#     print(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
