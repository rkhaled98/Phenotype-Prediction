{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# READ THE CSV INTO DATAFRAME\n",
    "\n",
    "df = pd.read_csv('Syngenta/Syngenta_2017/Experiment_dataset.csv')\n",
    "\n",
    "np_ar = np.asarray(df)\n",
    "# df2 = pd.read_csv('Syngenta/Syngenta_2017/Region_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.Temperature.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Experiment  Location  Variety Planting date  Yield  Check Yield  \\\n",
      "0      09SUBWYG201      2291  V000047       5/20/09  55.62        53.61   \n",
      "1      09SUBWYG202      2291  V000047       5/20/09  53.72        53.61   \n",
      "2      09SUBWYG203      2291  V000047       5/20/09  49.09        53.61   \n",
      "3      09SUBWYG204      2291  V000047       5/20/09  53.00        53.61   \n",
      "4      09SUBWYG301      2290  V130307        5/8/09  44.51        57.32   \n",
      "5      09SUBWYG301      2290  V000025        5/8/09  47.41        57.32   \n",
      "6      09SUBWYG302      2290  V000024        5/8/09  49.15        57.32   \n",
      "7      09SUBWYG302      2290  V130307        5/8/09  43.23        57.32   \n",
      "8      09SUBWYG302      2290  V000124        5/8/09  50.99        57.32   \n",
      "9      09SUBWYG302      2290  V000025        5/8/09  51.32        57.32   \n",
      "10     09SUBWYG302      3490  V000024       5/26/09  70.99        61.90   \n",
      "11     09SUBWYG302      3490  V130307       5/26/09  65.61        61.90   \n",
      "12     09SUBWYG302      3490  V000124       5/26/09  68.84        61.90   \n",
      "13     09SUBWYG302      3490  V000025       5/26/09  74.75        61.90   \n",
      "14     09SUBWYG304      2290  V139094        5/8/09  48.81        57.32   \n",
      "15     09SUBWYG304      2290  V000024        5/8/09  45.01        57.32   \n",
      "16     09SUBWYG304      2290  V000023        5/8/09  50.71        57.32   \n",
      "17     09SUBWYG304      2290  V000124        5/8/09  52.16        57.32   \n",
      "18     09SUBWYG304      3490  V139094       5/26/09  70.99        61.90   \n",
      "19     09SUBWYG304      3490  V000024       5/26/09  66.15        61.90   \n",
      "20     09SUBWYG304      3490  V000023       5/26/09  66.69        61.90   \n",
      "21     09SUBWYG304      3490  V000124       5/26/09  72.06        61.90   \n",
      "22     09SUBWYG306      3490  V000125       5/26/09  64.00        61.90   \n",
      "23     09SUBWYG306      3490  V000051       5/26/09  69.91        61.90   \n",
      "24     09SUBWYG306      3490  V000025       5/26/09  70.45        61.90   \n",
      "25     09SUBWYG309      3210  V000051       5/11/09  48.54        64.12   \n",
      "26     09SUBWYG309      3210  V000025       5/11/09  42.89        64.12   \n",
      "27     09SUBWYG309      3490  V000051       5/28/09  65.61        61.90   \n",
      "28     09SUBWYG309      3490  V000025       5/28/09  65.61        61.90   \n",
      "29     09SUBWYG310      3490  V000051       5/28/09  70.45        61.90   \n",
      "...            ...       ...      ...           ...    ...          ...   \n",
      "82006  15SUSGYGY23      3395  V137136        6/6/15  59.12        55.49   \n",
      "82007  15SUSGYGY23      3395  V113396        6/6/15  59.95        55.49   \n",
      "82008  15SUSGYGY23      3395  V152440        6/6/15  53.73        55.49   \n",
      "82009  15SUSGYGY23      3395  V120047        6/6/15  55.49        55.49   \n",
      "82010  15SUSGYGY23      3435  V152779        6/5/15  58.64        53.36   \n",
      "82011  15SUSGYGY23      3435  V121015        6/5/15  54.16        53.36   \n",
      "82012  15SUSGYGY23      3435  V137136        6/5/15  57.39        53.36   \n",
      "82013  15SUSGYGY23      3435  V113396        6/5/15  57.25        53.36   \n",
      "82014  15SUSGYGY23      3435  V152440        6/5/15  59.38        53.36   \n",
      "82015  15SUSGYGY23      3435  V120047        6/5/15  49.29        53.36   \n",
      "82016  15SUSGYGY24      3325  V121015        6/6/15  69.85        58.72   \n",
      "82017  15SUSGYGY24      3325  V137136        6/6/15  65.46        58.72   \n",
      "82018  15SUSGYGY24      3325  V113396        6/6/15  66.17        58.72   \n",
      "82019  15SUSGYGY24      3325  V152440        6/6/15  64.53        58.72   \n",
      "82020  15SUSGYGY24      3325  V136868        6/6/15  65.71        58.72   \n",
      "82021  15SUSGYGY24      3395  V121015        6/6/15  56.32        55.49   \n",
      "82022  15SUSGYGY24      3395  V137136        6/6/15  58.19        55.49   \n",
      "82023  15SUSGYGY24      3395  V113396        6/6/15  61.61        55.49   \n",
      "82024  15SUSGYGY24      3395  V152440        6/6/15  59.84        55.49   \n",
      "82025  15SUSGYGY24      3395  V136868        6/6/15  59.01        55.49   \n",
      "82026  15SUSGYGY25      3325  V121015        6/6/15  59.09        58.72   \n",
      "82027  15SUSGYGY25      3325  V137136        6/6/15  57.98        58.72   \n",
      "82028  15SUSGYGY25      3325  V113396        6/6/15  65.99        58.72   \n",
      "82029  15SUSGYGY25      3325  V152440        6/6/15  62.43        58.72   \n",
      "82030  15SUSGYGY25      3325  V136868        6/6/15  73.75        58.72   \n",
      "82031  15SUSGYGY25      3395  V121015        6/6/15  54.14        55.49   \n",
      "82032  15SUSGYGY25      3395  V137136        6/6/15  51.96        55.49   \n",
      "82033  15SUSGYGY25      3395  V113396        6/6/15  59.64        55.49   \n",
      "82034  15SUSGYGY25      3395  V152440        6/6/15  55.49        55.49   \n",
      "82035  15SUSGYGY25      3395  V136868        6/6/15  55.07        55.49   \n",
      "\n",
      "       Yield difference  Year   Latitude  Longitude   ...     V156565  \\\n",
      "0                  2.01  2009  42.019111 -93.525735   ...           0   \n",
      "1                  0.11  2009  42.019111 -93.525735   ...           0   \n",
      "2                 -4.52  2009  42.019111 -93.525735   ...           0   \n",
      "3                 -0.61  2009  42.019111 -93.525735   ...           0   \n",
      "4                -12.81  2009  42.016877 -93.526748   ...           0   \n",
      "5                 -9.90  2009  42.016877 -93.526748   ...           0   \n",
      "6                 -8.17  2009  42.016877 -93.526748   ...           0   \n",
      "7                -14.09  2009  42.016877 -93.526748   ...           0   \n",
      "8                 -6.33  2009  42.016877 -93.526748   ...           0   \n",
      "9                 -5.99  2009  42.016877 -93.526748   ...           0   \n",
      "10                 9.09  2009  40.699070 -86.900038   ...           0   \n",
      "11                 3.71  2009  40.699070 -86.900038   ...           0   \n",
      "12                 6.94  2009  40.699070 -86.900038   ...           0   \n",
      "13                12.85  2009  40.699070 -86.900038   ...           0   \n",
      "14                -8.51  2009  42.016877 -93.526748   ...           0   \n",
      "15               -12.30  2009  42.016877 -93.526748   ...           0   \n",
      "16                -6.61  2009  42.016877 -93.526748   ...           0   \n",
      "17                -5.16  2009  42.016877 -93.526748   ...           0   \n",
      "18                 9.09  2009  40.699070 -86.900038   ...           0   \n",
      "19                 4.25  2009  40.699070 -86.900038   ...           0   \n",
      "20                 4.79  2009  40.699070 -86.900038   ...           0   \n",
      "21                10.16  2009  40.699070 -86.900038   ...           0   \n",
      "22                 2.10  2009  40.699070 -86.900038   ...           0   \n",
      "23                 8.01  2009  40.699070 -86.900038   ...           0   \n",
      "24                 8.55  2009  40.699070 -86.900038   ...           0   \n",
      "25               -15.59  2009  41.272980 -91.666230   ...           0   \n",
      "26               -21.24  2009  41.272980 -91.666230   ...           0   \n",
      "27                 3.71  2009  40.699070 -86.900038   ...           0   \n",
      "28                 3.71  2009  40.699070 -86.900038   ...           0   \n",
      "29                 8.55  2009  40.699070 -86.900038   ...           0   \n",
      "...                 ...   ...        ...        ...   ...         ...   \n",
      "82006              3.63  2015  39.153770 -87.879040   ...           0   \n",
      "82007              4.46  2015  39.153770 -87.879040   ...           0   \n",
      "82008             -1.77  2015  39.153770 -87.879040   ...           0   \n",
      "82009             -0.01  2015  39.153770 -87.879040   ...           0   \n",
      "82010              5.29  2015  40.225240 -83.844521   ...           0   \n",
      "82011              0.81  2015  40.225240 -83.844521   ...           0   \n",
      "82012              4.03  2015  40.225240 -83.844521   ...           0   \n",
      "82013              3.90  2015  40.225240 -83.844521   ...           0   \n",
      "82014              6.03  2015  40.225240 -83.844521   ...           0   \n",
      "82015             -4.07  2015  40.225240 -83.844521   ...           0   \n",
      "82016             11.12  2015  39.697050 -89.962130   ...           0   \n",
      "82017              6.73  2015  39.697050 -89.962130   ...           0   \n",
      "82018              7.45  2015  39.697050 -89.962130   ...           0   \n",
      "82019              5.81  2015  39.697050 -89.962130   ...           0   \n",
      "82020              6.99  2015  39.697050 -89.962130   ...           0   \n",
      "82021              0.82  2015  39.153770 -87.879040   ...           0   \n",
      "82022              2.69  2015  39.153770 -87.879040   ...           0   \n",
      "82023              6.11  2015  39.153770 -87.879040   ...           0   \n",
      "82024              4.35  2015  39.153770 -87.879040   ...           0   \n",
      "82025              3.52  2015  39.153770 -87.879040   ...           0   \n",
      "82026              0.36  2015  39.697050 -89.962130   ...           0   \n",
      "82027             -0.74  2015  39.697050 -89.962130   ...           0   \n",
      "82028              7.27  2015  39.697050 -89.962130   ...           0   \n",
      "82029              3.71  2015  39.697050 -89.962130   ...           0   \n",
      "82030             15.03  2015  39.697050 -89.962130   ...           0   \n",
      "82031             -1.35  2015  39.153770 -87.879040   ...           0   \n",
      "82032             -3.53  2015  39.153770 -87.879040   ...           0   \n",
      "82033              4.14  2015  39.153770 -87.879040   ...           0   \n",
      "82034             -0.01  2015  39.153770 -87.879040   ...           0   \n",
      "82035             -0.42  2015  39.153770 -87.879040   ...           0   \n",
      "\n",
      "       V156574  V156642  V156763  V156774  V156783  V156786  V156797  V156806  \\\n",
      "0            0        0        0        0        0        0        0        0   \n",
      "1            0        0        0        0        0        0        0        0   \n",
      "2            0        0        0        0        0        0        0        0   \n",
      "3            0        0        0        0        0        0        0        0   \n",
      "4            0        0        0        0        0        0        0        0   \n",
      "5            0        0        0        0        0        0        0        0   \n",
      "6            0        0        0        0        0        0        0        0   \n",
      "7            0        0        0        0        0        0        0        0   \n",
      "8            0        0        0        0        0        0        0        0   \n",
      "9            0        0        0        0        0        0        0        0   \n",
      "10           0        0        0        0        0        0        0        0   \n",
      "11           0        0        0        0        0        0        0        0   \n",
      "12           0        0        0        0        0        0        0        0   \n",
      "13           0        0        0        0        0        0        0        0   \n",
      "14           0        0        0        0        0        0        0        0   \n",
      "15           0        0        0        0        0        0        0        0   \n",
      "16           0        0        0        0        0        0        0        0   \n",
      "17           0        0        0        0        0        0        0        0   \n",
      "18           0        0        0        0        0        0        0        0   \n",
      "19           0        0        0        0        0        0        0        0   \n",
      "20           0        0        0        0        0        0        0        0   \n",
      "21           0        0        0        0        0        0        0        0   \n",
      "22           0        0        0        0        0        0        0        0   \n",
      "23           0        0        0        0        0        0        0        0   \n",
      "24           0        0        0        0        0        0        0        0   \n",
      "25           0        0        0        0        0        0        0        0   \n",
      "26           0        0        0        0        0        0        0        0   \n",
      "27           0        0        0        0        0        0        0        0   \n",
      "28           0        0        0        0        0        0        0        0   \n",
      "29           0        0        0        0        0        0        0        0   \n",
      "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "82006        0        0        0        0        0        0        0        0   \n",
      "82007        0        0        0        0        0        0        0        0   \n",
      "82008        0        0        0        0        0        0        0        0   \n",
      "82009        0        0        0        0        0        0        0        0   \n",
      "82010        0        0        0        0        0        0        0        0   \n",
      "82011        0        0        0        0        0        0        0        0   \n",
      "82012        0        0        0        0        0        0        0        0   \n",
      "82013        0        0        0        0        0        0        0        0   \n",
      "82014        0        0        0        0        0        0        0        0   \n",
      "82015        0        0        0        0        0        0        0        0   \n",
      "82016        0        0        0        0        0        0        0        0   \n",
      "82017        0        0        0        0        0        0        0        0   \n",
      "82018        0        0        0        0        0        0        0        0   \n",
      "82019        0        0        0        0        0        0        0        0   \n",
      "82020        0        0        0        0        0        0        0        0   \n",
      "82021        0        0        0        0        0        0        0        0   \n",
      "82022        0        0        0        0        0        0        0        0   \n",
      "82023        0        0        0        0        0        0        0        0   \n",
      "82024        0        0        0        0        0        0        0        0   \n",
      "82025        0        0        0        0        0        0        0        0   \n",
      "82026        0        0        0        0        0        0        0        0   \n",
      "82027        0        0        0        0        0        0        0        0   \n",
      "82028        0        0        0        0        0        0        0        0   \n",
      "82029        0        0        0        0        0        0        0        0   \n",
      "82030        0        0        0        0        0        0        0        0   \n",
      "82031        0        0        0        0        0        0        0        0   \n",
      "82032        0        0        0        0        0        0        0        0   \n",
      "82033        0        0        0        0        0        0        0        0   \n",
      "82034        0        0        0        0        0        0        0        0   \n",
      "82035        0        0        0        0        0        0        0        0   \n",
      "\n",
      "       V156807  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "5            0  \n",
      "6            0  \n",
      "7            0  \n",
      "8            0  \n",
      "9            0  \n",
      "10           0  \n",
      "11           0  \n",
      "12           0  \n",
      "13           0  \n",
      "14           0  \n",
      "15           0  \n",
      "16           0  \n",
      "17           0  \n",
      "18           0  \n",
      "19           0  \n",
      "20           0  \n",
      "21           0  \n",
      "22           0  \n",
      "23           0  \n",
      "24           0  \n",
      "25           0  \n",
      "26           0  \n",
      "27           0  \n",
      "28           0  \n",
      "29           0  \n",
      "...        ...  \n",
      "82006        0  \n",
      "82007        0  \n",
      "82008        0  \n",
      "82009        0  \n",
      "82010        0  \n",
      "82011        0  \n",
      "82012        0  \n",
      "82013        0  \n",
      "82014        0  \n",
      "82015        0  \n",
      "82016        0  \n",
      "82017        0  \n",
      "82018        0  \n",
      "82019        0  \n",
      "82020        0  \n",
      "82021        0  \n",
      "82022        0  \n",
      "82023        0  \n",
      "82024        0  \n",
      "82025        0  \n",
      "82026        0  \n",
      "82027        0  \n",
      "82028        0  \n",
      "82029        0  \n",
      "82030        0  \n",
      "82031        0  \n",
      "82032        0  \n",
      "82033        0  \n",
      "82034        0  \n",
      "82035        0  \n",
      "\n",
      "[82036 rows x 196 columns]\n"
     ]
    }
   ],
   "source": [
    "# CURRENTLY NECESSARY IF: USING 174 ADDITIONAL VARIETY COLUMNS METHOD\n",
    "\n",
    "# THIS IS A DIFFERENT APPROACH TO THE ABOVE FOUR CELLS, WHERE WE HAVE 174 ADDITIONAL FEATURE COLUMNS\n",
    "# EACH WITH A 0 (IF IT IS NOT OF THAT VARIETY) OR A 1 (IF IT IS OF THAT VARIETY)\n",
    "\n",
    "# print(df)\n",
    "variety_dummies = pd.get_dummies(df.Variety)\n",
    "# print(dummies)\n",
    "df = pd.concat([df, variety_dummies], axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plant_date = pd.to_datetime(df['Planting date'], infer_datetime_format=True)\n",
    "# df = df['Planting date'].apply(lambda dt: (dt.month%12 + 3)//3)\n",
    "df[df['Planting date'].str.match(\".\")]\n",
    "# pd.get_dummies(df['Planting date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATITUDE AND LONGITUDE CLUSTERING INTO FEATURES\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "latlong = df.loc[:, ['Latitude', 'Longitude']]\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(latlong)\n",
    "kmeans.labels_.shape\n",
    "lat_long_dummies = pd.get_dummies(kmeans.labels_).rename(index=int, columns={0: \"Loc Clust 0\", 1: \"Loc Clust 1\", 2: \"Loc Clust 2\", 3: \"Loc Clust 3\"})\n",
    "df = pd.concat([df, lat_long_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "binarized = lb.fit(df.Variety)\n",
    "print(binarized)\n",
    "df.Variety = pd.Series(binarized.transform(df.Variety))\n",
    "print(binarized.transform(df.Variety).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Silt', 'Precipitation', 'Temperature', 'Yield'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# DROP ALL THE CELLS THAT ARE NOT USABLE SUCH AS THE ONES THAT ARE STRINGS OR DATES\n",
    "\n",
    "# set if want to drop some columns specifically\n",
    "should_drop = 1\n",
    "columns_to_drop = ['Experiment', 'Location', 'Planting date',\n",
    "                   'Check Yield', 'Yield difference', 'Latitude',\n",
    "                   'Longitude', 'Variety', 'PI']\n",
    "\n",
    "# set if want to keep some columns specifically\n",
    "should_keep = 1\n",
    "# columns_to_keep = ['Loc Clust 0', 'Loc Clust 1', 'Loc Clust 2', 'Loc Clust 3']\n",
    "columns_to_keep = ['Silt', 'Precipitation', 'Temperature']\n",
    "MUST_HAVE_COLUMNS = ['Yield']\n",
    "# print(columns_to_keep)\n",
    "\n",
    "df = df.drop(columns_to_drop, axis=1) if should_drop else df\n",
    "df = df.loc[:, columns_to_keep + MUST_HAVE_COLUMNS] if should_keep else df\n",
    "df['YieldBucket'] = pd.Series(pd.qcut(df.Yield, q=3, labels=[\"high\", \"medium\", \"low\"]))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Silt', 'Precipitation', 'Temperature', 'Yield', 'YieldBucket'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We expect to be 0 nan values and there actually are 0 nan values\n",
      "\n",
      "Silt <class 'numpy.float64'>\n",
      "Precipitation <class 'numpy.float64'>\n",
      "Temperature <class 'numpy.float64'>\n",
      "Yield <class 'numpy.float64'>\n",
      "YieldBucket <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# LET US ALSO MAKE SURE THERE ARE NO NAN IN THE DATA\n",
    "print(\"We expect to be %s nan values and there actually are %s nan values\\n\" % (0, np.sum(df.isnull().sum())))\n",
    "# AFTER COLUMNS, MAKE SURE NO SKETCHY ONES\n",
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Silt', 'Precipitation', 'Temperature'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "\n",
    "X = df.drop(['Yield', 'YieldBucket'], axis=1)\n",
    "\n",
    "print(X.columns)\n",
    "\n",
    "y = df.Yield\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, train_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8203, 3) \n",
      "y_train shape: (8203,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape, \"\\ny_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will evaluate the errors based on RMSE (from the challenge spec)\n",
    "# also will evaluate based on average error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def evaluate_errors(prediction, actual):\n",
    "    print(\"RMSE Error: \", np.sqrt(mean_squared_error(prediction, actual)))\n",
    "    avg_error_vector = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(\"Average Error: \", np.mean(avg_error_vector))\n",
    "    print(\"Average Error details:\\n\", avg_error_vector.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Error:  7.327869621508968\n",
      "Average Error details:\n",
      " count    4102.000000\n",
      "mean       10.248333\n",
      "std        10.627742\n",
      "min         0.002683\n",
      "25%         3.535558\n",
      "50%         7.535967\n",
      "75%        13.421413\n",
      "max       169.791993\n",
      "Name: Yield, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(n_estimators=10, max_depth=20, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "evaluate_errors(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature has importance:  0.29512575281730485\n",
      "Precipitation has importance:  0.30880080546643224\n",
      "Silt has importance:  0.39607344171626296\n"
     ]
    }
   ],
   "source": [
    "# GET OUTPUT OF FEATURE IMPORTANCE\n",
    "\n",
    "feature_importances = regr.feature_importances_\n",
    "feature_importances = pd.Series(feature_importances)\n",
    "feature_importance_df = pd.DataFrame({'feature': X_train.columns,'feature_importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by=['feature_importance'])\n",
    "for index, row in feature_importance_df.iterrows():\n",
    "    print(row['feature'], 'has importance: ', row['feature_importance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WILL ONLY WORK WITH THE BUCKET METHOD\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regr = RandomForestClassifier(n_estimators=10, max_depth=20, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "classifiers = [\n",
    "    svm.SVR(),\n",
    "    MLPRegressor(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(5, 2), random_state=1),\n",
    "    linear_model.SGDRegressor(),\n",
    "    linear_model.BayesianRidge(),\n",
    "    linear_model.LassoLars(),\n",
    "#     linear_model.ARDRegression(),\n",
    "#     linear_model.ARDRegression(),\n",
    "    linear_model.PassiveAggressiveRegressor(),\n",
    "    linear_model.TheilSenRegressor(),\n",
    "    linear_model.LinearRegression()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# estimator = svm.SVR(kernel=\"linear\")\n",
    "\n",
    "# selector = RFECV(estimator, step=1, cv=5, verbose=1)\n",
    "# selector = selector.fit(X_train, y_train)\n",
    "# selector.support_ \n",
    "# # array([ True,  True,  True,  True,  True,\n",
    "# #         False, False, False, False, False], dtype=bool)\n",
    "# selector.ranking_\n",
    "# # array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "\n",
    "\n",
    "#     print(np.sum(preds - y_test))\n",
    "#     print(clf.predict(X_test),'\\n')\n",
    "#     print(y_test)\n",
    "#     print('accuracy score:', accuracy_score(y_test, clf.predict(X_test)), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    errors = mean_squared_error(preds, y_test)\n",
    "#     errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(errors)\n",
    "    print(errors)\n",
    "#     print(errors.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "#     KNeighborsClassifier(3),\n",
    "#     SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "from sklearn.metrics import accuracy_score\n",
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(scale(X_train), y_train)\n",
    "    preds = clf.predict(scale(X_test))\n",
    "    print(accuracy_score(y_test, preds))\n",
    "#     errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(errors)\n",
    "#     print(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
