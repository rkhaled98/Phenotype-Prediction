{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# READ THE CSV INTO DATAFRAME\n",
    "\n",
    "df = pd.read_csv('Syngenta/Syngenta_2017/Experiment_dataset.csv')\n",
    "\n",
    "np_ar = np.asarray(df)\n",
    "# df2 = pd.read_csv('Syngenta/Syngenta_2017/Region_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.Temperature.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY NECESSARY IF: USING 174 ADDITIONAL VARIETY COLUMNS METHOD\n",
    "\n",
    "# THIS IS A DIFFERENT APPROACH TO THE ABOVE FOUR CELLS, WHERE WE HAVE 174 ADDITIONAL FEATURE COLUMNS\n",
    "# EACH WITH A 0 (IF IT IS NOT OF THAT VARIETY) OR A 1 (IF IT IS OF THAT VARIETY)\n",
    "\n",
    "# print(df)\n",
    "variety_dummies = pd.get_dummies(df.Variety)\n",
    "# print(dummies)\n",
    "df = pd.concat([df, variety_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL VARIETY DISTRIBUTION ANALYSIS\n",
    "\n",
    "print(variety_dummies.sum().describe())\n",
    "print(np.sort(variety_dummies.sum()))\n",
    "for idx, cl in enumerate(variety_dummies.sum()):\n",
    "    print(variety_dummies.columns[idx], cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GOAL OF THIS MODULE:\n",
    "# Encode the planting date as a season\n",
    "\n",
    "# remove the dates that are \".\"\n",
    "df = df[~df['Planting date'].str.match(\"\\.\")]\n",
    "plant_date = df['Planting date'].apply(lambda dt: pd.to_datetime(dt))\n",
    "plant_date = plant_date.rename(\"Season\")\n",
    "plant_date = pd.to_datetime(plant_date)\n",
    "plant_date = plant_date.apply(lambda dt: (dt.month%12 + 3)//3)\n",
    "# df['Plant date'] = pd.to_datetime(df['Plant date'])\n",
    "df = pd.concat([df, plant_date], axis=1)\n",
    "\n",
    "# plant_date = pd.to_datetime(df['Planting date'], infer_datetime_format=True)\n",
    "# df = df['Planting date'].apply(lambda dt: (dt.month%12 + 3)//3)\n",
    "# pd.get_dummies(df['Planting date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['Season']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATITUDE AND LONGITUDE CLUSTERING INTO FEATURES\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "latlong = df.loc[:, ['Latitude', 'Longitude']]\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(latlong)\n",
    "kmeans.labels_.shape\n",
    "lat_long_dummies = pd.get_dummies(kmeans.labels_).rename(index=int, columns={0: \"Loc Clust 0\", 1: \"Loc Clust 1\", 2: \"Loc Clust 2\", 3: \"Loc Clust 3\"})\n",
    "df = pd.concat([df, lat_long_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(df.iloc[:, df.columns.str.match('V\\d\\d\\d\\d\\d\\d')].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fa3e52131d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS A VISUALIZATION FOR LATITUDE AND LONGITUDE CLUSTERING\n",
    "\n",
    "cent = kmeans.cluster_centers_\n",
    "clust_labels = kmeans.labels_\n",
    "means = pd.DataFrame(clust_labels)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(df['Latitude'],df['Longitude'],\n",
    "                     c=means[0], s=50)\n",
    "ax.set_title('K-Means Clustering')\n",
    "ax.set_xlabel('Latitude')\n",
    "ax.set_ylabel('Longitude')\n",
    "plt.colorbar(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE ANY NAN VALUES\n",
    "\n",
    "print(df.columns)\n",
    "df = df[~df.Silt.isnull()]\n",
    "df = df[~df['Loc Clust 1'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "binarized = lb.fit(df.Variety)\n",
    "print(binarized)\n",
    "df.Variety = pd.Series(binarized.transform(df.Variety))\n",
    "print(binarized.transform(df.Variety).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Yield', 'Year', 'Temperature', 'Precipitation', 'Solar Radiation',\n",
      "       'Soil class', 'CEC', 'Organic matter', 'pH', 'Clay', 'Silt', 'Sand',\n",
      "       'Area', 'Loc Clust 0', 'Loc Clust 1', 'Loc Clust 2', 'Loc Clust 3',\n",
      "       'YieldBucket'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# DROP ALL THE CELLS THAT ARE NOT USABLE SUCH AS THE ONES THAT ARE STRINGS OR DATES\n",
    "\n",
    "# set if want to drop some columns specifically\n",
    "should_drop = 1\n",
    "columns_to_drop = ['Experiment', 'Location',\n",
    "                   'Check Yield', 'Yield difference', 'Latitude',\n",
    "                   'Longitude', 'Variety', 'PI', 'Planting date']\n",
    "\n",
    "# set if want to keep some columns specifically\n",
    "should_keep = 0\n",
    "# columns_to_keep = ['Loc Clust 0', 'Loc Clust 1', 'Loc Clust 2', 'Loc Clust 3']\n",
    "columns_to_keep_top = ['Silt', 'Precipitation', 'Temperature']\n",
    "columns_VARIETIES_ONLY = np.asarray(df.iloc[:, df.columns.str.match('V\\d\\d\\d\\d\\d\\d')].columns)\n",
    "\n",
    "#set the below variable to whatever columns you want to keep\n",
    "columns_to_keep = columns_VARIETIES_ONLY\n",
    "\n",
    "MUST_HAVE_COLUMNS = ['Yield']\n",
    "# print(columns_to_keep)\n",
    "\n",
    "df = df.drop(columns_to_drop, axis=1) if should_drop else df\n",
    "df = df.loc[:, np.concatenate((columns_to_keep, MUST_HAVE_COLUMNS))] if should_keep else df\n",
    "df['YieldBucket'] = pd.Series(pd.qcut(df.Yield, q=3, labels=[\"high\", \"medium\", \"low\"]))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.get_dummies(df.YieldBucket).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We expect to be 0 nan values and there actually are 0 nan values\n",
      "\n",
      "Yield              0\n",
      "Year               0\n",
      "Temperature        0\n",
      "Precipitation      0\n",
      "Solar Radiation    0\n",
      "Soil class         0\n",
      "CEC                0\n",
      "Organic matter     0\n",
      "pH                 0\n",
      "Clay               0\n",
      "Silt               0\n",
      "Sand               0\n",
      "Area               0\n",
      "Loc Clust 0        0\n",
      "Loc Clust 1        0\n",
      "Loc Clust 2        0\n",
      "Loc Clust 3        0\n",
      "YieldBucket        0\n",
      "dtype: int64\n",
      "Yield <class 'numpy.float64'>\n",
      "Year <class 'numpy.int64'>\n",
      "Temperature <class 'numpy.float64'>\n",
      "Precipitation <class 'numpy.float64'>\n",
      "Solar Radiation <class 'numpy.int64'>\n",
      "Soil class <class 'numpy.int64'>\n",
      "CEC <class 'numpy.float64'>\n",
      "Organic matter <class 'numpy.float64'>\n",
      "pH <class 'numpy.float64'>\n",
      "Clay <class 'numpy.float64'>\n",
      "Silt <class 'numpy.float64'>\n",
      "Sand <class 'numpy.float64'>\n",
      "Area <class 'numpy.float64'>\n",
      "Loc Clust 0 <class 'numpy.uint8'>\n",
      "Loc Clust 1 <class 'numpy.uint8'>\n",
      "Loc Clust 2 <class 'numpy.uint8'>\n",
      "Loc Clust 3 <class 'numpy.uint8'>\n",
      "YieldBucket <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# LET US ALSO MAKE SURE THERE ARE NO NAN IN THE DATA\n",
    "print(\"We expect to be %s nan values and there actually are %s nan values\\n\" % (0, np.sum(df.isnull().sum())))\n",
    "print(df.isnull().sum())\n",
    "# AFTER COLUMNS, MAKE SURE NO SKETCHY ONES\n",
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'Temperature', 'Precipitation', 'Solar Radiation', 'Soil class',\n",
      "       'CEC', 'Organic matter', 'pH', 'Clay', 'Silt', 'Sand', 'Area',\n",
      "       'Loc Clust 0', 'Loc Clust 1', 'Loc Clust 2', 'Loc Clust 3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "\n",
    "X = df.drop(['Yield', 'YieldBucket'], axis=1)\n",
    "\n",
    "print(X.columns)\n",
    "\n",
    "y = df.Yield\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, train_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape, \"\\ny_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will evaluate the errors based on RMSE (from the challenge spec)\n",
    "# also will evaluate based on average error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def evaluate_errors(prediction, actual):\n",
    "    print(\"RMSE Error: \", np.sqrt(mean_squared_error(prediction, actual)))\n",
    "    avg_error_vector = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(\"Average Error: \", np.mean(avg_error_vector))\n",
    "    print(\"Average Error details:\\n\", avg_error_vector.describe())\n",
    "    return avg_error_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Error:  7.286046932916138\n",
      "Average Error details:\n",
      " count    4102.000000\n",
      "mean       10.214530\n",
      "std        10.612184\n",
      "min         0.000967\n",
      "25%         3.562631\n",
      "50%         7.515530\n",
      "75%        13.377225\n",
      "max       171.625417\n",
      "Name: Yield, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(n_estimators=20, max_depth=13, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "evaluate_errors(preds, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['Plant date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc Clust 1 has importance:  0.0019782225783457987\n",
      "Loc Clust 0 has importance:  0.003120506992478384\n",
      "Loc Clust 2 has importance:  0.005820109936884353\n",
      "Loc Clust 3 has importance:  0.0058934581272339125\n",
      "Soil class has importance:  0.01933195562288706\n",
      "pH has importance:  0.040783306266072836\n",
      "Sand has importance:  0.046619161650392996\n",
      "CEC has importance:  0.06044121342806226\n",
      "Clay has importance:  0.06190371098238797\n",
      "Area has importance:  0.07691924104295542\n",
      "Organic matter has importance:  0.08169355801487979\n",
      "Solar Radiation has importance:  0.09554505237484413\n",
      "Precipitation has importance:  0.1043630222426392\n",
      "Year has importance:  0.10889841062908041\n",
      "Temperature has importance:  0.11636072046801742\n",
      "Silt has importance:  0.1703283496428381\n"
     ]
    }
   ],
   "source": [
    "# GET OUTPUT OF FEATURE IMPORTANCE\n",
    "def get_feature_importances(regr):\n",
    "    feature_importances = regr.feature_importances_\n",
    "    feature_importances = pd.Series(feature_importances)\n",
    "    feature_importance_df = pd.DataFrame({'feature': X_train.columns,'feature_importance': feature_importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by=['feature_importance'])\n",
    "    for index, row in feature_importance_df.iterrows():\n",
    "        print(row['feature'], 'has importance: ', row['feature_importance'])\n",
    "get_feature_importances(regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WILL ONLY WORK WITH THE BUCKET METHOD\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regr = RandomForestClassifier(n_estimators=10, max_depth=20, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "classifiers = [\n",
    "    svm.SVR(),\n",
    "    MLPRegressor(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(5, 2), random_state=1),\n",
    "    linear_model.SGDRegressor(),\n",
    "    linear_model.BayesianRidge(),\n",
    "    linear_model.LassoLars(),\n",
    "#     linear_model.ARDRegression(),\n",
    "#     linear_model.ARDRegression(),\n",
    "    linear_model.PassiveAggressiveRegressor(),\n",
    "    linear_model.TheilSenRegressor(),\n",
    "    linear_model.LinearRegression()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# estimator = svm.SVR(kernel=\"linear\")\n",
    "\n",
    "# selector = RFECV(estimator, step=1, cv=5, verbose=1)\n",
    "# selector = selector.fit(X_train, y_train)\n",
    "# selector.support_ \n",
    "# # array([ True,  True,  True,  True,  True,\n",
    "# #         False, False, False, False, False], dtype=bool)\n",
    "# selector.ranking_\n",
    "# # array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "\n",
    "\n",
    "#     print(np.sum(preds - y_test))\n",
    "#     print(clf.predict(X_test),'\\n')\n",
    "#     print(y_test)\n",
    "#     print('accuracy score:', accuracy_score(y_test, clf.predict(X_test)), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "RMSE Error:  8.20675363379203\n",
      "Average Error details:\n",
      " count    4102.000000\n",
      "mean       11.844229\n",
      "std        13.748968\n",
      "min         0.000433\n",
      "25%         3.868332\n",
      "50%         8.489203\n",
      "75%        14.994232\n",
      "max       257.847394\n",
      "Name: Yield, dtype: float64\n",
      "NO FEATURE IMPORTANCE METRIC\n",
      "None\n",
      "MLPRegressor(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "RMSE Error:  11.498026353418481\n",
      "Average Error details:\n",
      " count    4102.000000\n",
      "mean       16.966659\n",
      "std        17.865910\n",
      "min         0.001149\n",
      "25%         6.024549\n",
      "50%        12.954241\n",
      "75%        21.713680\n",
      "max       338.324569\n",
      "Name: Yield, dtype: float64\n",
      "NO FEATURE IMPORTANCE METRIC\n",
      "None\n",
      "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
      "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "       loss='squared_loss', max_iter=None, n_iter=None, penalty='l2',\n",
      "       power_t=0.25, random_state=None, shuffle=True, tol=None, verbose=0,\n",
      "       warm_start=False)\n",
      "RMSE Error:  2.0243748237370556e+20\n",
      "Average Error details:\n",
      " count    4.102000e+03\n",
      "mean     3.581588e+20\n",
      "std      8.614493e+19\n",
      "min      1.905555e+20\n",
      "25%      3.002335e+20\n",
      "50%      3.417114e+20\n",
      "75%      3.936078e+20\n",
      "max      1.677218e+21\n",
      "Name: Yield, dtype: float64\n",
      "NO FEATURE IMPORTANCE METRIC\n",
      "None\n",
      "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "       normalize=False, tol=0.001, verbose=False)\n",
      "RMSE Error:  10.712561405582516\n",
      "Average Error details:\n",
      " count    4102.000000\n",
      "mean       15.705344\n",
      "std        16.843150\n",
      "min         0.000593\n",
      "25%         5.720896\n",
      "50%        11.889092\n",
      "75%        19.983828\n",
      "max       340.289328\n",
      "Name: Yield, dtype: float64\n",
      "NO FEATURE IMPORTANCE METRIC\n",
      "None\n",
      "LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16,\n",
      "     fit_intercept=True, fit_path=True, max_iter=500, normalize=True,\n",
      "     positive=False, precompute='auto', verbose=False)\n",
      "RMSE Error:  11.498026353409058\n",
      "Average Error details:\n",
      " count    4102.000000\n",
      "mean       16.966659\n",
      "std        17.865910\n",
      "min         0.001149\n",
      "25%         6.024549\n",
      "50%        12.954241\n",
      "75%        21.713680\n",
      "max       338.324569\n",
      "Name: Yield, dtype: float64\n",
      "NO FEATURE IMPORTANCE METRIC\n",
      "None\n",
      "PassiveAggressiveRegressor(C=1.0, average=False, epsilon=0.1,\n",
      "              fit_intercept=True, loss='epsilon_insensitive',\n",
      "              max_iter=None, n_iter=None, random_state=None, shuffle=True,\n",
      "              tol=None, verbose=0, warm_start=False)\n",
      "RMSE Error:  13.682126927809007\n",
      "Average Error details:\n",
      " count    4102.000000\n",
      "mean       18.537511\n",
      "std        14.513031\n",
      "min         0.002389\n",
      "25%         8.726073\n",
      "50%        16.880525\n",
      "75%        25.670365\n",
      "max       330.193203\n",
      "Name: Yield, dtype: float64\n",
      "NO FEATURE IMPORTANCE METRIC\n",
      "None\n",
      "TheilSenRegressor(copy_X=True, fit_intercept=True, max_iter=300,\n",
      "         max_subpopulation=10000, n_jobs=1, n_subsamples=None,\n",
      "         random_state=None, tol=0.001, verbose=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/kafi/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Error:  11.032889432415264\n",
      "Average Error details:\n",
      " count    4102.000000\n",
      "mean       16.275976\n",
      "std        17.994219\n",
      "min         0.002458\n",
      "25%         5.672874\n",
      "50%        11.793014\n",
      "75%        20.473766\n",
      "max       359.117261\n",
      "Name: Yield, dtype: float64\n",
      "NO FEATURE IMPORTANCE METRIC\n",
      "None\n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "RMSE Error:  10.707322429761183\n",
      "Average Error details:\n",
      " count    4102.000000\n",
      "mean       15.690770\n",
      "std        16.829560\n",
      "min         0.000424\n",
      "25%         5.703156\n",
      "50%        11.933397\n",
      "75%        19.902929\n",
      "max       339.045027\n",
      "Name: Yield, dtype: float64\n",
      "NO FEATURE IMPORTANCE METRIC\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    errors = evaluate_errors(preds, y_test)\n",
    "    try:\n",
    "        get_feature_importances(clf)\n",
    "    except:\n",
    "        print(\"NO FEATURE IMPORTANCE METRIC\")\n",
    "#     errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(errors)\n",
    "    print(errors)\n",
    "#     print(errors.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "#     KNeighborsClassifier(3),\n",
    "#     SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "from sklearn.metrics import accuracy_score\n",
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(scale(X_train), y_train)\n",
    "    preds = clf.predict(scale(X_test))\n",
    "    print(accuracy_score(y_test, preds))\n",
    "#     errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(errors)\n",
    "#     print(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
