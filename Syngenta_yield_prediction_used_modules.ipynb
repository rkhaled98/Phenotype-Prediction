{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# READ THE CSV INTO DATAFRAME\n",
    "\n",
    "df = pd.read_csv('Syngenta/Syngenta_2017/Experiment_dataset.csv')\n",
    "\n",
    "np_ar = np.asarray(df)\n",
    "# df2 = pd.read_csv('Syngenta/Syngenta_2017/Region_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.Temperature.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY NECESSARY IF: USING 174 ADDITIONAL VARIETY COLUMNS METHOD\n",
    "\n",
    "# THIS IS A DIFFERENT APPROACH TO THE ABOVE FOUR CELLS, WHERE WE HAVE 174 ADDITIONAL FEATURE COLUMNS\n",
    "# EACH WITH A 0 (IF IT IS NOT OF THAT VARIETY) OR A 1 (IF IT IS OF THAT VARIETY)\n",
    "\n",
    "# print(df)\n",
    "variety_dummies = pd.get_dummies(df.Variety)\n",
    "# print(dummies)\n",
    "df = pd.concat([df, variety_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     174.000000\n",
      "mean      471.471264\n",
      "std       642.755886\n",
      "min        38.000000\n",
      "25%       111.250000\n",
      "50%       215.000000\n",
      "75%       560.000000\n",
      "max      3849.000000\n",
      "dtype: float64\n",
      "[  38   40   45   46   56   57   66   70   71   71   73   75   76   76\n",
      "   76   77   80   80   81   81   82   82   87   88   89   93   93   94\n",
      "   94   95   96   96   96   97   98   98  103  104  106  106  106  107\n",
      "  110  111  112  113  114  114  115  115  118  124  125  125  127  131\n",
      "  131  132  134  135  145  145  146  150  150  152  154  154  159  161\n",
      "  163  163  167  170  179  179  180  180  186  186  187  188  192  193\n",
      "  198  211  213  217  218  218  220  222  223  230  231  237  245  249\n",
      "  258  271  273  279  281  283  283  291  299  303  304  304  310  311\n",
      "  312  319  319  322  325  325  330  337  345  345  369  389  410  439\n",
      "  442  464  543  545  565  576  581  596  604  611  622  628  673  679\n",
      "  723  729  738  788  793  819  834  835  872 1004 1040 1095 1114 1133\n",
      " 1212 1226 1236 1246 1269 1371 1445 1744 1819 1829 1841 1858 1934 2214\n",
      " 2375 2389 2556 3072 3177 3849]\n",
      "V000016 330\n",
      "V000017 75\n",
      "V000018 299\n",
      "V000023 596\n",
      "V000024 231\n",
      "V000025 1114\n",
      "V000030 56\n",
      "V000032 93\n",
      "V000034 322\n",
      "V000036 104\n",
      "V000039 258\n",
      "V000047 604\n",
      "V000050 71\n",
      "V000051 464\n",
      "V000058 179\n",
      "V000060 180\n",
      "V000062 622\n",
      "V000067 279\n",
      "V000070 71\n",
      "V000071 95\n",
      "V000075 66\n",
      "V000078 145\n",
      "V000079 187\n",
      "V000080 188\n",
      "V000081 345\n",
      "V000082 192\n",
      "V000092 410\n",
      "V000096 103\n",
      "V000098 304\n",
      "V000110 154\n",
      "V000115 131\n",
      "V000122 218\n",
      "V000123 545\n",
      "V000124 673\n",
      "V000125 167\n",
      "V000134 218\n",
      "V000147 94\n",
      "V000157 87\n",
      "V000172 312\n",
      "V000721 125\n",
      "V031243 180\n",
      "V051214 96\n",
      "V103132 186\n",
      "V103136 819\n",
      "V103139 345\n",
      "V103142 3849\n",
      "V103150 2389\n",
      "V103155 163\n",
      "V103156 146\n",
      "V103159 217\n",
      "V103163 3072\n",
      "V103173 57\n",
      "V103193 111\n",
      "V103198 46\n",
      "V103259 273\n",
      "V103266 106\n",
      "V103273 325\n",
      "V103277 337\n",
      "V103281 283\n",
      "V103286 325\n",
      "V103293 186\n",
      "V103302 304\n",
      "V103303 110\n",
      "V103308 93\n",
      "V103332 40\n",
      "V103425 127\n",
      "V103466 81\n",
      "V103620 193\n",
      "V103624 723\n",
      "V103866 565\n",
      "V103970 145\n",
      "V104000 835\n",
      "V110768 82\n",
      "V110890 319\n",
      "V110923 77\n",
      "V111237 1934\n",
      "V111336 1445\n",
      "V113396 291\n",
      "V113424 97\n",
      "V113476 73\n",
      "V114530 96\n",
      "V114541 150\n",
      "V114545 230\n",
      "V114553 311\n",
      "V114564 793\n",
      "V114565 872\n",
      "V114655 1004\n",
      "V114944 112\n",
      "V114951 124\n",
      "V119975 738\n",
      "V120038 198\n",
      "V120047 1744\n",
      "V120246 1133\n",
      "V120410 442\n",
      "V120585 211\n",
      "V120810 220\n",
      "V120912 107\n",
      "V120999 271\n",
      "V121015 76\n",
      "V121097 38\n",
      "V121140 45\n",
      "V121524 82\n",
      "V124343 115\n",
      "V130305 70\n",
      "V130307 679\n",
      "V130308 369\n",
      "V131675 245\n",
      "V131778 131\n",
      "V136868 611\n",
      "V137136 281\n",
      "V137147 170\n",
      "V137160 118\n",
      "V137237 576\n",
      "V137289 80\n",
      "V139094 81\n",
      "V139107 283\n",
      "V139548 1212\n",
      "V140091 88\n",
      "V140364 543\n",
      "V140784 125\n",
      "V150834 729\n",
      "V150844 1371\n",
      "V150847 2214\n",
      "V150853 788\n",
      "V150974 163\n",
      "V151036 132\n",
      "V151236 159\n",
      "V151273 106\n",
      "V151284 223\n",
      "V151329 76\n",
      "V151331 98\n",
      "V151332 179\n",
      "V151333 222\n",
      "V151334 115\n",
      "V151336 80\n",
      "V151337 76\n",
      "V151340 94\n",
      "V151399 98\n",
      "V151407 114\n",
      "V152053 834\n",
      "V152061 310\n",
      "V152067 134\n",
      "V152079 1226\n",
      "V152102 96\n",
      "V152253 628\n",
      "V152300 249\n",
      "V152312 152\n",
      "V152320 303\n",
      "V152322 154\n",
      "V152440 319\n",
      "V152734 89\n",
      "V152779 106\n",
      "V155180 135\n",
      "V155820 1236\n",
      "V155842 3177\n",
      "V155843 2375\n",
      "V155918 1829\n",
      "V156247 2556\n",
      "V156305 1269\n",
      "V156314 1858\n",
      "V156367 113\n",
      "V156368 1841\n",
      "V156516 389\n",
      "V156553 1095\n",
      "V156565 213\n",
      "V156574 150\n",
      "V156642 581\n",
      "V156763 161\n",
      "V156774 1246\n",
      "V156783 114\n",
      "V156786 439\n",
      "V156797 1040\n",
      "V156806 1819\n",
      "V156807 237\n"
     ]
    }
   ],
   "source": [
    "print(variety_dummies.sum().describe())\n",
    "print(np.sort(variety_dummies.sum()))\n",
    "for idx, cl in enumerate(variety_dummies.sum()):\n",
    "    print(variety_dummies.columns[idx], cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.str.match('V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GOAL OF THIS MODULE:\n",
    "# Encode the planting date as a season\n",
    "\n",
    "# remove the dates that are \".\"\n",
    "df = df[~df['Planting date'].str.match(\"\\.\")]\n",
    "plant_date = df['Planting date'].apply(lambda dt: pd.to_datetime(dt))\n",
    "plant_date = plant_date.rename(\"Season\")\n",
    "plant_date = pd.to_datetime(plant_date)\n",
    "plant_date = plant_date.apply(lambda dt: (dt.month%12 + 3)//3)\n",
    "# df['Plant date'] = pd.to_datetime(df['Plant date'])\n",
    "df = pd.concat([df, plant_date], axis=1)\n",
    "\n",
    "# plant_date = pd.to_datetime(df['Planting date'], infer_datetime_format=True)\n",
    "# df = df['Planting date'].apply(lambda dt: (dt.month%12 + 3)//3)\n",
    "# pd.get_dummies(df['Planting date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATITUDE AND LONGITUDE CLUSTERING INTO FEATURES\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "latlong = df.loc[:, ['Latitude', 'Longitude']]\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(latlong)\n",
    "kmeans.labels_.shape\n",
    "lat_long_dummies = pd.get_dummies(kmeans.labels_).rename(index=int, columns={0: \"Loc Clust 0\", 1: \"Loc Clust 1\", 2: \"Loc Clust 2\", 3: \"Loc Clust 3\"})\n",
    "df = pd.concat([df, lat_long_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['V000016', 'V000017', 'V000018', 'V000023', 'V000024', 'V000025',\n",
       "       'V000030', 'V000032', 'V000034', 'V000036', 'V000039', 'V000047',\n",
       "       'V000050', 'V000051', 'V000058', 'V000060', 'V000062', 'V000067',\n",
       "       'V000070', 'V000071', 'V000075', 'V000078', 'V000079', 'V000080',\n",
       "       'V000081', 'V000082', 'V000092', 'V000096', 'V000098', 'V000110',\n",
       "       'V000115', 'V000122', 'V000123', 'V000124', 'V000125', 'V000134',\n",
       "       'V000147', 'V000157', 'V000172', 'V000721', 'V031243', 'V051214',\n",
       "       'V103132', 'V103136', 'V103139', 'V103142', 'V103150', 'V103155',\n",
       "       'V103156', 'V103159', 'V103163', 'V103173', 'V103193', 'V103198',\n",
       "       'V103259', 'V103266', 'V103273', 'V103277', 'V103281', 'V103286',\n",
       "       'V103293', 'V103302', 'V103303', 'V103308', 'V103332', 'V103425',\n",
       "       'V103466', 'V103620', 'V103624', 'V103866', 'V103970', 'V104000',\n",
       "       'V110768', 'V110890', 'V110923', 'V111237', 'V111336', 'V113396',\n",
       "       'V113424', 'V113476', 'V114530', 'V114541', 'V114545', 'V114553',\n",
       "       'V114564', 'V114565', 'V114655', 'V114944', 'V114951', 'V119975',\n",
       "       'V120038', 'V120047', 'V120246', 'V120410', 'V120585', 'V120810',\n",
       "       'V120912', 'V120999', 'V121015', 'V121097', 'V121140', 'V121524',\n",
       "       'V124343', 'V130305', 'V130307', 'V130308', 'V131675', 'V131778',\n",
       "       'V136868', 'V137136', 'V137147', 'V137160', 'V137237', 'V137289',\n",
       "       'V139094', 'V139107', 'V139548', 'V140091', 'V140364', 'V140784',\n",
       "       'V150834', 'V150844', 'V150847', 'V150853', 'V150974', 'V151036',\n",
       "       'V151236', 'V151273', 'V151284', 'V151329', 'V151331', 'V151332',\n",
       "       'V151333', 'V151334', 'V151336', 'V151337', 'V151340', 'V151399',\n",
       "       'V151407', 'V152053', 'V152061', 'V152067', 'V152079', 'V152102',\n",
       "       'V152253', 'V152300', 'V152312', 'V152320', 'V152322', 'V152440',\n",
       "       'V152734', 'V152779', 'V155180', 'V155820', 'V155842', 'V155843',\n",
       "       'V155918', 'V156247', 'V156305', 'V156314', 'V156367', 'V156368',\n",
       "       'V156516', 'V156553', 'V156565', 'V156574', 'V156642', 'V156763',\n",
       "       'V156774', 'V156783', 'V156786', 'V156797', 'V156806', 'V156807'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(df.iloc[:, df.columns.str.match('V\\d\\d\\d\\d\\d\\d')].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS A VISUALIZATION FOR LATITUDE AND LONGITUDE CLUSTERING\n",
    "\n",
    "cent = kmeans.cluster_centers_\n",
    "clust_labels = kmeans.labels_\n",
    "means = pd.DataFrame(clust_labels)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(df['Latitude'],df['Longitude'],\n",
    "                     c=means[0], s=50)\n",
    "ax.set_title('K-Means Clustering')\n",
    "ax.set_xlabel('Latitude')\n",
    "ax.set_ylabel('Longitude')\n",
    "plt.colorbar(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE ANY NAN VALUES\n",
    "\n",
    "print(df.columns)\n",
    "df = df[~df.Silt.isnull()]\n",
    "df = df[~df['Loc Clust 1'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "binarized = lb.fit(df.Variety)\n",
    "print(binarized)\n",
    "df.Variety = pd.Series(binarized.transform(df.Variety))\n",
    "print(binarized.transform(df.Variety).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['V000016', 'V000017', 'V000018', 'V000023', 'V000024', 'V000025',\n",
      "       'V000030', 'V000032', 'V000034', 'V000036',\n",
      "       ...\n",
      "       'V156642', 'V156763', 'V156774', 'V156783', 'V156786', 'V156797',\n",
      "       'V156806', 'V156807', 'Yield', 'YieldBucket'],\n",
      "      dtype='object', length=176)\n"
     ]
    }
   ],
   "source": [
    "# DROP ALL THE CELLS THAT ARE NOT USABLE SUCH AS THE ONES THAT ARE STRINGS OR DATES\n",
    "\n",
    "# set if want to drop some columns specifically\n",
    "should_drop = 0\n",
    "columns_to_drop = ['Experiment', 'Location',\n",
    "                   'Check Yield', 'Yield difference', 'Latitude',\n",
    "                   'Longitude', 'Variety', 'PI', 'Planting date']\n",
    "\n",
    "# set if want to keep some columns specifically\n",
    "should_keep = 1\n",
    "# columns_to_keep = ['Loc Clust 0', 'Loc Clust 1', 'Loc Clust 2', 'Loc Clust 3']\n",
    "columns_to_keep_top = ['Silt', 'Precipitation', 'Temperature']\n",
    "columns_VARIETIES_ONLY = np.asarray(df.iloc[:, df.columns.str.match('V\\d\\d\\d\\d\\d\\d')].columns)\n",
    "\n",
    "#set the below variable to whatever columns you want to keep\n",
    "columns_to_keep = columns_VARIETIES_ONLY\n",
    "\n",
    "MUST_HAVE_COLUMNS = ['Yield']\n",
    "# print(columns_to_keep)\n",
    "\n",
    "df = df.drop(columns_to_drop, axis=1) if should_drop else df\n",
    "df = df.loc[:, np.concatenate((columns_to_keep, MUST_HAVE_COLUMNS))] if should_keep else df\n",
    "df['YieldBucket'] = pd.Series(pd.qcut(df.Yield, q=3, labels=[\"high\", \"medium\", \"low\"]))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.get_dummies(df.YieldBucket).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET US ALSO MAKE SURE THERE ARE NO NAN IN THE DATA\n",
    "print(\"We expect to be %s nan values and there actually are %s nan values\\n\" % (0, np.sum(df.isnull().sum())))\n",
    "print(df.isnull().sum())\n",
    "# AFTER COLUMNS, MAKE SURE NO SKETCHY ONES\n",
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['V000016', 'V000017', 'V000018', 'V000023', 'V000024', 'V000025',\n",
      "       'V000030', 'V000032', 'V000034', 'V000036',\n",
      "       ...\n",
      "       'V156565', 'V156574', 'V156642', 'V156763', 'V156774', 'V156783',\n",
      "       'V156786', 'V156797', 'V156806', 'V156807'],\n",
      "      dtype='object', length=174)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "\n",
    "X = df.drop(['Yield', 'YieldBucket'], axis=1)\n",
    "\n",
    "print(X.columns)\n",
    "\n",
    "y = df.Yield\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, train_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape, \"\\ny_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will evaluate the errors based on RMSE (from the challenge spec)\n",
    "# also will evaluate based on average error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def evaluate_errors(prediction, actual):\n",
    "    print(\"RMSE Error: \", np.sqrt(mean_squared_error(prediction, actual)))\n",
    "    avg_error_vector = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(\"Average Error: \", np.mean(avg_error_vector))\n",
    "    print(\"Average Error details:\\n\", avg_error_vector.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Error:  11.019433343628817\n",
      "Average Error details:\n",
      " count    4102.000000\n",
      "mean       16.064871\n",
      "std        16.584654\n",
      "min         0.005440\n",
      "25%         5.662837\n",
      "50%        12.117249\n",
      "75%        20.806759\n",
      "max       249.859833\n",
      "Name: Yield, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(n_estimators=10, max_depth=20, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "evaluate_errors(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['Plant date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V000016 has importance:  0.0\n",
      "V136868 has importance:  0.0\n",
      "V137136 has importance:  0.0\n",
      "V137147 has importance:  0.0\n",
      "V137160 has importance:  0.0\n",
      "V137237 has importance:  0.0\n",
      "V137289 has importance:  0.0\n",
      "V139094 has importance:  0.0\n",
      "V131778 has importance:  0.0\n",
      "V139548 has importance:  0.0\n",
      "V140784 has importance:  0.0\n",
      "V150844 has importance:  0.0\n",
      "V150847 has importance:  0.0\n",
      "V150853 has importance:  0.0\n",
      "V150974 has importance:  0.0\n",
      "V151036 has importance:  0.0\n",
      "V151236 has importance:  0.0\n",
      "V140091 has importance:  0.0\n",
      "V151273 has importance:  0.0\n",
      "V131675 has importance:  0.0\n",
      "V124343 has importance:  0.0\n",
      "V114541 has importance:  0.0\n",
      "V114545 has importance:  0.0\n",
      "V114553 has importance:  0.0\n",
      "V114564 has importance:  0.0\n",
      "V114944 has importance:  0.0\n",
      "V114951 has importance:  0.0\n",
      "V120047 has importance:  0.0\n",
      "V130305 has importance:  0.0\n",
      "V120410 has importance:  0.0\n",
      "V120810 has importance:  0.0\n",
      "V120912 has importance:  0.0\n",
      "V120999 has importance:  0.0\n",
      "V121015 has importance:  0.0\n",
      "V121097 has importance:  0.0\n",
      "V121140 has importance:  0.0\n",
      "V121524 has importance:  0.0\n",
      "V120585 has importance:  0.0\n",
      "V114530 has importance:  0.0\n",
      "V151284 has importance:  0.0\n",
      "V151331 has importance:  0.0\n",
      "V155180 has importance:  0.0\n",
      "V155820 has importance:  0.0\n",
      "V155843 has importance:  0.0\n",
      "V155918 has importance:  0.0\n",
      "V156305 has importance:  0.0\n",
      "V156314 has importance:  0.0\n",
      "V156367 has importance:  0.0\n",
      "V152779 has importance:  0.0\n",
      "V156516 has importance:  0.0\n",
      "V156565 has importance:  0.0\n",
      "V156574 has importance:  0.0\n",
      "V156642 has importance:  0.0\n",
      "V156763 has importance:  0.0\n",
      "V156783 has importance:  0.0\n",
      "V156786 has importance:  0.0\n",
      "V156797 has importance:  0.0\n",
      "V156553 has importance:  0.0\n",
      "V151329 has importance:  0.0\n",
      "V152734 has importance:  0.0\n",
      "V152322 has importance:  0.0\n",
      "V151332 has importance:  0.0\n",
      "V151333 has importance:  0.0\n",
      "V151334 has importance:  0.0\n",
      "V151336 has importance:  0.0\n",
      "V151337 has importance:  0.0\n",
      "V151340 has importance:  0.0\n",
      "V151407 has importance:  0.0\n",
      "V152440 has importance:  0.0\n",
      "V152053 has importance:  0.0\n",
      "V152067 has importance:  0.0\n",
      "V152079 has importance:  0.0\n",
      "V152102 has importance:  0.0\n",
      "V152253 has importance:  0.0\n",
      "V152300 has importance:  0.0\n",
      "V152312 has importance:  0.0\n",
      "V152320 has importance:  0.0\n",
      "V152061 has importance:  0.0\n",
      "V113476 has importance:  0.0\n",
      "V156807 has importance:  0.0\n",
      "V113396 has importance:  0.0\n",
      "V000096 has importance:  0.0\n",
      "V000098 has importance:  0.0\n",
      "V000115 has importance:  0.0\n",
      "V000125 has importance:  0.0\n",
      "V000147 has importance:  0.0\n",
      "V000157 has importance:  0.0\n",
      "V000092 has importance:  0.0\n",
      "V000172 has importance:  0.0\n",
      "V031243 has importance:  0.0\n",
      "V113424 has importance:  0.0\n",
      "V103132 has importance:  0.0\n",
      "V103139 has importance:  0.0\n",
      "V103142 has importance:  0.0\n",
      "V103150 has importance:  0.0\n",
      "V000721 has importance:  0.0\n",
      "V000082 has importance:  0.0\n",
      "V000081 has importance:  0.0\n",
      "V000080 has importance:  0.0\n",
      "V000017 has importance:  0.0\n",
      "V000018 has importance:  0.0\n",
      "V000023 has importance:  0.0\n",
      "V000025 has importance:  0.0\n",
      "V000030 has importance:  0.0\n",
      "V000034 has importance:  0.0\n",
      "V000036 has importance:  0.0\n",
      "V000039 has importance:  0.0\n",
      "V000050 has importance:  0.0\n",
      "V000051 has importance:  0.0\n",
      "V000062 has importance:  0.0\n",
      "V000070 has importance:  0.0\n",
      "V000075 has importance:  0.0\n",
      "V000078 has importance:  0.0\n",
      "V000079 has importance:  0.0\n",
      "V103156 has importance:  0.0\n",
      "V103159 has importance:  0.0\n",
      "V051214 has importance:  0.0\n",
      "V103302 has importance:  0.0\n",
      "V103332 has importance:  0.0\n",
      "V103425 has importance:  0.0\n",
      "V103866 has importance:  0.0\n",
      "V110923 has importance:  0.0\n",
      "V103286 has importance:  0.0\n",
      "V103970 has importance:  0.0\n",
      "V111237 has importance:  0.0\n",
      "V103308 has importance:  0.0\n",
      "V103277 has importance:  0.0\n",
      "V103281 has importance:  0.0\n",
      "V103266 has importance:  0.0\n",
      "V103259 has importance:  0.0\n",
      "V104000 has importance:  0.0\n",
      "V103193 has importance:  0.0\n",
      "V103173 has importance:  0.0\n",
      "V110890 has importance:  0.0\n",
      "V110768 has importance:  0.0\n",
      "V103273 has importance:  0.0\n",
      "V103303 has importance:  0.0\n",
      "V000067 has importance:  0.002985165071047014\n",
      "V000122 has importance:  0.0030046514636274985\n",
      "V140364 has importance:  0.003042327453103449\n",
      "V000024 has importance:  0.003559040568006569\n",
      "V156247 has importance:  0.003650122416416364\n",
      "V103466 has importance:  0.003707696982373604\n",
      "V156774 has importance:  0.004405826526421586\n",
      "V000071 has importance:  0.0060194041364598876\n",
      "V103293 has importance:  0.006687563699770223\n",
      "V103198 has importance:  0.008134594505300454\n",
      "V103624 has importance:  0.008800735848224342\n",
      "V114565 has importance:  0.00952484563274086\n",
      "V103620 has importance:  0.009824056249727273\n",
      "V111336 has importance:  0.013726767356092207\n",
      "V000032 has importance:  0.014649129926701168\n",
      "V000134 has importance:  0.01627159572964814\n",
      "V000060 has importance:  0.019551493135828164\n",
      "V119975 has importance:  0.01984525855635977\n",
      "V150834 has importance:  0.020330874523652398\n",
      "V000110 has importance:  0.023048326346824827\n",
      "V000124 has importance:  0.023729176979123134\n",
      "V139107 has importance:  0.024359257214200656\n",
      "V103155 has importance:  0.024511549378483134\n",
      "V120038 has importance:  0.026752031966672463\n",
      "V130307 has importance:  0.027219226996097252\n",
      "V000123 has importance:  0.02869460068091544\n",
      "V000047 has importance:  0.031295697910620104\n",
      "V151399 has importance:  0.03232099117894053\n",
      "V103163 has importance:  0.03825737357921042\n",
      "V155842 has importance:  0.03837502923650685\n",
      "V156368 has importance:  0.041343692872973015\n",
      "V000058 has importance:  0.042707290282898\n",
      "V114655 has importance:  0.046094379451464315\n",
      "V156806 has importance:  0.05348983349146626\n",
      "V103136 has importance:  0.056605688149600576\n",
      "V130308 has importance:  0.11149099842131072\n",
      "V120246 has importance:  0.15198370608119133\n"
     ]
    }
   ],
   "source": [
    "# GET OUTPUT OF FEATURE IMPORTANCE\n",
    "\n",
    "feature_importances = regr.feature_importances_\n",
    "feature_importances = pd.Series(feature_importances)\n",
    "feature_importance_df = pd.DataFrame({'feature': X_train.columns,'feature_importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by=['feature_importance'])\n",
    "for index, row in feature_importance_df.iterrows():\n",
    "    print(row['feature'], 'has importance: ', row['feature_importance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WILL ONLY WORK WITH THE BUCKET METHOD\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regr = RandomForestClassifier(n_estimators=10, max_depth=20, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "classifiers = [\n",
    "    svm.SVR(),\n",
    "    MLPRegressor(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(5, 2), random_state=1),\n",
    "    linear_model.SGDRegressor(),\n",
    "    linear_model.BayesianRidge(),\n",
    "    linear_model.LassoLars(),\n",
    "#     linear_model.ARDRegression(),\n",
    "#     linear_model.ARDRegression(),\n",
    "    linear_model.PassiveAggressiveRegressor(),\n",
    "    linear_model.TheilSenRegressor(),\n",
    "    linear_model.LinearRegression()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# estimator = svm.SVR(kernel=\"linear\")\n",
    "\n",
    "# selector = RFECV(estimator, step=1, cv=5, verbose=1)\n",
    "# selector = selector.fit(X_train, y_train)\n",
    "# selector.support_ \n",
    "# # array([ True,  True,  True,  True,  True,\n",
    "# #         False, False, False, False, False], dtype=bool)\n",
    "# selector.ranking_\n",
    "# # array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "\n",
    "\n",
    "#     print(np.sum(preds - y_test))\n",
    "#     print(clf.predict(X_test),'\\n')\n",
    "#     print(y_test)\n",
    "#     print('accuracy score:', accuracy_score(y_test, clf.predict(X_test)), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    errors = mean_squared_error(preds, y_test)\n",
    "#     errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(errors)\n",
    "    print(errors)\n",
    "#     print(errors.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "#     KNeighborsClassifier(3),\n",
    "#     SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "from sklearn.metrics import accuracy_score\n",
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(scale(X_train), y_train)\n",
    "    preds = clf.predict(scale(X_test))\n",
    "    print(accuracy_score(y_test, preds))\n",
    "#     errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(errors)\n",
    "#     print(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
