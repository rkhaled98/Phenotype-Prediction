{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THERE ARE SEPARATE NOTEBOOKS FOR VISUALIZATIONS, DATASET ANALYSIS, ETC. IN THE REPO.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# READ THE CSV INTO DATAFRAME\n",
    "\n",
    "df = pd.read_csv('Syngenta/Syngenta_2017/Experiment_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY NECESSARY IF: USING 174 ADDITIONAL VARIETY COLUMNS METHOD\n",
    "\n",
    "# THIS IS A DIFFERENT APPROACH TO THE ABOVE FOUR CELLS, WHERE WE HAVE 174 ADDITIONAL FEATURE COLUMNS\n",
    "# EACH WITH A 0 (IF IT IS NOT OF THAT VARIETY) OR A 1 (IF IT IS OF THAT VARIETY)\n",
    "\n",
    "variety_dummies = pd.get_dummies(df.Variety)\n",
    "df = pd.concat([df, variety_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GOAL OF THIS MODULE:\n",
    "# Encode the planting date as a season\n",
    "# NEW GOAL:\n",
    "# GET DUMMIES FOR SEASONS\n",
    "\n",
    "# remove the dates that are \".\"\n",
    "df = df[~df['Planting date'].str.match(\"\\.\")]\n",
    "plant_date = df['Planting date'].apply(lambda dt: pd.to_datetime(dt))\n",
    "plant_months = plant_date.apply(lambda dt: dt.month)\n",
    "season = plant_date.rename(\"Season\")\n",
    "season = pd.to_datetime(season)\n",
    "season = season.apply(lambda dt: (dt.month%12 + 3)//3)\n",
    "# df['Plant date'] = pd.to_datetime(df['Plant date'])\n",
    "df = pd.concat([df, season], axis=1)\n",
    "\n",
    "# plant_date = pd.to_datetime(df['Planting date'], infer_datetime_format=True)\n",
    "# df = df['Planting date'].apply(lambda dt: (dt.month%12 + 3)//3)\n",
    "# pd.get_dummies(df['Planting date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MONTH OF MAY AND JUNE ONE HOT ENCODING INTO THE DATAFRAME\n",
    "pd.get_dummies(plant_months).sum()\n",
    "june = pd.get_dummies(plant_months).loc[:,6]\n",
    "june = june.rename(\"June\")\n",
    "may = pd.get_dummies(plant_months).loc[:,5]\n",
    "may = may.rename(\"May\")\n",
    "df = pd.concat([df, may], axis=1)\n",
    "df = pd.concat([df, june], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATITUDE AND LONGITUDE CLUSTERING INTO FEATURES\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "latlong = df.loc[:, ['Latitude', 'Longitude']]\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(latlong)\n",
    "kmeans.labels_.shape\n",
    "lat_long_dummies = pd.get_dummies(kmeans.labels_)\n",
    "lat_long_dummies = lat_long_dummies.rename(index=int, columns={0: \"Loc Clust 0\",\n",
    "                                                               1: \"Loc Clust 1\",\n",
    "                                                               2: \"Loc Clust 2\",\n",
    "                                                               3: \"Loc Clust 3\"})\n",
    "df = pd.concat([df, lat_long_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Yield', 'Year', 'Temperature', 'Precipitation', 'Solar Radiation',\n",
      "       'Soil class', 'CEC', 'Organic matter', 'pH', 'Clay',\n",
      "       ...\n",
      "       'V156797', 'V156806', 'V156807', 'May', 'June', 'Loc Clust 0',\n",
      "       'Loc Clust 1', 'Loc Clust 2', 'Loc Clust 3', 'YieldBucket'],\n",
      "      dtype='object', length=194)\n"
     ]
    }
   ],
   "source": [
    "#REMOVE ANY NAN VALUES\n",
    "\n",
    "print(df.columns)\n",
    "df = df[~df.Silt.isnull()]\n",
    "df = df[~df['Loc Clust 1'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOAL OF THIS MODULE:\n",
    "#TO LIMIT THE NUMBER OF VARIETY COLUMNS NEEDED AS FEATURES, USE BINARY REPRESENTATION\n",
    "\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "binarized = lb.fit(df.Variety)\n",
    "print(binarized)\n",
    "df.Variety = pd.Series(binarized.transform(df.Variety))\n",
    "print(binarized.transform(df.Variety).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['Experiment' 'Location' 'Check Yield' 'Yield difference' 'Latitude'\n 'Longitude' 'Variety' 'PI' 'Planting date' 'Season'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ca23425f7101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# print(columns_to_keep)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns_to_drop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshould_drop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns_to_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMUST_HAVE_COLUMNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshould_keep\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'YieldBucket'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mYield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"high\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"medium\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"low\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   2560\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2563\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3742\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3743\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3744\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3745\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3746\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['Experiment' 'Location' 'Check Yield' 'Yield difference' 'Latitude'\n 'Longitude' 'Variety' 'PI' 'Planting date' 'Season'] not contained in axis"
     ]
    }
   ],
   "source": [
    "# DROP ALL THE CELLS THAT ARE NOT USABLE SUCH AS THE ONES THAT ARE STRINGS OR DATES\n",
    "\n",
    "# set if want to drop some columns specifically\n",
    "should_drop = 1\n",
    "columns_to_drop = ['Experiment', 'Location',\n",
    "                   'Check Yield', 'Yield difference', 'Latitude',\n",
    "                   'Longitude', 'Variety', 'PI', 'Planting date', 'Season']\n",
    "\n",
    "# set if want to keep some columns specifically\n",
    "should_keep = 0\n",
    "# columns_to_keep = ['Loc Clust 0', 'Loc Clust 1', 'Loc Clust 2', 'Loc Clust 3']\n",
    "columns_to_keep_top = ['Silt', 'Precipitation', 'Temperature', 'Solar Radiation', 'Organic matter']\n",
    "columns_VARIETIES_ONLY = np.asarray(df.iloc[:, df.columns.str.match('V\\d\\d\\d\\d\\d\\d')].columns)\n",
    "\n",
    "#set the below variable to whatever columns you want to keep\n",
    "columns_to_keep = columns_to_keep_top\n",
    "\n",
    "MUST_HAVE_COLUMNS = ['Yield']\n",
    "# print(columns_to_keep)\n",
    "\n",
    "df = df.drop(columns_to_drop, axis=1) if should_drop else df\n",
    "df = df.loc[:, np.concatenate((columns_to_keep, MUST_HAVE_COLUMNS))] if should_keep else df\n",
    "df['YieldBucket'] = pd.Series(pd.qcut(df.Yield, q=3, labels=[\"high\", \"medium\", \"low\"]))\n",
    "print(\"The final dataframe has columns: \", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We expect to be 0 nan values and there actually are 378300 nan values\n",
      "\n",
      "Yield              1950\n",
      "Year               1950\n",
      "Temperature        1950\n",
      "Precipitation      1950\n",
      "Solar Radiation    1950\n",
      "Soil class         1950\n",
      "CEC                1950\n",
      "Organic matter     1950\n",
      "pH                 1950\n",
      "Clay               1950\n",
      "Silt               1950\n",
      "Sand               1950\n",
      "Area               1950\n",
      "V000016            1950\n",
      "V000017            1950\n",
      "V000018            1950\n",
      "V000023            1950\n",
      "V000024            1950\n",
      "V000025            1950\n",
      "V000030            1950\n",
      "V000032            1950\n",
      "V000034            1950\n",
      "V000036            1950\n",
      "V000039            1950\n",
      "V000047            1950\n",
      "V000050            1950\n",
      "V000051            1950\n",
      "V000058            1950\n",
      "V000060            1950\n",
      "V000062            1950\n",
      "                   ... \n",
      "V152779            1950\n",
      "V155180            1950\n",
      "V155820            1950\n",
      "V155842            1950\n",
      "V155843            1950\n",
      "V155918            1950\n",
      "V156247            1950\n",
      "V156305            1950\n",
      "V156314            1950\n",
      "V156367            1950\n",
      "V156368            1950\n",
      "V156516            1950\n",
      "V156553            1950\n",
      "V156565            1950\n",
      "V156574            1950\n",
      "V156642            1950\n",
      "V156763            1950\n",
      "V156774            1950\n",
      "V156783            1950\n",
      "V156786            1950\n",
      "V156797            1950\n",
      "V156806            1950\n",
      "V156807            1950\n",
      "May                1950\n",
      "June               1950\n",
      "Loc Clust 0        1950\n",
      "Loc Clust 1        1950\n",
      "Loc Clust 2        1950\n",
      "Loc Clust 3        1950\n",
      "YieldBucket        1950\n",
      "Length: 194, dtype: int64\n",
      "Yield <class 'numpy.float64'>\n",
      "Year <class 'numpy.float64'>\n",
      "Temperature <class 'numpy.float64'>\n",
      "Precipitation <class 'numpy.float64'>\n",
      "Solar Radiation <class 'numpy.float64'>\n",
      "Soil class <class 'numpy.float64'>\n",
      "CEC <class 'numpy.float64'>\n",
      "Organic matter <class 'numpy.float64'>\n",
      "pH <class 'numpy.float64'>\n",
      "Clay <class 'numpy.float64'>\n",
      "Silt <class 'numpy.float64'>\n",
      "Sand <class 'numpy.float64'>\n",
      "Area <class 'numpy.float64'>\n",
      "V000016 <class 'numpy.float64'>\n",
      "V000017 <class 'numpy.float64'>\n",
      "V000018 <class 'numpy.float64'>\n",
      "V000023 <class 'numpy.float64'>\n",
      "V000024 <class 'numpy.float64'>\n",
      "V000025 <class 'numpy.float64'>\n",
      "V000030 <class 'numpy.float64'>\n",
      "V000032 <class 'numpy.float64'>\n",
      "V000034 <class 'numpy.float64'>\n",
      "V000036 <class 'numpy.float64'>\n",
      "V000039 <class 'numpy.float64'>\n",
      "V000047 <class 'numpy.float64'>\n",
      "V000050 <class 'numpy.float64'>\n",
      "V000051 <class 'numpy.float64'>\n",
      "V000058 <class 'numpy.float64'>\n",
      "V000060 <class 'numpy.float64'>\n",
      "V000062 <class 'numpy.float64'>\n",
      "V000067 <class 'numpy.float64'>\n",
      "V000070 <class 'numpy.float64'>\n",
      "V000071 <class 'numpy.float64'>\n",
      "V000075 <class 'numpy.float64'>\n",
      "V000078 <class 'numpy.float64'>\n",
      "V000079 <class 'numpy.float64'>\n",
      "V000080 <class 'numpy.float64'>\n",
      "V000081 <class 'numpy.float64'>\n",
      "V000082 <class 'numpy.float64'>\n",
      "V000092 <class 'numpy.float64'>\n",
      "V000096 <class 'numpy.float64'>\n",
      "V000098 <class 'numpy.float64'>\n",
      "V000110 <class 'numpy.float64'>\n",
      "V000115 <class 'numpy.float64'>\n",
      "V000122 <class 'numpy.float64'>\n",
      "V000123 <class 'numpy.float64'>\n",
      "V000124 <class 'numpy.float64'>\n",
      "V000125 <class 'numpy.float64'>\n",
      "V000134 <class 'numpy.float64'>\n",
      "V000147 <class 'numpy.float64'>\n",
      "V000157 <class 'numpy.float64'>\n",
      "V000172 <class 'numpy.float64'>\n",
      "V000721 <class 'numpy.float64'>\n",
      "V031243 <class 'numpy.float64'>\n",
      "V051214 <class 'numpy.float64'>\n",
      "V103132 <class 'numpy.float64'>\n",
      "V103136 <class 'numpy.float64'>\n",
      "V103139 <class 'numpy.float64'>\n",
      "V103142 <class 'numpy.float64'>\n",
      "V103150 <class 'numpy.float64'>\n",
      "V103155 <class 'numpy.float64'>\n",
      "V103156 <class 'numpy.float64'>\n",
      "V103159 <class 'numpy.float64'>\n",
      "V103163 <class 'numpy.float64'>\n",
      "V103173 <class 'numpy.float64'>\n",
      "V103193 <class 'numpy.float64'>\n",
      "V103198 <class 'numpy.float64'>\n",
      "V103259 <class 'numpy.float64'>\n",
      "V103266 <class 'numpy.float64'>\n",
      "V103273 <class 'numpy.float64'>\n",
      "V103277 <class 'numpy.float64'>\n",
      "V103281 <class 'numpy.float64'>\n",
      "V103286 <class 'numpy.float64'>\n",
      "V103293 <class 'numpy.float64'>\n",
      "V103302 <class 'numpy.float64'>\n",
      "V103303 <class 'numpy.float64'>\n",
      "V103308 <class 'numpy.float64'>\n",
      "V103332 <class 'numpy.float64'>\n",
      "V103425 <class 'numpy.float64'>\n",
      "V103466 <class 'numpy.float64'>\n",
      "V103620 <class 'numpy.float64'>\n",
      "V103624 <class 'numpy.float64'>\n",
      "V103866 <class 'numpy.float64'>\n",
      "V103970 <class 'numpy.float64'>\n",
      "V104000 <class 'numpy.float64'>\n",
      "V110768 <class 'numpy.float64'>\n",
      "V110890 <class 'numpy.float64'>\n",
      "V110923 <class 'numpy.float64'>\n",
      "V111237 <class 'numpy.float64'>\n",
      "V111336 <class 'numpy.float64'>\n",
      "V113396 <class 'numpy.float64'>\n",
      "V113424 <class 'numpy.float64'>\n",
      "V113476 <class 'numpy.float64'>\n",
      "V114530 <class 'numpy.float64'>\n",
      "V114541 <class 'numpy.float64'>\n",
      "V114545 <class 'numpy.float64'>\n",
      "V114553 <class 'numpy.float64'>\n",
      "V114564 <class 'numpy.float64'>\n",
      "V114565 <class 'numpy.float64'>\n",
      "V114655 <class 'numpy.float64'>\n",
      "V114944 <class 'numpy.float64'>\n",
      "V114951 <class 'numpy.float64'>\n",
      "V119975 <class 'numpy.float64'>\n",
      "V120038 <class 'numpy.float64'>\n",
      "V120047 <class 'numpy.float64'>\n",
      "V120246 <class 'numpy.float64'>\n",
      "V120410 <class 'numpy.float64'>\n",
      "V120585 <class 'numpy.float64'>\n",
      "V120810 <class 'numpy.float64'>\n",
      "V120912 <class 'numpy.float64'>\n",
      "V120999 <class 'numpy.float64'>\n",
      "V121015 <class 'numpy.float64'>\n",
      "V121097 <class 'numpy.float64'>\n",
      "V121140 <class 'numpy.float64'>\n",
      "V121524 <class 'numpy.float64'>\n",
      "V124343 <class 'numpy.float64'>\n",
      "V130305 <class 'numpy.float64'>\n",
      "V130307 <class 'numpy.float64'>\n",
      "V130308 <class 'numpy.float64'>\n",
      "V131675 <class 'numpy.float64'>\n",
      "V131778 <class 'numpy.float64'>\n",
      "V136868 <class 'numpy.float64'>\n",
      "V137136 <class 'numpy.float64'>\n",
      "V137147 <class 'numpy.float64'>\n",
      "V137160 <class 'numpy.float64'>\n",
      "V137237 <class 'numpy.float64'>\n",
      "V137289 <class 'numpy.float64'>\n",
      "V139094 <class 'numpy.float64'>\n",
      "V139107 <class 'numpy.float64'>\n",
      "V139548 <class 'numpy.float64'>\n",
      "V140091 <class 'numpy.float64'>\n",
      "V140364 <class 'numpy.float64'>\n",
      "V140784 <class 'numpy.float64'>\n",
      "V150834 <class 'numpy.float64'>\n",
      "V150844 <class 'numpy.float64'>\n",
      "V150847 <class 'numpy.float64'>\n",
      "V150853 <class 'numpy.float64'>\n",
      "V150974 <class 'numpy.float64'>\n",
      "V151036 <class 'numpy.float64'>\n",
      "V151236 <class 'numpy.float64'>\n",
      "V151273 <class 'numpy.float64'>\n",
      "V151284 <class 'numpy.float64'>\n",
      "V151329 <class 'numpy.float64'>\n",
      "V151331 <class 'numpy.float64'>\n",
      "V151332 <class 'numpy.float64'>\n",
      "V151333 <class 'numpy.float64'>\n",
      "V151334 <class 'numpy.float64'>\n",
      "V151336 <class 'numpy.float64'>\n",
      "V151337 <class 'numpy.float64'>\n",
      "V151340 <class 'numpy.float64'>\n",
      "V151399 <class 'numpy.float64'>\n",
      "V151407 <class 'numpy.float64'>\n",
      "V152053 <class 'numpy.float64'>\n",
      "V152061 <class 'numpy.float64'>\n",
      "V152067 <class 'numpy.float64'>\n",
      "V152079 <class 'numpy.float64'>\n",
      "V152102 <class 'numpy.float64'>\n",
      "V152253 <class 'numpy.float64'>\n",
      "V152300 <class 'numpy.float64'>\n",
      "V152312 <class 'numpy.float64'>\n",
      "V152320 <class 'numpy.float64'>\n",
      "V152322 <class 'numpy.float64'>\n",
      "V152440 <class 'numpy.float64'>\n",
      "V152734 <class 'numpy.float64'>\n",
      "V152779 <class 'numpy.float64'>\n",
      "V155180 <class 'numpy.float64'>\n",
      "V155820 <class 'numpy.float64'>\n",
      "V155842 <class 'numpy.float64'>\n",
      "V155843 <class 'numpy.float64'>\n",
      "V155918 <class 'numpy.float64'>\n",
      "V156247 <class 'numpy.float64'>\n",
      "V156305 <class 'numpy.float64'>\n",
      "V156314 <class 'numpy.float64'>\n",
      "V156367 <class 'numpy.float64'>\n",
      "V156368 <class 'numpy.float64'>\n",
      "V156516 <class 'numpy.float64'>\n",
      "V156553 <class 'numpy.float64'>\n",
      "V156565 <class 'numpy.float64'>\n",
      "V156574 <class 'numpy.float64'>\n",
      "V156642 <class 'numpy.float64'>\n",
      "V156763 <class 'numpy.float64'>\n",
      "V156774 <class 'numpy.float64'>\n",
      "V156783 <class 'numpy.float64'>\n",
      "V156786 <class 'numpy.float64'>\n",
      "V156797 <class 'numpy.float64'>\n",
      "V156806 <class 'numpy.float64'>\n",
      "V156807 <class 'numpy.float64'>\n",
      "May <class 'numpy.float64'>\n",
      "June <class 'numpy.float64'>\n",
      "Loc Clust 0 <class 'numpy.float64'>\n",
      "Loc Clust 1 <class 'numpy.float64'>\n",
      "Loc Clust 2 <class 'numpy.float64'>\n",
      "Loc Clust 3 <class 'numpy.float64'>\n",
      "YieldBucket <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# LET US ALSO MAKE SURE THERE ARE NO NAN IN THE DATA\n",
    "print(\"We expect to be %s nan values and there actually are %s nan values\\n\" % (0, np.sum(df.isnull().sum())))\n",
    "print(df.isnull().sum())\n",
    "# AFTER COLUMNS, MAKE SURE NO SKETCHY ONES\n",
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'Temperature', 'Precipitation', 'Solar Radiation', 'Soil class',\n",
      "       'CEC', 'Organic matter', 'pH', 'Clay', 'Silt',\n",
      "       ...\n",
      "       'V156786', 'V156797', 'V156806', 'V156807', 'May', 'June',\n",
      "       'Loc Clust 0', 'Loc Clust 1', 'Loc Clust 2', 'Loc Clust 3'],\n",
      "      dtype='object', length=192)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "\n",
    "X = df.drop(['Yield', 'YieldBucket'], axis=1)\n",
    "\n",
    "print(X.columns)\n",
    "\n",
    "y = df.Yield\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, train_size = 0.1, random_state = 42)\n",
    "\n",
    "INPUT_COLS = X_train.columns\n",
    "# TEST_COLS = y_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7813, 192) \n",
      "y_train shape: (7813,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape, \"\\ny_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will evaluate the errors based on RMSE (from the challenge spec)\n",
    "# also will evaluate based on average error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def evaluate_errors(prediction, actual):\n",
    "    print(\"RMSE Error: \", np.sqrt(mean_squared_error(prediction, actual)))\n",
    "    avg_error_vector = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(\"Average Error: \", np.mean(avg_error_vector))\n",
    "    print(\"Average Error details:\\n\", avg_error_vector.describe())\n",
    "    return avg_error_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Error:  7.230648964933217\n",
      "Average Error details:\n",
      " count    3907.000000\n",
      "mean       10.273712\n",
      "std        10.902952\n",
      "min         0.005787\n",
      "25%         3.504977\n",
      "50%         7.463465\n",
      "75%        13.390107\n",
      "max       141.373792\n",
      "Name: Yield, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37747     3.318402\n",
       "34960     6.745501\n",
       "77631     0.285277\n",
       "29678     2.371641\n",
       "58370     6.285895\n",
       "44709     7.483849\n",
       "37353     2.087536\n",
       "11083    10.865165\n",
       "65678     9.758797\n",
       "68237     6.487128\n",
       "69975    12.843164\n",
       "53136     7.409571\n",
       "25669    29.816400\n",
       "22055     6.815375\n",
       "53416     1.862979\n",
       "1165     20.048285\n",
       "21660    25.220308\n",
       "10943    19.820369\n",
       "29486     3.817819\n",
       "73128     3.841617\n",
       "77434    13.139076\n",
       "74450     6.527960\n",
       "29534     9.277164\n",
       "79136     9.895220\n",
       "51908     4.277561\n",
       "6382      8.507752\n",
       "4497      8.274020\n",
       "29180     6.889641\n",
       "40968    13.152110\n",
       "56236     5.154415\n",
       "           ...    \n",
       "55268     9.818288\n",
       "36424     8.397165\n",
       "38873     1.053541\n",
       "53108     0.445981\n",
       "57176    10.455698\n",
       "9969      5.625612\n",
       "63588     3.505989\n",
       "53502    11.167884\n",
       "45568     4.174549\n",
       "5098     13.638104\n",
       "27685    20.754869\n",
       "19609    60.007392\n",
       "31942    13.979377\n",
       "3338     17.543903\n",
       "62444     9.761453\n",
       "67291     6.708281\n",
       "47450     7.662806\n",
       "11728     4.445628\n",
       "3337     20.585995\n",
       "49107     8.438182\n",
       "19969    20.259952\n",
       "55298    12.675728\n",
       "52721     8.814417\n",
       "15185    14.916882\n",
       "31732     9.198787\n",
       "78093     9.401261\n",
       "53280     4.447510\n",
       "63604    11.450413\n",
       "41242     2.256978\n",
       "19341     7.329639\n",
       "Name: Yield, Length: 3907, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(n_estimators=20, max_depth=13, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "evaluate_errors(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V121140 has importance:  0.0\n",
      "V121097 has importance:  2.8596859520076963e-06\n",
      "V000036 has importance:  6.0793353871786106e-06\n",
      "V151273 has importance:  6.285801449910723e-06\n",
      "V152779 has importance:  1.2189654511586188e-05\n",
      "V152312 has importance:  1.596049707717542e-05\n",
      "V121524 has importance:  1.7510543902265844e-05\n",
      "V152102 has importance:  2.3973597899738836e-05\n",
      "V139094 has importance:  2.3983845058519083e-05\n",
      "V113424 has importance:  2.457841899194059e-05\n",
      "V130305 has importance:  3.293727499560608e-05\n",
      "V000147 has importance:  3.572359519830296e-05\n",
      "V120999 has importance:  3.644191642371407e-05\n",
      "V000080 has importance:  3.991145866587386e-05\n",
      "V156763 has importance:  4.1178649633552265e-05\n",
      "V156367 has importance:  4.369949671632177e-05\n",
      "V120912 has importance:  5.087373334012611e-05\n",
      "V114951 has importance:  5.2992524010322516e-05\n",
      "V151337 has importance:  5.601140168717358e-05\n",
      "V103466 has importance:  5.645016604406419e-05\n",
      "V131675 has importance:  5.977419733786208e-05\n",
      "V152734 has importance:  6.512798928094396e-05\n",
      "V151340 has importance:  6.66324389959605e-05\n",
      "V110923 has importance:  6.767804020632226e-05\n",
      "V137289 has importance:  6.890091503027947e-05\n",
      "V151407 has importance:  7.24741144380493e-05\n",
      "V151332 has importance:  7.299708101339123e-05\n",
      "V156565 has importance:  7.705168487742528e-05\n",
      "V151334 has importance:  9.040353422343858e-05\n",
      "V000075 has importance:  9.904079732296739e-05\n",
      "V103308 has importance:  9.986514581794487e-05\n",
      "V103159 has importance:  0.00010767234257150295\n",
      "V051214 has importance:  0.00010995762149276416\n",
      "V140784 has importance:  0.00011298993029398952\n",
      "V000050 has importance:  0.00011990977281818246\n",
      "V151329 has importance:  0.00012411537393349558\n",
      "V110768 has importance:  0.00013401983430649895\n",
      "V151284 has importance:  0.00013620062277311184\n",
      "V000157 has importance:  0.00013666748007265468\n",
      "V151333 has importance:  0.00015699372507866182\n",
      "V103198 has importance:  0.0001592176631081086\n",
      "V151331 has importance:  0.00016254844738334962\n",
      "V103266 has importance:  0.00017210475466352573\n",
      "V000032 has importance:  0.00017347915633967106\n",
      "V103303 has importance:  0.00017941800796753896\n",
      "V113476 has importance:  0.00018295211553861818\n",
      "V000096 has importance:  0.0002082851531153409\n",
      "V121015 has importance:  0.00021295397781685629\n",
      "V114530 has importance:  0.00021687970287793718\n",
      "V131778 has importance:  0.0002226730309786501\n",
      "V000721 has importance:  0.00022444365570362205\n",
      "V156783 has importance:  0.0002298790250060758\n",
      "V103259 has importance:  0.0002312471265743547\n",
      "V103281 has importance:  0.0002499638566363864\n",
      "V114545 has importance:  0.00027410606119507736\n",
      "V000098 has importance:  0.0002743308758792226\n",
      "V000030 has importance:  0.0002750117422274346\n",
      "V152300 has importance:  0.00027519932646287887\n",
      "V000060 has importance:  0.0002894142144788256\n",
      "V103193 has importance:  0.00029548903535831276\n",
      "V103277 has importance:  0.00030401551510521546\n",
      "V140091 has importance:  0.0003136256266150505\n",
      "V103155 has importance:  0.0003175444504381828\n",
      "V103302 has importance:  0.0003196466358984695\n",
      "V151036 has importance:  0.0003229458069460867\n",
      "V114541 has importance:  0.0003445546586740613\n",
      "V124343 has importance:  0.00034655584347337184\n",
      "V136868 has importance:  0.0003606646007604097\n",
      "V000079 has importance:  0.00036356771111399074\n",
      "V103173 has importance:  0.00037079959288369976\n",
      "V103132 has importance:  0.0003716194440171035\n",
      "V031243 has importance:  0.0003760119837306145\n",
      "V000078 has importance:  0.0004033078305262897\n",
      "V155180 has importance:  0.00040455629271325493\n",
      "V114944 has importance:  0.00041115229066463366\n",
      "V103425 has importance:  0.0004223846735971782\n",
      "V103620 has importance:  0.00045029384278281526\n",
      "V000016 has importance:  0.00045145860959770485\n",
      "V103273 has importance:  0.0004635240803749035\n",
      "V151336 has importance:  0.0004790111929465006\n",
      "V156574 has importance:  0.0004836018015046576\n",
      "V000134 has importance:  0.0004904378164578057\n",
      "V120038 has importance:  0.0004959545361591524\n",
      "V000017 has importance:  0.0004962350680638096\n",
      "V137136 has importance:  0.0005191550724452744\n",
      "V000067 has importance:  0.0005216325393964037\n",
      "V000070 has importance:  0.0005495665189211749\n",
      "V000125 has importance:  0.0005579643415068291\n",
      "V152322 has importance:  0.000565348750963223\n",
      "V103139 has importance:  0.000570496160046833\n",
      "V000058 has importance:  0.000578889896834485\n",
      "V103332 has importance:  0.0005848475346340525\n",
      "V139107 has importance:  0.0005893028650717348\n",
      "V150974 has importance:  0.0005970167482965996\n",
      "V000115 has importance:  0.0006012189863348948\n",
      "V152061 has importance:  0.0006294958075260229\n",
      "V152067 has importance:  0.0006400974114019799\n",
      "V103970 has importance:  0.0006416189179313593\n",
      "V000082 has importance:  0.0006651637645681028\n",
      "V114565 has importance:  0.0006767387156033581\n",
      "V000039 has importance:  0.0007019406676311139\n",
      "V103286 has importance:  0.0007109189058557174\n",
      "V114553 has importance:  0.0007300801908229419\n",
      "V156797 has importance:  0.0007393022934815586\n",
      "V113396 has importance:  0.0007647991768706543\n",
      "V000034 has importance:  0.0007994707589157885\n",
      "V140364 has importance:  0.000808682593074185\n",
      "V000110 has importance:  0.0008139534136190025\n",
      "V151399 has importance:  0.0008162296527273322\n",
      "V151236 has importance:  0.000826359774527952\n",
      "V000062 has importance:  0.0008395196434933742\n",
      "V137160 has importance:  0.0008412172018476318\n",
      "V119975 has importance:  0.0008430818918107975\n",
      "V000051 has importance:  0.0008438542261703725\n",
      "V000071 has importance:  0.0008452553392489216\n",
      "V137147 has importance:  0.0009270637833795744\n",
      "V103156 has importance:  0.0009298998954660588\n",
      "V152253 has importance:  0.0009380799721241277\n",
      "V120410 has importance:  0.000939886453943193\n",
      "V000122 has importance:  0.00096870943495356\n",
      "V156516 has importance:  0.000988823748212867\n",
      "V152440 has importance:  0.0009925016356819624\n",
      "V104000 has importance:  0.000997316905944705\n",
      "V152320 has importance:  0.0010398969189863046\n",
      "V156807 has importance:  0.0010437491556827595\n",
      "V110890 has importance:  0.0010717355416763304\n",
      "V000124 has importance:  0.0010731668364140712\n",
      "V156786 has importance:  0.0011198743687053482\n",
      "V000018 has importance:  0.0011408059113127533\n",
      "V137237 has importance:  0.0011464995496335629\n",
      "V120810 has importance:  0.0011505682536430341\n",
      "V120585 has importance:  0.0011593780293740774\n",
      "V130307 has importance:  0.001172652573073098\n",
      "V000025 has importance:  0.001195985280302364\n",
      "V103293 has importance:  0.0012045447124894562\n",
      "V156553 has importance:  0.0012154012209877841\n",
      "V114655 has importance:  0.0012336159469226507\n",
      "V000172 has importance:  0.0012780801136407119\n",
      "V000023 has importance:  0.0013410850497507574\n",
      "V156774 has importance:  0.0013417265290129477\n",
      "V103624 has importance:  0.0013575353782289926\n",
      "V000123 has importance:  0.0014344714540489509\n",
      "V156305 has importance:  0.0014554749665178564\n",
      "V103866 has importance:  0.0015098609500555228\n",
      "V156314 has importance:  0.0015107326085770751\n",
      "V150844 has importance:  0.0015501651551228421\n",
      "V111237 has importance:  0.00156543175326291\n",
      "V000092 has importance:  0.0016324340134309025\n",
      "V114564 has importance:  0.001674690330405372\n",
      "V000024 has importance:  0.0017073486023662491\n",
      "V150853 has importance:  0.0017870055769448189\n",
      "V150834 has importance:  0.0018401622684661057\n",
      "V103136 has importance:  0.0018611181635818479\n",
      "V155918 has importance:  0.0019056408404165138\n",
      "V000047 has importance:  0.001981620345819506\n",
      "V000081 has importance:  0.0020377337359895747\n",
      "V152053 has importance:  0.0021153373275843057\n",
      "V155820 has importance:  0.002216747282827753\n",
      "V103142 has importance:  0.00231595755122778\n",
      "V156368 has importance:  0.002666763211078042\n",
      "V156642 has importance:  0.0026681359483447095\n",
      "V152079 has importance:  0.0027396944818006883\n",
      "V120047 has importance:  0.002748989017639198\n",
      "V139548 has importance:  0.002779643268256011\n",
      "V156806 has importance:  0.0029691807432224828\n",
      "V156247 has importance:  0.00297956534251586\n",
      "V150847 has importance:  0.003266001761548824\n",
      "V103150 has importance:  0.00332806075959806\n",
      "V120246 has importance:  0.0034471363094714423\n",
      "V155843 has importance:  0.003990804747121766\n",
      "V111336 has importance:  0.004078740029870881\n",
      "V130308 has importance:  0.004520645695658394\n",
      "V155842 has importance:  0.004991444551317363\n",
      "Loc Clust 0 has importance:  0.006010129536508399\n",
      "V103163 has importance:  0.006436335578214232\n",
      "Loc Clust 3 has importance:  0.007095057337524566\n",
      "Loc Clust 1 has importance:  0.008165676190208266\n",
      "Loc Clust 2 has importance:  0.008305135756127627\n",
      "Soil class has importance:  0.012724822899887656\n",
      "June has importance:  0.014101113921893175\n",
      "May has importance:  0.0188384388505928\n",
      "Clay has importance:  0.0339856155731954\n",
      "Sand has importance:  0.03660704198589461\n",
      "CEC has importance:  0.045588995752205805\n",
      "pH has importance:  0.046491380588088355\n",
      "Area has importance:  0.05668690006782404\n",
      "Solar Radiation has importance:  0.06531796980855603\n",
      "Temperature has importance:  0.0729329733198696\n",
      "Year has importance:  0.07596034143698824\n",
      "Organic matter has importance:  0.08285843470323374\n",
      "Precipitation has importance:  0.09624085086808298\n",
      "Silt has importance:  0.1593630629967046\n"
     ]
    }
   ],
   "source": [
    "# GET OUTPUT OF FEATURE IMPORTANCE\n",
    "def get_feature_importances(regr):\n",
    "    feature_importances = regr.feature_importances_\n",
    "    feature_importances = pd.Series(feature_importances)\n",
    "    feature_importance_df = pd.DataFrame({'feature': X_train.columns,'feature_importance': feature_importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by=['feature_importance'])\n",
    "    for index, row in feature_importance_df.iterrows():\n",
    "        print(row['feature'], 'has importance: ', row['feature_importance'])\n",
    "get_feature_importances(regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WILL ONLY WORK WITH THE BUCKET METHOD\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regr = RandomForestClassifier(n_estimators=10, max_depth=20, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "classifiers = [\n",
    "    svm.SVR(),\n",
    "    MLPRegressor(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(5, 2), random_state=1),\n",
    "    linear_model.SGDRegressor(),\n",
    "    linear_model.BayesianRidge(),\n",
    "    linear_model.LassoLars(),\n",
    "#     linear_model.ARDRegression(),\n",
    "#     linear_model.ARDRegression(),\n",
    "    linear_model.PassiveAggressiveRegressor(),\n",
    "    linear_model.TheilSenRegressor(),\n",
    "    linear_model.LinearRegression()]\n",
    "\n",
    "# estimator = svm.SVR(kernel=\"linear\")\n",
    "\n",
    "# selector = RFECV(estimator, step=1, cv=5, verbose=1)\n",
    "# selector = selector.fit(X_train, y_train)\n",
    "# selector.support_ \n",
    "# # array([ True,  True,  True,  True,  True,\n",
    "# #         False, False, False, False, False], dtype=bool)\n",
    "# selector.ranking_\n",
    "# # array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "\n",
    "\n",
    "#     print(np.sum(preds - y_test))\n",
    "#     print(clf.predict(X_test),'\\n')\n",
    "#     print(y_test)\n",
    "#     print('accuracy score:', accuracy_score(y_test, clf.predict(X_test)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    errors = evaluate_errors(preds, y_test)\n",
    "    try:\n",
    "        get_feature_importances(clf)\n",
    "    except:\n",
    "        print(\"NO FEATURE IMPORTANCE METRIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "#     KNeighborsClassifier(3),\n",
    "#     SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "from sklearn.metrics import accuracy_score\n",
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(scale(X_train), y_train)\n",
    "    preds = clf.predict(scale(X_test))\n",
    "    print(accuracy_score(y_test, preds))\n",
    "#     errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(errors)\n",
    "#     print(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_VARIETIES = 174\n",
    "\n",
    "def best_yield_variety(regr, test_set, random_sel = True, n_samples = 174, print_variety_preds = True):\n",
    "    \n",
    "    #create empty df\n",
    "    dup_df = pd.DataFrame()\n",
    "    \n",
    "    #choose a rand sample of input test_set (for dev purposes, wouldn't be used in app)\n",
    "    test_set_sample = test_set.sample(n= n_samples) if random_sel else test_set\n",
    "    \n",
    "    #progress of intensive, long for loop upcoming\n",
    "    counter = 0\n",
    "    \n",
    "    #for loop will, for each row in test_set_sample, duplicate that row by NUM_VARIETIES\n",
    "    for index, row in test_set_sample.iterrows():\n",
    "        counter+=1\n",
    "        print(counter)\n",
    "        dup_df = dup_df.append([row] * NUM_VARIETIES, ignore_index = True)\n",
    "        \n",
    "    #extract the varieties columns\n",
    "    duplicated_df_varieties = dup_df.loc[:, dup_df.columns.str.match('V\\d\\d\\d\\d\\d\\d')]\n",
    "    #extract the names of the varieties\n",
    "    varieties_array = duplicated_df_varieties.columns\n",
    "    num_expanded_data_pts = duplicated_df_varieties.shape[0]\n",
    "    #we must have a zeroed matrix of the same shape as the duplicated_df_varieties\n",
    "    #so that we can input a 1 just once for each variety per 174 rows\n",
    "    d = np.zeros((duplicated_df_varieties.shape[0], NUM_VARIETIES))\n",
    "    #make d our dataframe, with columns equal to the varieties\n",
    "    duplicated_df_varieties = pd.DataFrame(d, columns=varieties_array)\n",
    "    #for loop will place a 1 just once for each variety per 174 rows (one hot rep)\n",
    "    for i in range(duplicated_df_varieties.shape[0]):\n",
    "        var_index = i % 174\n",
    "        duplicated_df_varieties.loc[i, varieties_array[var_index]] = 1\n",
    "    #remove the varieties (will be added back with the new values)\n",
    "    dup_df = dup_df.drop(varieties_array, axis = 1)\n",
    "    #add the new values (one hot representations) from above for loop\n",
    "    dup_df = pd.concat([dup_df, duplicated_df_varieties], axis=1)\n",
    "    #do prediction on the entire dataframe\n",
    "    preds_per_variety = regr.predict(dup_df)\n",
    "    #*******make it into a dataframe where each row will give the performance of each variety\n",
    "    #with the same environmental conditions*******\n",
    "    preds_df = pd.DataFrame(preds_per_variety.reshape((int(num_expanded_data_pts/NUM_VARIETIES), NUM_VARIETIES)),\n",
    "                            columns=varieties_array)\n",
    "    \n",
    "    #environmental conditions (everything except the variety data)\n",
    "    envcond = test_set_sample.drop(varieties_array, axis=1)\n",
    "    \n",
    "    #a simple print out for best variety given the environmental conditions\n",
    "    hr_preds = []\n",
    "    if print_variety_preds:\n",
    "        envcond_cols = envcond.columns\n",
    "        counter = -1\n",
    "        for idx, row in envcond.iterrows():\n",
    "            counter+=1\n",
    "            out = \"For environmental conditions:\\n%s\\nthe best variety is:%s\" % (row, preds_df.idxmax(axis=1)[counter])\n",
    "            hr_preds.append(out)\n",
    "            print(out)\n",
    "    \n",
    "    return preds_df, envcond, hr_preds\n",
    "    \n",
    "        \n",
    "preds_df, envcond, hr_preds = best_yield_variety(regr, X_test, n_samples = 174)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find number of times a variety is the maximum in the above prediction dataframe\n",
    "maxes = preds_df.idxmax(axis=1) # get first max variety per env\n",
    "# print(pd.get_dummies(maxes).describe())\n",
    "print(pd.get_dummies(maxes).sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hr_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.describe().sort_values(by=\"mean\", axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
