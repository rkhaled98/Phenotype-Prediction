{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN THE CSV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_sf = pd.read_csv('Smart_Farm_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO ANY PRE OBSERVATION HERE\n",
    "\n",
    "df_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE HOT REPRESENT (PREVIOUSLY) STRING ENCODED LOC AND GEN COLUMNS\n",
    "\n",
    "df_sf = pd.concat([df_sf, pd.get_dummies(df_sf['Loc'])], axis=1)\n",
    "df_sf = pd.concat([df_sf, pd.get_dummies(df_sf['Gen'])], axis=1)\n",
    "\n",
    "# DROP ANY COLUMNS HERE\n",
    "COLUMNS_DROPPED = ['Date', 'Loc', 'Gen', 'PlantID']\n",
    "df_sf = df_sf.drop(COLUMNS_DROPPED, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS MODULE WILL SPLIT UP EACH 100 POINTS IN THE DATAFRAME INTO SEPARATE CYCLES SO THAT WE CAN DO TIME SHIFTING\n",
    "# ON THE APPROPRIATE CYCLES\n",
    "cycles = []\n",
    "for i in range(0, 4):\n",
    "    start_index = i*100\n",
    "    end_index = start_index + 99\n",
    "    df_cycle = df_sf.loc[start_index:end_index]\n",
    "    cycles.append(df_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, col_names, n_in=1, n_out=1,  dropnan=True, ):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    print(\"vars_names\", col_names)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('%s%d(t-%d)' % (col_names[j], j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('%s%d(t)' % (col_names[j], j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('%s%d(t+%d)' % (col_names[j], j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE DATAFRAME TO BE USED FOR TRAIN AND TEST SPLITTING\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for df in cycles:\n",
    "    values = df.values\n",
    "    values_supervised = series_to_supervised(values, df.columns, 10, 1)\n",
    "    data = pd.concat([data, values_supervised], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE TERRIBLY CODED TRAIN AND TEST SPLIT APPROACH FOR TESTING WITH THE LAST 5 DAYS AND TRAINING WITH\n",
    "# THE OTHER DAYS.\n",
    "# TODO: MAKE EXTENSIBLE\n",
    "new_data = data.reset_index().drop(['index'], axis=1)\n",
    "\n",
    "X_train = new_data.iloc[[i for i in range(0,85)]\n",
    "                        + [i for i in range(90,175)]\n",
    "                        + [i for i in range(180, 265)]\n",
    "                        + [i for i in range(270,355)]]\n",
    "y_train = X_train.loc[:, 'GrowthRate2(t)']\n",
    "X_train = X_train.loc[:,~data.columns.str.contains('\\(t\\)')]\n",
    "# X = data.drop([data.columns.str.contains('\\(t\\)')], axis=1)\n",
    "X_test = new_data.iloc[[i for i in range(85,90)]\n",
    "                        + [i for i in range(175,180)]\n",
    "                        + [i for i in range(265,270)]\n",
    "                        + [i for i in range(355, 360)]]\n",
    "y_test = X_test.loc[:, 'GrowthRate2(t)']\n",
    "X_test = X_test.loc[:,~data.columns.str.contains('\\(t\\)')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vars_names Index(['DAP', 'GrowthRate', 'Temperature', 'Solar.Rad', 'Humidity', 'Rainfall',\n",
      "       'K', 'N', 'A', 'B'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "values = df_sf.values\n",
    "data = series_to_supervised(values,df_sf.columns,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#TRAIN AND TEST SPLIT RANDOMLY (INSTEAD)\n",
    "\n",
    "X = data.loc[:,~data.columns.str.contains('\\(t\\)')]\n",
    "# X = data.drop([data.columns.str.contains('\\(t\\)')], axis=1)\n",
    "y = data.loc[:, 'GrowthRate2(t)']\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.95, test_size = 0.05, random_state = 42)\n",
    "# for i in range(0, 50):\n",
    "#     print(i)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.95, test_size = 0.05, random_state = i)\n",
    "#     RF_train(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR COMPUTATION FUNCTIONS\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def evaluate_errors(prediction, actual):\n",
    "    print(\"RMSE Error: \", np.sqrt(mean_squared_error(prediction, actual)))\n",
    "    avg_error_vector = np.absolute(((prediction - actual) / actual) * 100)\n",
    "    print(\"Average Error details:\\n\", np.mean(avg_error_vector))\n",
    "    return avg_error_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE IMPORTANCE FUNCTIONS\n",
    "\n",
    "def get_feature_importances(regr):\n",
    "    feature_importances = regr.feature_importances_\n",
    "    feature_importances = pd.Series(feature_importances)\n",
    "    feature_importance_df = pd.DataFrame({'feature': X_train.columns,'feature_importance': feature_importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by=['feature_importance'])\n",
    "    for index, row in feature_importance_df.iterrows():\n",
    "        print(row['feature'], 'has importance: ', row['feature_importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 0.11844984113493803\n",
      "RMSE 0.24384904931931922\n",
      "A9(t-5) has importance:  6.107092687644009e-08\n",
      "N8(t-5) has importance:  1.4540044676836902e-07\n",
      "B10(t-3) has importance:  1.6513145177284342e-07\n",
      "K7(t-10) has importance:  1.9441809943552976e-07\n",
      "K7(t-9) has importance:  2.7964191375554406e-07\n",
      "A9(t-1) has importance:  3.9825739100017034e-07\n",
      "N8(t-10) has importance:  4.3478085691295494e-07\n",
      "A9(t-2) has importance:  6.154446603399998e-07\n",
      "N8(t-6) has importance:  7.122705431483357e-07\n",
      "K7(t-7) has importance:  7.269386685733827e-07\n",
      "N8(t-4) has importance:  8.276870226034404e-07\n",
      "A9(t-10) has importance:  8.935667008970173e-07\n",
      "N8(t-9) has importance:  8.977447513172287e-07\n",
      "N8(t-8) has importance:  9.153436097290487e-07\n",
      "A9(t-8) has importance:  9.87353122500488e-07\n",
      "B10(t-10) has importance:  9.96884837386198e-07\n",
      "A9(t-3) has importance:  1.2509498177882424e-06\n",
      "B10(t-6) has importance:  1.36851456444999e-06\n",
      "B10(t-7) has importance:  1.5959922160435203e-06\n",
      "B10(t-5) has importance:  1.8744576621720063e-06\n",
      "N8(t-7) has importance:  1.9114280690766557e-06\n",
      "N8(t-1) has importance:  2.0309085635590377e-06\n",
      "A9(t-4) has importance:  2.2750836462632792e-06\n",
      "K7(t-8) has importance:  2.524833457638403e-06\n",
      "K7(t-2) has importance:  2.6297246323026703e-06\n",
      "N8(t-2) has importance:  2.843916954489588e-06\n",
      "A9(t-7) has importance:  2.9717220072430707e-06\n",
      "K7(t-1) has importance:  3.188452606600913e-06\n",
      "A9(t-6) has importance:  3.4043966449416364e-06\n",
      "B10(t-4) has importance:  3.4114251813161163e-06\n",
      "K7(t-4) has importance:  4.451663658636275e-06\n",
      "K7(t-3) has importance:  5.315474116194002e-06\n",
      "K7(t-5) has importance:  6.228269827285108e-06\n",
      "B10(t-1) has importance:  6.8552831008466225e-06\n",
      "Rainfall6(t-7) has importance:  8.265367081475523e-06\n",
      "B10(t-2) has importance:  9.387906605524674e-06\n",
      "B10(t-8) has importance:  1.27704611190624e-05\n",
      "Rainfall6(t-2) has importance:  1.3639429085693423e-05\n",
      "B10(t-9) has importance:  1.4413217631993287e-05\n",
      "N8(t-3) has importance:  1.6970463968115326e-05\n",
      "Solar.Rad4(t-1) has importance:  2.5619008414825948e-05\n",
      "DAP1(t-2) has importance:  2.658248800956036e-05\n",
      "DAP1(t-5) has importance:  2.8987084432503746e-05\n",
      "DAP1(t-7) has importance:  3.4186661670273776e-05\n",
      "K7(t-6) has importance:  3.743315227195151e-05\n",
      "DAP1(t-1) has importance:  3.745852448699671e-05\n",
      "A9(t-9) has importance:  4.180669247202972e-05\n",
      "Temperature3(t-4) has importance:  4.447761061035966e-05\n",
      "Rainfall6(t-9) has importance:  4.9179115062352364e-05\n",
      "Humidity5(t-3) has importance:  5.14534813412702e-05\n",
      "DAP1(t-4) has importance:  5.2328015888245775e-05\n",
      "Solar.Rad4(t-6) has importance:  5.4270707519450735e-05\n",
      "DAP1(t-8) has importance:  5.854182730513886e-05\n",
      "Solar.Rad4(t-9) has importance:  7.031498156761998e-05\n",
      "Solar.Rad4(t-3) has importance:  8.630618927990567e-05\n",
      "Rainfall6(t-10) has importance:  0.00011025978924545117\n",
      "DAP1(t-6) has importance:  0.00012759511249025893\n",
      "DAP1(t-9) has importance:  0.00013624346249039679\n",
      "Solar.Rad4(t-10) has importance:  0.0001465041763603516\n",
      "Rainfall6(t-5) has importance:  0.00014853591119501054\n",
      "Temperature3(t-2) has importance:  0.0001490034731138549\n",
      "Solar.Rad4(t-8) has importance:  0.00015285249477421623\n",
      "Rainfall6(t-4) has importance:  0.00016094851983389973\n",
      "Humidity5(t-8) has importance:  0.00017074821816419325\n",
      "Humidity5(t-7) has importance:  0.00017454393758799133\n",
      "DAP1(t-10) has importance:  0.0001753032362685726\n",
      "Rainfall6(t-8) has importance:  0.00018489330202712983\n",
      "Temperature3(t-7) has importance:  0.00019843843987985484\n",
      "Humidity5(t-5) has importance:  0.00020032562608008676\n",
      "Humidity5(t-1) has importance:  0.0002184710418778462\n",
      "Solar.Rad4(t-2) has importance:  0.0002229328394003114\n",
      "Temperature3(t-3) has importance:  0.0002296927930389533\n",
      "DAP1(t-3) has importance:  0.000281073494599449\n",
      "Humidity5(t-10) has importance:  0.0003097170970069073\n",
      "Temperature3(t-8) has importance:  0.0003220993835765399\n",
      "Temperature3(t-1) has importance:  0.00034138281671778215\n",
      "Humidity5(t-4) has importance:  0.0003458085506572718\n",
      "Humidity5(t-6) has importance:  0.00036649863234399096\n",
      "Humidity5(t-9) has importance:  0.00036733087589872407\n",
      "Rainfall6(t-3) has importance:  0.00037955484362109707\n",
      "Temperature3(t-5) has importance:  0.0004270941275476427\n",
      "Solar.Rad4(t-4) has importance:  0.0004759499546384086\n",
      "Rainfall6(t-1) has importance:  0.00047804002184205204\n",
      "Humidity5(t-2) has importance:  0.0005620618330617273\n",
      "GrowthRate2(t-8) has importance:  0.0005705139061780712\n",
      "Temperature3(t-10) has importance:  0.0007193946193687166\n",
      "Temperature3(t-6) has importance:  0.0007696795377447839\n",
      "GrowthRate2(t-7) has importance:  0.0008517977827811249\n",
      "Solar.Rad4(t-5) has importance:  0.0008843892472630685\n",
      "GrowthRate2(t-6) has importance:  0.0008854394052131013\n",
      "Temperature3(t-9) has importance:  0.0009052450024541194\n",
      "GrowthRate2(t-10) has importance:  0.0009182963920912734\n",
      "Solar.Rad4(t-7) has importance:  0.001123712694136024\n",
      "Rainfall6(t-6) has importance:  0.0011872493433998702\n",
      "GrowthRate2(t-5) has importance:  0.0012343230423242906\n",
      "GrowthRate2(t-3) has importance:  0.0014463248216197429\n",
      "GrowthRate2(t-9) has importance:  0.0014671797643821065\n",
      "GrowthRate2(t-4) has importance:  0.0029393856877116655\n",
      "GrowthRate2(t-2) has importance:  0.0035514177840167234\n",
      "GrowthRate2(t-1) has importance:  0.9721419701464212\n"
     ]
    }
   ],
   "source": [
    "# MODELS RF\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def RF_train(X_train, X_test, y_train, y_test):\n",
    "    regr = RandomForestRegressor(n_estimators=20, max_depth=15, random_state=0, verbose=0, n_jobs=-1)\n",
    "    regr.fit(X_train, y_train)\n",
    "    preds = regr.predict(X_test)\n",
    "\n",
    "    preds = pd.DataFrame(preds)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    # evaluate_errors(preds, y_test)\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    print('MAE', mean_absolute_error(preds, y_test))\n",
    "    print('RMSE', np.sqrt(mean_squared_error(preds, y_test)))\n",
    "    get_feature_importances(regr)\n",
    "\n",
    "RF_train(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS LIGHTGBM\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "clf = LGBMRegressor(n_estimators=1000, learning_rate=0.01)\n",
    "clf.fit(X_train, np.log1p(y_train))\n",
    "preds = np.expm1(clf.predict(X_test))\n",
    "\n",
    "preds = pd.DataFrame(preds)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "\n",
    "evaluate_errors(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model.predict(X_test)\n",
    "preds = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)\n",
    "errs = evaluate_errors(preds, y_train)\n",
    "print(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "clf = MLPRegressor(solver='adam', activation='tanh')\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "# evaluate_errors(preds, y_test)\n",
    "# print(preds)\n",
    "# print(y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(preds, y_test))\n",
    "# get_feature_importances(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "clf = MLPRegressor(solver='lbfgs', activation='tanh')\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "evaluate_errors(preds, y_test)\n",
    "print(preds)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS AN EXAMPLE OF A TIME SERIES PROBLEM WITH KERAS\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# # convert an array of values into a dataset matrix\n",
    "# def create_dataset(dataset, look_back=1):\n",
    "# \tdataX, dataY = [], []\n",
    "# \tfor i in range(len(dataset)-look_back-1):\n",
    "# \t\ta = dataset[i:(i+look_back), 0]\n",
    "# \t\tdataX.append(a)\n",
    "# \t\tdataY.append(dataset[i + look_back, 0])\n",
    "# \treturn numpy.array(dataX), numpy.array(dataY)\n",
    "# # fix random seed for reproducibility\n",
    "# numpy.random.seed(7)\n",
    "# # load the dataset\n",
    "# # dataframe = read_csv('Syngenta/Syngenta_2017/Experiment_dataset.csv', engine='python', skipfooter=3)\n",
    "# dataset = dataframe.values\n",
    "# dataset = dataset.astype('float32')\n",
    "# # normalize the dataset\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# dataset = scaler.fit_transform(dataset)\n",
    "# # split into train and test sets\n",
    "# train_size = int(len(dataset) * 0.67)\n",
    "# test_size = len(dataset) - train_size\n",
    "# train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# # reshape into X=t and Y=t+1\n",
    "look_back = 12\n",
    "# trainX, trainY = create_dataset(train, look_back)\n",
    "# testX, testY = create_dataset(test, look_back)\n",
    "# # reshape input to be [samples, time steps, features]\n",
    "# trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "# testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(380, look_back)))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=2)\n",
    "# make predictions\n",
    "trainPredict = model.predict(X_train)\n",
    "testPredict = model.predict(X_test)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMER TRAIN AND TEST SPLIT APPORACH\n",
    "\n",
    "X_train = pd.DataFrame()\n",
    "y_train = np.array([])\n",
    "X_test = pd.DataFrame()\n",
    "y_test = np.array([])\n",
    "\n",
    "true_index_counter = -1\n",
    "for i in range (0, int(len(df_sf)/100)):\n",
    "    for k in range (1, 101):\n",
    "        true_index_counter += 1\n",
    "        if k <= 95:\n",
    "            X_train = X_train.append(df_sf.loc[true_index_counter].drop(['GrowthRate']))\n",
    "            y_train = np.append(y_train, df_sf.loc[true_index_counter, 'GrowthRate'])\n",
    "        else:\n",
    "            X_test = X_test.append(df_sf.loc[true_index_counter].drop(['GrowthRate']))\n",
    "            y_test = np.append(y_test, df_sf.loc[true_index_counter, 'GrowthRate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS AN EXAMPLE OF A TIME SERIES PROBLEM WITH KERAS\n",
    "\n",
    "# Stacked LSTM for international airline passengers problem with memory\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "dataframe = read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "# create and fit the LSTM network\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "for i in range(100):\n",
    "\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "model.reset_states()\n",
    "testPredict = model.predict(testX, batch_size=batch_size)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS AN EXAMPLE OF A TIME SERIES PROBLEM WITH KERAS\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import datasets\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load pima price dataset\n",
    "dataset = datasets.load_boston()\n",
    "# split into input (X) and output (Y) variables\n",
    "# X = dataset[:,0:8]\n",
    "# Y = dataset[:,8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=109, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "# model.fit(X, y, epochs=1000, verbose=0)\n",
    "\n",
    "# Compile model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=10)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "# print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
