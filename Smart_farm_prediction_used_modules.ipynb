{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN THE CSV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_sf = pd.read_csv('Smart_Farm_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO ANY PRE OBSERVATION HERE\n",
    "\n",
    "df_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE HOT REPRESENT (PREVIOUSLY) STRING ENCODED LOC AND GEN COLUMNS\n",
    "\n",
    "df_sf = pd.concat([df_sf, pd.get_dummies(df_sf['Loc'])], axis=1)\n",
    "df_sf = pd.concat([df_sf, pd.get_dummies(df_sf['Gen'])], axis=1)\n",
    "\n",
    "# DROP ANY COLUMNS HERE\n",
    "COLUMNS_DROPPED = ['Date', 'Loc', 'Gen', 'PlantID']\n",
    "df_sf = df_sf.drop(COLUMNS_DROPPED, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN AND TEST SPLIT\n",
    "\n",
    "X_train = pd.DataFrame()\n",
    "y_train = np.array([])\n",
    "X_test = pd.DataFrame()\n",
    "y_test = np.array([])\n",
    "\n",
    "true_index_counter = -1\n",
    "for i in range (0, int(len(df_sf)/100)):\n",
    "    for k in range (1, 101):\n",
    "        true_index_counter += 1\n",
    "        if k <= 95:\n",
    "            X_train = X_train.append(df_sf.loc[true_index_counter].drop(['GrowthRate']))\n",
    "            y_train = np.append(y_train, df_sf.loc[true_index_counter, 'GrowthRate'])\n",
    "        else:\n",
    "            X_test = X_test.append(df_sf.loc[true_index_counter].drop(['GrowthRate']))\n",
    "            y_test = np.append(y_test, df_sf.loc[true_index_counter, 'GrowthRate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "values = df_sf.values\n",
    "data = series_to_supervised(values,10,1)\n",
    "# print(data)\n",
    "# raw = DataFrame()\n",
    "# raw['ob1'] = [x for x in range(10)]\n",
    "# raw['ob2'] = [x for x in range(50, 60)]\n",
    "# values = raw.values\n",
    "# data = series_to_supervised(values, 1, 19)\n",
    "# print(data)\n",
    "data\n",
    "\n",
    "# X_train = data.drop(['var2(t)'], axis=1)\n",
    "# y_train = data.loc[:, 'var2(t)']\n",
    "# print([data if val == True else None for ind, val in enumerate(data.columns.str.contains('\\(t\\)'))])\n",
    "# for ind, val in enumerate(data.columns.str.contains('\\(t\\)')):\n",
    "#     if val == True:\n",
    "#         data.drop([])\n",
    "        \n",
    "X = data.loc[:,~data.columns.str.contains('\\(t\\)')]\n",
    "# X = data.drop([data.columns.str.contains('\\(t\\)')], axis=1)\n",
    "y = data.loc[:, 'var2(t)']\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.95, test_size = 0.05, random_state=42)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10     8.022080e-02\n",
       "204    1.046234e-02\n",
       "377    6.264949e+00\n",
       "191    2.060000e-09\n",
       "88     8.270000e-06\n",
       "141    2.179766e+00\n",
       "136    1.531918e+00\n",
       "263    1.093363e+00\n",
       "254    5.379734e-01\n",
       "389    2.346042e+00\n",
       "388    2.993216e+00\n",
       "56     3.633607e+00\n",
       "233    1.028219e-01\n",
       "82     5.385110e-04\n",
       "291    3.011005e+00\n",
       "87     1.660000e-05\n",
       "357    1.340267e+00\n",
       "35     6.373543e-01\n",
       "118    4.303355e-01\n",
       "175    5.099549e-03\n",
       "150    4.112670e+00\n",
       "330    1.479931e-01\n",
       "100    1.208868e-01\n",
       "361    1.857071e+00\n",
       "311    3.139045e-02\n",
       "49     2.034382e+00\n",
       "66     6.405798e+00\n",
       "380    6.939912e+00\n",
       "158    7.227862e+00\n",
       "128    8.712769e-01\n",
       "           ...     \n",
       "354    1.049276e+00\n",
       "245    2.647019e-01\n",
       "262    1.010513e+00\n",
       "31     4.574734e-01\n",
       "323    8.358543e-02\n",
       "170    4.878985e-01\n",
       "286    6.602047e+00\n",
       "201    8.259640e-03\n",
       "303    1.633962e-02\n",
       "353    9.670532e-01\n",
       "267    1.498494e+00\n",
       "318    5.557871e-02\n",
       "159    7.750917e+00\n",
       "140    2.031304e+00\n",
       "161    8.872804e+00\n",
       "369    3.546488e+00\n",
       "109    2.280831e-01\n",
       "382    6.770950e+00\n",
       "97     1.570000e-08\n",
       "340    3.347231e-01\n",
       "224    5.059199e-02\n",
       "131    1.076616e+00\n",
       "399    1.059273e-01\n",
       "30     4.210776e-01\n",
       "198    3.280000e-12\n",
       "81     1.080030e-03\n",
       "116    3.737123e-01\n",
       "280    4.173825e+00\n",
       "358    1.454172e+00\n",
       "112    2.818369e-01\n",
       "Name: var2(t), Length: 370, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.str.contains('\\(t\\)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR COMPUTATION FUNCTIONS\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def evaluate_errors(prediction, actual):\n",
    "    print(\"RMSE Error: \", np.sqrt(mean_squared_error(prediction, actual)))\n",
    "    avg_error_vector = np.absolute(((prediction - actual) / actual) * 100)\n",
    "    print(\"Average Error details:\\n\", np.mean(avg_error_vector))\n",
    "    return avg_error_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE IMPORTANCE FUNCTIONS\n",
    "\n",
    "def get_feature_importances(regr):\n",
    "    feature_importances = regr.feature_importances_\n",
    "    feature_importances = pd.Series(feature_importances)\n",
    "    feature_importance_df = pd.DataFrame({'feature': X_train.columns,'feature_importance': feature_importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by=['feature_importance'])\n",
    "    for index, row in feature_importance_df.iterrows():\n",
    "        print(row['feature'], 'has importance: ', row['feature_importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 0.11844984113493809\n",
      "RMSE 0.24384904931931925\n",
      "var9(t-5) has importance:  6.107092687644009e-08\n",
      "var8(t-5) has importance:  1.4540044676836902e-07\n",
      "var10(t-3) has importance:  1.6513145177284342e-07\n",
      "var7(t-10) has importance:  1.9441809943552976e-07\n",
      "var7(t-9) has importance:  2.7964191375554406e-07\n",
      "var9(t-1) has importance:  3.9825739100017034e-07\n",
      "var8(t-10) has importance:  4.3478085691295494e-07\n",
      "var9(t-2) has importance:  6.154446603399998e-07\n",
      "var8(t-6) has importance:  7.122705431483357e-07\n",
      "var7(t-7) has importance:  7.269386685733827e-07\n",
      "var8(t-4) has importance:  8.276870226034404e-07\n",
      "var9(t-10) has importance:  8.935667008970173e-07\n",
      "var8(t-9) has importance:  8.977447513172287e-07\n",
      "var8(t-8) has importance:  9.153436097290487e-07\n",
      "var9(t-8) has importance:  9.87353122500488e-07\n",
      "var10(t-10) has importance:  9.96884837386198e-07\n",
      "var9(t-3) has importance:  1.2509498177882424e-06\n",
      "var10(t-6) has importance:  1.36851456444999e-06\n",
      "var10(t-7) has importance:  1.5959922160435203e-06\n",
      "var10(t-5) has importance:  1.8744576621720063e-06\n",
      "var8(t-7) has importance:  1.9114280690766557e-06\n",
      "var8(t-1) has importance:  2.0309085635590377e-06\n",
      "var9(t-4) has importance:  2.2750836462632792e-06\n",
      "var7(t-8) has importance:  2.524833457638403e-06\n",
      "var7(t-2) has importance:  2.6297246323026703e-06\n",
      "var8(t-2) has importance:  2.843916954489588e-06\n",
      "var9(t-7) has importance:  2.9717220072430707e-06\n",
      "var7(t-1) has importance:  3.188452606600913e-06\n",
      "var9(t-6) has importance:  3.4043966449416364e-06\n",
      "var10(t-4) has importance:  3.4114251813161163e-06\n",
      "var7(t-4) has importance:  4.451663658636275e-06\n",
      "var7(t-3) has importance:  5.315474116194002e-06\n",
      "var7(t-5) has importance:  6.228269827285108e-06\n",
      "var10(t-1) has importance:  6.8552831008466225e-06\n",
      "var6(t-7) has importance:  8.265367081475523e-06\n",
      "var10(t-2) has importance:  9.387906605524674e-06\n",
      "var10(t-8) has importance:  1.27704611190624e-05\n",
      "var6(t-2) has importance:  1.3639429085693423e-05\n",
      "var10(t-9) has importance:  1.4413217631993287e-05\n",
      "var8(t-3) has importance:  1.6970463968115326e-05\n",
      "var4(t-1) has importance:  2.5619008414825948e-05\n",
      "var1(t-2) has importance:  2.658248800956036e-05\n",
      "var1(t-5) has importance:  2.8987084432503746e-05\n",
      "var1(t-7) has importance:  3.4186661670273776e-05\n",
      "var7(t-6) has importance:  3.743315227195151e-05\n",
      "var1(t-1) has importance:  3.745852448699671e-05\n",
      "var9(t-9) has importance:  4.180669247202972e-05\n",
      "var3(t-4) has importance:  4.447761061035966e-05\n",
      "var6(t-9) has importance:  4.9179115062352364e-05\n",
      "var5(t-3) has importance:  5.14534813412702e-05\n",
      "var1(t-4) has importance:  5.2328015888245775e-05\n",
      "var4(t-6) has importance:  5.4270707519450735e-05\n",
      "var1(t-8) has importance:  5.854182730513886e-05\n",
      "var4(t-9) has importance:  7.031498156761998e-05\n",
      "var4(t-3) has importance:  8.630618927990567e-05\n",
      "var6(t-10) has importance:  0.00011025978924545117\n",
      "var1(t-6) has importance:  0.00012759511249025893\n",
      "var1(t-9) has importance:  0.00013624346249039679\n",
      "var4(t-10) has importance:  0.0001465041763603516\n",
      "var6(t-5) has importance:  0.00014853591119501054\n",
      "var3(t-2) has importance:  0.0001490034731138549\n",
      "var4(t-8) has importance:  0.00015285249477421623\n",
      "var6(t-4) has importance:  0.00016094851983389973\n",
      "var5(t-8) has importance:  0.00017074821816419325\n",
      "var5(t-7) has importance:  0.00017454393758799133\n",
      "var1(t-10) has importance:  0.0001753032362685726\n",
      "var6(t-8) has importance:  0.00018489330202712983\n",
      "var3(t-7) has importance:  0.00019843843987985484\n",
      "var5(t-5) has importance:  0.00020032562608008676\n",
      "var5(t-1) has importance:  0.0002184710418778462\n",
      "var4(t-2) has importance:  0.0002229328394003114\n",
      "var3(t-3) has importance:  0.0002296927930389533\n",
      "var1(t-3) has importance:  0.000281073494599449\n",
      "var5(t-10) has importance:  0.0003097170970069073\n",
      "var3(t-8) has importance:  0.0003220993835765399\n",
      "var3(t-1) has importance:  0.00034138281671778215\n",
      "var5(t-4) has importance:  0.0003458085506572718\n",
      "var5(t-6) has importance:  0.00036649863234399096\n",
      "var5(t-9) has importance:  0.00036733087589872407\n",
      "var6(t-3) has importance:  0.00037955484362109707\n",
      "var3(t-5) has importance:  0.0004270941275476427\n",
      "var4(t-4) has importance:  0.0004759499546384086\n",
      "var6(t-1) has importance:  0.00047804002184205204\n",
      "var5(t-2) has importance:  0.0005620618330617273\n",
      "var2(t-8) has importance:  0.0005705139061780712\n",
      "var3(t-10) has importance:  0.0007193946193687166\n",
      "var3(t-6) has importance:  0.0007696795377447839\n",
      "var2(t-7) has importance:  0.0008517977827811249\n",
      "var4(t-5) has importance:  0.0008843892472630685\n",
      "var2(t-6) has importance:  0.0008854394052131013\n",
      "var3(t-9) has importance:  0.0009052450024541194\n",
      "var2(t-10) has importance:  0.0009182963920912734\n",
      "var4(t-7) has importance:  0.001123712694136024\n",
      "var6(t-6) has importance:  0.0011872493433998702\n",
      "var2(t-5) has importance:  0.0012343230423242906\n",
      "var2(t-3) has importance:  0.0014463248216197429\n",
      "var2(t-9) has importance:  0.0014671797643821065\n",
      "var2(t-4) has importance:  0.0029393856877116655\n",
      "var2(t-2) has importance:  0.0035514177840167234\n",
      "var2(t-1) has importance:  0.9721419701464212\n"
     ]
    }
   ],
   "source": [
    "# MODELS RF\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(n_estimators=20, max_depth=13, random_state=0, verbose=1, n_jobs=-1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "preds = pd.DataFrame(preds)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "# evaluate_errors(preds, y_test)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print('MAE', mean_absolute_error(preds, y_test))\n",
    "print('RMSE', np.sqrt(mean_squared_error(preds, y_test)))\n",
    "get_feature_importances(regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0  index   var2(t)\n",
      "0   0.170075     19  0.169169\n",
      "1   2.600314     52  2.608779\n",
      "2   1.190694     43  1.237118\n",
      "3   0.066420    321  0.070997\n",
      "4   4.677217    282  4.885199\n",
      "5   3.547287    387  3.720411\n",
      "6   6.591988     65  6.673638\n",
      "7   3.639187    278  3.565349\n",
      "8   0.569618    255  0.582081\n",
      "9   0.582223    123  0.612325\n",
      "10  2.674302    275  2.814726\n",
      "11  8.894836    163  9.851556\n",
      "12  0.000058     86  0.000033\n",
      "13  0.022202    212  0.019652\n",
      "14  6.705743    288  7.078277\n",
      "15  4.381625    281  4.515760\n",
      "16  0.136639    103  0.149377\n",
      "17  0.261871     25  0.278191\n",
      "18  1.421681    134  1.330349\n",
      "19  0.190446    241  0.193137\n"
     ]
    }
   ],
   "source": [
    "print(pd.concat([preds, y_test.astype(float).reset_index()], axis=1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[19,data.columns.str.contains('var2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS LIGHTGBM\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "clf = LGBMRegressor(n_estimators=1000, learning_rate=0.01)\n",
    "clf.fit(X_train, np.log1p(y_train))\n",
    "preds = np.expm1(clf.predict(X_test))\n",
    "\n",
    "preds = pd.DataFrame(preds)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "\n",
    "evaluate_errors(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model.predict(X_test)\n",
    "preds = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)\n",
    "errs = evaluate_errors(preds, y_train)\n",
    "print(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7861815302160826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "clf = MLPRegressor(solver='adam', activation='tanh')\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "# evaluate_errors(preds, y_test)\n",
    "# print(preds)\n",
    "# print(y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(preds, y_test))\n",
    "# get_feature_importances(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "clf = MLPRegressor(solver='lbfgs', activation='tanh')\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "evaluate_errors(preds, y_test)\n",
    "print(preds)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS AN EXAMPLE OF A TIME SERIES PROBLEM WITH KERAS\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# # convert an array of values into a dataset matrix\n",
    "# def create_dataset(dataset, look_back=1):\n",
    "# \tdataX, dataY = [], []\n",
    "# \tfor i in range(len(dataset)-look_back-1):\n",
    "# \t\ta = dataset[i:(i+look_back), 0]\n",
    "# \t\tdataX.append(a)\n",
    "# \t\tdataY.append(dataset[i + look_back, 0])\n",
    "# \treturn numpy.array(dataX), numpy.array(dataY)\n",
    "# # fix random seed for reproducibility\n",
    "# numpy.random.seed(7)\n",
    "# # load the dataset\n",
    "# # dataframe = read_csv('Syngenta/Syngenta_2017/Experiment_dataset.csv', engine='python', skipfooter=3)\n",
    "# dataset = dataframe.values\n",
    "# dataset = dataset.astype('float32')\n",
    "# # normalize the dataset\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# dataset = scaler.fit_transform(dataset)\n",
    "# # split into train and test sets\n",
    "# train_size = int(len(dataset) * 0.67)\n",
    "# test_size = len(dataset) - train_size\n",
    "# train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# # reshape into X=t and Y=t+1\n",
    "look_back = 12\n",
    "# trainX, trainY = create_dataset(train, look_back)\n",
    "# testX, testY = create_dataset(test, look_back)\n",
    "# # reshape input to be [samples, time steps, features]\n",
    "# trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "# testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(380, look_back)))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=2)\n",
    "# make predictions\n",
    "trainPredict = model.predict(X_train)\n",
    "testPredict = model.predict(X_test)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS AN EXAMPLE OF A TIME SERIES PROBLEM WITH KERAS\n",
    "\n",
    "# Stacked LSTM for international airline passengers problem with memory\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "dataframe = read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "# create and fit the LSTM network\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "for i in range(100):\n",
    "\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "model.reset_states()\n",
    "testPredict = model.predict(testX, batch_size=batch_size)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS AN EXAMPLE OF A TIME SERIES PROBLEM WITH KERAS\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import datasets\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load pima price dataset\n",
    "dataset = datasets.load_boston()\n",
    "# split into input (X) and output (Y) variables\n",
    "# X = dataset[:,0:8]\n",
    "# Y = dataset[:,8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=109, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "# model.fit(X, y, epochs=1000, verbose=0)\n",
    "\n",
    "# Compile model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=10)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "# print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
